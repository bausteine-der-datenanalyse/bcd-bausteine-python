{
  "hash": "707f9a9a4437baee117e5b2cbd6bf623",
  "result": {
    "engine": "jupyter",
    "markdown": "# Einleitung\nExperiementell gewonnene Daten können stark verrauscht sein oder die Beziehung der Variablen wird am besten durch einen nicht-linearen Zusammenhang beschrieben. Datenfitting ist der Prozess, ein Modell an einen Datensatz anzupassen, um die zugrundeliegende Beziehung zwischen den Variablen zu beschreiben, die Daten zu glätten oder Werte zwischen den vorhandenen Datenpunkten zu schätzen. Das Ziel dieses Prozesses ist es, eine Funktion zu finden, die den Datensatz so gut wie möglich beschreibt, indem die Abweichung zwischen dem Modell und den tatsächlichen Daten minimiert wird. Man nennt diesen Prozess auch Modellierung.\n\nIn diesem Baustein werden die folgenden Module verwendet:\n\n::: {#b5577246 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport numpy.polynomial.polynomial as poly\nimport matplotlib.pyplot as plt\n```\n:::\n\n\nDie Modellierung von Daten kann auf folgendes Problem verallgemeinert werden:\n\n1. Gegeben sind $n$ Messpunktpaare $(x_i, y_i)$ mit $x_i, y_i \\in \\mathbb{R}$.\n2. Gesucht ist eine Modellfunktion $y(x)$, welche die Messpunktpaare approximiert.\n\nEin möglicher Ansatz ist die Darstellung der Modellfunktion als Summe von $m$ Basisfunktionen $\\phi_i(x)$ mit den Koeffizienten $\\beta_i$. \n$$\ny(x) = \\sum_{i=1}^{m}\\beta_i \\cdot \\phi_i(x) = \\beta_1\\cdot \\phi_1(x) + \\cdots + \\beta_m\\cdot \\phi_m(x) \n$$\n\nDie Koeffizienten $\\beta_i$ müssen dabei so bestimmt werden, dass $y(x)$ so gut wie möglich – oder gar exakt – die Messpunkte approximiert. \n\nAls Abstandmaß zwischen einer Modellfunktion und den Messpunkten kann die Methode der kleinsten Quadrate genutzt werden.\n\n::: {#nte-ols .callout-note collapse=\"true\"}\n## Methode der kleinsten Quadrate\n\nMit der Methode der kleinsten Quadrate soll diejenige Gerade $\\hat{y} = \\beta_0 + \\beta_1 \\cdot x$ gefunden werden, die die quadrierten Abstände der  Vorhersagewerte $\\hat{y}$ von den tatsächlich gemessenen Werten $y$ minimiert. Die Werte $y_i - \\hat{y_i}$ sind die Residuen $e_i$. Es gilt also:\n\n$$\n\\sum_{i=1}^{N}(y_i - \\hat{y_i})^2 = \\sum_{i=1}^{N} e_i = \\min\n$$\n\nGrafisch kann man sich die Minimierung der quadrierten Abstände so vorstellen.\n\n:::: {.panel-tabset}\n## Grafik\n\n::: {#92497864 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](einleitung_files/figure-html/cell-3-output-1.png){fig-alt='Auf einer Geraden sind 9 Punkte markiert, die die Vorhersagewerte des linearen Modells repräsentieren. Vertikal darüber oder darunter sind die Messwerte eingezeichnet. Jeder Vorhersagewerte ist durch eine gestrichelte, vertikale Linie mit einem der Messwerte verbunden.'}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nRegressionskoeffizienten: [ 2.93333333 -0.73333333]\n```\n:::\n:::\n\n\n## Code\n\n::: {#218f08f6 .cell execution_count=3}\n``` {.python .cell-code}\nx = np.arange(1, 11)\ny = - x.copy() + 4\ny[0] -= 2\ny[2] -= 2\ny[3] += 3\ny[-3] += 5\n\nlm = poly.polyfit(x, y, 1)\nvorhersagewerte = poly.polyval(x, lm)\n\nplt.scatter(x, vorhersagewerte, label = 'Vorhersagewerte', marker = \"^\", color = \"tab:blue\")\nplt.scatter(x, y, label = 'Messwerte', marker = 'o', color = \"tab:orange\")\nplt.axline(xy1 = (0, lm[0]), slope = lm[1], label = \"Regressionsgerade\", color = \"tab:blue\")\ndotted = plt.vlines(x, ymin = vorhersagewerte, ymax = y, alpha = 0.6, ls = 'dotted', label = 'Residuen')\n\nplt.legend()\nplt.show()\n\nprint(\"Regressionskoeffizienten:\", lm)\n```\n:::\n\n\n::::\n\n&nbsp;\n\nDie eingezeichnete Gerade entspricht der linearen Funktion $\\hat{y} = \\beta_0 + \\beta_1 \\cdot x + e_i$. Die Dreiecksmarker sind die Vorhersagewerte $\\hat{y_i}$ des linearen Modells für die Werte $x_i = np.arange(1, 11)$. Die tatsächlichen Messwerte $y$ sind mit Kreismarkern markiert. Die Länge der gestrichelten Linien entspricht der Größe der Abweichung zwischen den Mess- und Vorhersagewerten $y_i - \\hat{y_i}$, also den Residuen $e_i$.\n\nGesucht wird diejenige Gerade, die die Summe der quadrierten Residuen minimiert. Die gesuchten Werte $\\beta_0$ und $\\beta_1$ sind die Kleinst-Quadrate-Schätzer.\n\n$$\n\\beta_0 = \\bar{y} - \\beta_1 \\cdot \\bar{x}\n$$\n\n$$\n\\beta_1 = { \\sum_{i=1}^n (x_i- \\bar{x}) \\cdot (y_i - \\bar{y}) \\over \\sum_{i=1}^n (x_i - \\bar{x})^2 }\n$$\n\nDer Vollständigkeit halber leiten wir die Kleinst-Quadrate-Schätzer her. Gesucht werden Werte für $\\beta_0$ und $\\beta_1$, damit die Summe der Residuenquadrate $\\sum_{i=1}^{n} e_i^2$ möglichst klein wird. Die Residuenquadratsumme ist die Summe der quadrierten Differenzen aus beobachteten Werten $y_i$ und der durch die lineare Funktion vorhergesagten Werte.\n$$\n\\sum_{i=1}^{n} e_i^2 ~ = ~ \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 \\cdot x_i))^2\n$$\n\nWir untersuchen also eine Funktion, die von zwei Variablen abhängig ist.\n$$\nf(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 \\cdot x_i))^2\n$$\n\nDas Summenzeichen ist die Kurzschreibweise für eine Summe.\n$$\nf(\\beta_0, \\beta_1) = (y_1 - (\\beta_0 + \\beta_1 \\cdot x_1))^2 ~ + ~ (y_2 - (\\beta_0 + \\beta_1 \\cdot x_2))^2 ~ + ~ ... ~ (y_n - (\\beta_0 + \\beta_1 \\cdot x_n))^2\n$$\n\nIm Minimum der Funktion müssen die beiden partiellen Ableitungen gleich Null sein (Warum das so ist, wird [hier](https://www.sofatutor.com/mathematik/funktionen/funktionen-mehrerer-veraenderlicher/lokale-extrempunkte-bei-funktionen-mit-mehreren-veraenderlichen) leicht verständlich erklärt.)\n\n:::: {#nte-partielleableitung .callout-note collapse=\"true\"}\n## Partielle Ableitung\nDie partielle Ableitung ist die Ableitung einer Funktion mit mehreren Variablen nach einer Variablen, wobei die übrigen Variablen als Konstanten behandelt werden.\n\nFür eine Funktion $f(x, y) = 2x + y^2$ wird die partielle Ableitung nach x so ausgedrückt:\n\n$\\frac{\\partial f(x, y)}{\\partial x}$\n\n  - Das Symbol ∂ ist die kursive Darstellung des kyrillischen Kleinbuchstaben д (d) und wird als \"del\" gelesen. Es zeigt an, das eine partielle Ableitung durchgeführt wird.\n  - Im Zähler steht die Funktion, die abgeleitet werden soll. Im Nenner steht die Variable nach der abgeleitet wird. Der Term wird gelesen als \"del f von x und y nach del x\".\n\nDie partielle Ableitung $\\frac{\\partial f(x, y)}{\\partial x} = 2$. $y^2$ wird als Konstante behandelt (z. B. $5^2$ ) und ist abgeleitet Null.\n\nDie partielle Ableitung $\\frac{\\partial f(x, y)}{\\partial y} = 2y$. $2x$ wird als Konstante behandelt (z. B. $2 \\cdot 3$ ) und ist abgeleitet Null.\n\n::::\n\nIn beiden partiellen Ableitungen sind $x_i$ und $y_i$ konstant. In der partiellen Ableitung nach $\\beta_0$ ist außerdem $\\beta_1$ konstant, in der partiellen Ableitung nach $\\beta_1$ ist entsprechend $\\beta_0$ konstant.\n\n:::: {.panel-tabset}\n\n## partielle Ableitung nach dem y-Achsenschnittpunkt\nFür die partielle Ableitung nach $\\beta_0$ gilt also nach der Kettenregel für die äußere Funktion (oben) und die innere Funktion (Mitte):\n$$ \\begin{aligned}\n\\frac{\\partial f(\\beta_0, \\beta_1)}{\\partial \\beta_0} = 2 \\cdot (y_1 - (\\beta_0 + \\beta_1 \\cdot x_1)) ~ + ~ ... ~ (y_n - (\\beta_0 + \\beta_1 \\cdot x_n)) = 2 \\cdot \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 \\cdot x_i)) ~ \\cdot\n\\newline \n(0 - (1 + 0 \\cdot 0)) ~ + ~ ... ~ (0 - (1 + 0 \\cdot 0)) = \\sum_{i=1}^{n} (0 - (1 + 0 \\cdot 0)) =\n\\newline\n2 \\cdot \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 \\cdot x_i)) ~ \\cdot (-1)\n\\end{aligned}\n$$\n\nFür die partielle Ableitung nach $\\beta_0$  gilt also:\n$$\n\\frac{\\partial f(\\beta_0, \\beta_1)}{\\partial \\beta_0} = -2 \\cdot \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 \\cdot x_i)) = 0\n$$\n\nDiese kann vereinfacht werden, indem der Vorfaktor $-2$ entfällt (weil $-2 \\cdot 0 = 0$ gelten muss) und die Vorzeichen aufgelöst werden. Sodass:\n$$\n\\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 \\cdot x_i) = 0\n$$\n\nMan kann auch schreiben:\n$$\n\\sum_{i=1}^{n} y_i - \\sum_{i=1}^{n} \\beta_0 - \\sum_{i=1}^{n} \\beta_1 \\cdot x_i = 0\n$$\n\n$\\beta_0$ und $\\beta_1$ sind Konstanten, sodass gilt $\\sum_{i=1}^{n} \\beta_0  = \\beta_0 \\cdot \\sum_{i=1}^{n} 1 = \\beta_0 \\cdot n$ und $\\sum_{i=1}^{n} \\beta_1 \\cdot x_i = \\beta_1 \\cdot \\sum_{i=1}^{n} 1 \\cdot x_i$. So gilt:\n$$\n\\sum_{i=1}^{n} y_i - n \\cdot \\beta_0 - \\beta_1 \\cdot \\sum_{i=1}^{n} x_i = 0\n$$\n\nJetzt kann man durch $n$ teilen. Dabei entspricht $\\frac{\\sum_{i=1}^{n}y_i}{n}$ dem arithmetischen Mittelwert von $y$ und $\\frac{\\sum_{i=1}^{n}x_i}{n}$ dem arithmetischen Mittelwert von $x$. Somit steht:\n$$\n\\bar{y} - \\beta_0 - \\beta_1 \\cdot \\bar{x}= 0\n$$\n\nUmgestellt:\n$$\n\\beta_0 = \\bar{y} - \\beta_1 \\cdot \\bar{x}\n$$\n\n## partielle Ableitung nach dem Anstieg\nFür die partielle Ableitung nach $\\beta_1$ ist ebenfalls die Kettenregel anzuwenden, sodass die äußere Funktion (oben) identisch abgeleitet wird:\n$$ \\begin{aligned}\n\\frac{\\partial f(\\beta_0, \\beta_1)}{\\partial \\beta_1} = 2 \\cdot (y_1 - (\\beta_0 + \\beta_1 \\cdot x_1)) ~ + ~ ... ~ (y_n - (\\beta_0 + \\beta_1 \\cdot x_n)) = 2 \\cdot \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 \\cdot x_i)) ~ \\cdot\n\\newline \n(0 - (0 + 1 \\cdot x_1)) ~ + ~ ... ~ (0 - (0 + 1 \\cdot x_n)) ~ = ~ \\sum_{i=1}^{n} -x_i = \n\\newline\n2 \\cdot \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 \\cdot x_i)) ~ \\cdot -x_i\n\\end{aligned}\n$$\n\nFür die partielle Ableitung nach $\\beta_1$  gilt also:\n$$\n\\frac{\\partial f(\\beta_0, \\beta_1)}{\\partial \\beta_1} = -2 \\sum_{i=1}^{n} x_i \\cdot (y_i - (\\beta_0 + \\beta_1 \\cdot x_i)) = 0\n$$\n\nAuch diese kann vereinfacht werden, indem der Vorfaktor $-2$ entfällt (weil $-2 \\cdot 0 = 0$ gelten muss) und die Vorzeichen aufgelöst werden. Außerdem kann ausmultipliziert werden:\n$$\n\\sum_{i=1}^{n} x_iy_i - \\sum_{i=1}^{n}  \\beta_0 \\cdot x_i - \\sum_{i=1}^{n}  \\beta_1 \\cdot x_ix_i = 0\n$$\n\nWieder können die Konstanten herausgezogen werden:\n$$\n\\sum_{i=1}^{n} x_iy_i - \\beta_0 \\cdot \\sum_{i=1}^{n} x_i - \\beta_1 \\cdot \\sum_{i=1}^{n} x_ix_i = 0\n$$\n\nJetzt kann man $\\beta_0 = \\bar{y} - \\beta_1 \\cdot \\bar{x}$ und $\\sum_{i=1}^{n} x_i = n \\cdot \\bar{x}$ einsetzen:\n$$\n\\sum_{i=1}^{n} x_iy_i - (\\bar{y} - \\beta_1 \\cdot \\bar{x}) \\cdot  n \\cdot \\bar{x} - \\beta_1 \\cdot \\sum_{i=1}^{n} x_ix_i = 0\n$$\n\nDer mittlere Term wird ausmultipliziert und $x_ix_i$ im letzten Term als $x_i^2$ geschrieben:\n$$\n\\sum_{i=1}^{n} x_iy_i - n\\bar{x}\\bar{y} - \\beta_1 \\cdot n\\bar{x}\\bar{x} - \\beta_1 \\cdot \\sum_{i=1}^{n} x_i^2 = 0\n$$\n\nDie letzten beiden Terme werden unter Anwendung des Distributivgesetzes $a−b = − (b−a)$ zusammengefasst.\n$$\n\\sum_{i=1}^{n} x_iy_i - n\\bar{x}\\bar{y} - \\beta_1 \\cdot ( \\sum_{i=1}^{n} x_i^2 - n \\bar{x}^2) = 0\n$$\n\nJetzt kann nach $\\beta_1$ umgestellt werden. Erst:\n$$\n\\beta_1 \\cdot ( \\sum_{i=1}^{n} x_i^2 - n \\bar{x}^2) = \\sum_{i=1}^{n} x_iy_i - n\\bar{x}\\bar{y}\n$$\n\nDann:\n$$\n\\beta_1 = \\frac{\\sum_{i=1}^{n} x_iy_i - n\\bar{x}\\bar{y}}{\\sum_{i=1}^{n} x_i^2 - n \\bar{x}^2}\n$$\n\nNun kann zuerst mit $\\sum_{i=1}^{n} x_i^2 - n \\bar{x}^2 = \\sum_{i=1}^{n} (x_i - \\bar{x})^2$ umgeformt werden.\n$$\n\\beta_1 = \\frac{\\sum_{i=1}^{n} x_iy_i - n\\bar{x}\\bar{y}}{\\sum_{i=1}^{n}  (x_i - \\bar{x})^2}\n$$\n\nDann - und das wird gleich gezeigt - mit $\\sum_{i=1}^{n} x_iy_i - n\\bar{x}\\bar{y} = \\sum_{i=1}^{n} (x_i - \\bar{x}) (y_i - \\bar{y})$. Sodass steht:\n$$\n\\beta_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum_{i=1}^{n}  (x_i - \\bar{x})^2}\n$$\n\nDer letzte Schritt wird ausgehend vom Ergebnis gezeigt und beginnt mit dem Ausmultiplizieren:\n$$\n\\sum_{i=1}^{n} (x_i - \\bar{x}) (y_i - \\bar{y}) = \\sum_{i=1}^{n} (x_iy_i - x_i\\bar{y} - \\bar{x}y_i + \\bar{x}\\bar{y})\n$$\n\nMan kann auch schreiben:\n$$\n\\sum_{i=1}^{n} x_iy_i - \\sum_{i=1}^{n} x_i\\bar{y} - \\sum_{i=1}^{n} \\bar{x}y_i + \\sum_{i=1}^{n} \\bar{x}\\bar{y}\n$$\n\n$\\bar{x}$ und $\\bar{y}$ sind Konstanten, sodass $\\bar{x} \\cdot \\sum_{i=1}^{n} y_i$ und $\\bar{y} \\cdot \\sum_{i=1}^{n} x_i$ geschrieben werden kann. $\\sum_{i=1}^{n} x_i$ ist gleich $n \\cdot \\bar{x}$ (analog für $y$). So ergibt sich:\n\n$$\n\\sum_{i=1}^{n} x_iy_i - \\bar{y} \\cdot n \\cdot \\bar{x} - \\bar{x} \\cdot n \\cdot \\bar{y} + \\sum_{i=1}^{n} \\bar{x}\\bar{y}\n$$\n\nSortieren:\n$$\n\\sum_{i=1}^{n} x_iy_i - n\\bar{x}\\bar{y}  -  n\\bar{x}\\bar{y} + \\sum_{i=1}^{n} \\bar{x}\\bar{y}\n$$\n\nDer letzte Term $\\sum_{i=1}^{n} \\bar{x}\\bar{y}$ kann auch $n \\cdot \\bar{x}\\bar{y}$ geschrieben werden, sodass sich ergibt:\n$$\n\\sum_{i=1}^{n} x_iy_i - n\\bar{x}\\bar{y} - n\\bar{x}\\bar{y} + n\\bar{x}\\bar{y}\n$$\n\nDie letzten beiden Terme entfallen somit und es bleibt:\n$$\n\\sum_{i=1}^{n} x_iy_i - n\\bar{x}\\bar{y} \n$$\n\n[@Baitsch-2019, S. 73-74]\n\n::::\n:::\n\nIn diesem Kapitel werden folgende Verfahren für die Modellierung von Daten vorgestellt:\n\n1. die Polynominterpolation,\n2. Datenfitting durch Polynome und\n3. Datenfitting durch Splines.\n\n",
    "supporting": [
      "einleitung_files"
    ],
    "filters": [],
    "includes": {}
  }
}