{
  "hash": "88f0a7fc1f2b62b92ad555111520311d",
  "result": {
    "engine": "jupyter",
    "markdown": "# Einleitung\n\n<div style=\"position: relative; width: 100%; aspect-ratio: 16 / 9;\">\n  <iframe src=\"https://av.tib.eu/player/71638\" allowfullscreen style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;\"></iframe>\n</div>\n\n&nbsp;\n\n2016 stellte eine Studie fest, dass ein Fünftel aller wissenschaftlichen Artikel im Bereich der Genetik auf der Grundlage von durch die Tabellenkalkulation Excel verfälschten Daten durchgeführt wurde [@Ziemann-2016]. Genbezeichnungen wie \"MARCH1\" wurden fälschlicherweise in ein Datumsformat umgewandelt. 2021 wurde diese Schätzung des Anteils betroffener Arbeiten sogar auf 30 Prozent angehoben. ([heise online](https://www.heise.de/news/Excel-wandelt-Genbezeichnungen-in-Datumsangaben-um-Problem-groesser-als-gedacht-6165902.html))\n\nAm Beginn der computergestützten Datenanalyse steht das Einlesen von Daten aus Dateien. In der Praxis ist das Einlesen von Daten alles andere als trivial. Daten werden in einer Vielzahl von Dateiformaten gespeichert. Deshalb ist es in der Datenanalyse erforderlich, mit verschiedenen Dateiformaten umgehen zu können: mit wenigen Kilobyte großen Textdateien, offenen und proprietären Formaten gängiger Büroanwendungen und mehreren hundert Megabyte großen Dateien in speziell für den Austausch wissenschaftlicher Daten entwickelten Formaten. Programmiersprachen wie Python und R bringen verschiedene Werkzeuge zum Lesen, Bearbeiten und Speichern von verschiedenen Dateiformaten mit. Spezialisierte Pakete ergänzen den Werkzeugkasten.\n\nDie praktischen Herausforderungen der Datenanalyse beschränken sich jedoch nicht nur auf technische Aspekte. Oftmals bereitet der innere Aufbau von Datensätzen die größten Schwierigkeiten. Ein wichtiger Bestandteil des Einlesens strukturierter Datensätze besteht darin, Fehler im Datensatz zu suchen und ggf. zu bereinigen. Dasu und Johnson schreiben: \n\n::: {.border layout=\"[5, 90, 5]\"}\n\n&nbsp;\n\n\"Unfortunately, the data set is usually dirty, composed of many tables, and has unknown properties. Before any results can be produced, the data must be cleaned and explored—often a long and\ndifficult task. [...] In our experience, the tasks of exploratory data mining and data cleaning constitute 80% of the effort that determines 80% of the value of the ultimate data\nmining results.\" (@Dasu-Johnson-2003, S. ix)\n\n&nbsp;\n\n:::\n\n&nbsp;\n\nDas Einlesen strukturierter Datensätze umfasst somit den gesamten Prozess des technischen Zugriffs auf Dateien, der Organisation, Fehlersuche und -korrektur sowie des Abspeicherns der Daten in einer für die weitere Bearbeitung geeigneten Form.\n\nIn der praktischen Datenanalyse helfen zwei einfache Tipps beim Einlesen strukturierter Datensätze:\n\n::: {#tip-editor .callout-tip collapse=\"false\"}\n**Schauen Sie sich Ihre Daten an, bevor Sie diese mit Python einlesen!** Dafür reicht ein Texteditor oder ein Tabellenkalkulationsprogramm (hier die automatische Erkennung und Umwandlung von Datumsformaten beachten). Ein kurzer Blick genügt, um die verwendeten Zeichentrenner, Tausendertrennzeichen, Datumsformate, die Kodierung fehlender Werte und die Unicode-Kodierung (wie UTF-8) zu identifizieren.\n:::\n  \nDies ist aber nicht immer möglich, beispielsweise wenn Ihr Datensatz aus hunderten Spalten und zehntausenden Zeilen besteht. Dieser Baustein vermittelt deshalb die Handwerkszeuge, um Datensätze ausschließlich mit den in Python verfügbaren Mitteln einzulesen.\n\nEs ist nicht erforderlich, die Besonderheiten aller hier vorgestellten Pakete und Funktionen auswendig zu beherrschen. Dafür ist das Themenfeld zu komplex und nicht selten ändert sich das Verhalten von Funktionen mit der Weiterentwicklung der Programmiersprache. Die hier vorgestellten Besonderheiten von Funktionen dienen jedoch als mentale Ankerpunkte, die als Anknüpfungspunkt dienen sollen, wenn Sie in der Praxis auf Probleme stoßen.\n\n::: {#tip-dokumentation .callout-tip collapse=\"false\"}\n**Benutzen Sie die Dokumentation!** Auf diese Weise erhalten Sie einen vollständigen Überblick über standardmäßig gesetzte und optional verfügbare Parameter. Außerdem erkennen Sie Änderungen in der Programmausführung und vermeiden so unerwartete Fehler.\n\n:::: {layout=\"[1, 1]\"}\n\n![Neuerung in Python](00-bilder/added-in-pyhton.png){fig-alt=\"Hinweis auf eine Neuerung in Python\"}\n\n![Abkündigung in Python](00-bilder/deprecated-in-python.png){fig-alt=\"Hinweis auf eine Abkündigung in Python\"}\n\n::::\n\n:::\n\n# Grundlagen: Merkmale von Datensätzen\nBevor wir uns mit den praktischen Herausforderungen des Einlesens strukturierter Datensätze beschäftigten, werden zunächst einige Merkmale von Datensätzen behandelt, um ein grundlegendes Verständnis der Begrifflichkeiten zu schaffen und den Umgang der in der Basis von Python enthaltenen Werkzeuge zu vermitteln. Am Ende dieses Kapitels wird mit tidy data ein grundlegendes Konzept zur Organisation von Datensätzen vorgestellt.\n\n::: {#imp-Datensatz .callout-important}\n## Datensatz\n\nEin Datensatz ist eine Sammlung zusammengehöriger Daten. Datensätze enthalten einer oder mehreren Variablen zugeordnete Werte. Jeder Datensatz besitzt ein technisches Format, eine Struktur, mindestens eine Variable und mindestens einen Wert.\n\n:::\n\n## Technisches Format\nDas technische Format eines Datensatzes gibt vor, mit welchen Mitteln Daten eingelesen, bearbeitet und gespeichert werden können. Einige Beispiele sind:\n\n  - Druckerzeugnis, z. B. Telefonbuch: manuelles Ablesen von Name und Telefonnummer, irreversible Bearbeitung per Stift\n  \n  - Lochkarte, z. B. Parkschein: Lesegerät erkennt Lochung und gewährt eine Freistunde, irreversible Bearbeitung mit Stanzgerät\n  \n  - Textdatei, z. B. Einwohnerzahl nach Bundesländern: Kann mit einer Vielzahl von Computerprogrammen wie Texteditor, Tabellenkalkulationsprogramm oder Programmierumgebung eingelesen, bearbeitet und gespeichert werden.\n  \n  - Hierarchical Data Format HDF5, z. B. räumliche Daten zur Blitzdichte: benötigt spezialisierte Programme oder Pakete\n\n## Struktur\nDatensätze speichern Daten in einer definierten n-dimensionalen Struktur.\n\n::: {.border}\n![n-dimensionale Datensätze](00-bilder/slicing_mf_mp.png){fig-alt=\"Dargestellt sind von links nach rechts ein-, zwei- und dreidimensionale Blockstrukturen, die Datensätze repräsentieren. Die Teilgrafiken werden in den folgenden Abschnitten wiederverwendet und dabei auch näher beschrieben.\"}\n\nslicing von Marc Fehr ist lizensiert unter [CC-BY-4.0](https://github.com/bausteine-der-datenanalyse/w-python-numpy-grundlagen#CC-BY-4.0-1-ov-file) und abrufbar auf [GitHub](https://github.com/bausteine-der-datenanalyse/w-python-numpy-grundlagen). 2024\n:::\n\n### Eindimensionale Datensätze\nDie einfachste Form sind eindimensionale Datensätze, die Werte einer einzigen Variablen zuordnen. Eindimensionale Datensätze mit Werten des gleichen Typs (bspw. Zahlen) werden **Vektor** genannt. Eindimensionale Datensätze, die unterschiedliche Datentypen enthalten können, heißen **Liste**. Eindimensionale Datensätze verfügen lediglich über eine Achse: den Index, über den Elemente angesprochen werden können.\n\n::: {.border}\n\n![eindimensionale Datensätze](00-bilder/eindimensionaler-datensatz-slicing-mf-mp.png){width=\"50%\" fig-alt=\"Dargestellt ist ein in fünf Blöcke unterteilter Streifen, der einen eindimensionalen Datensatz repräsentiert. Die Blöcke sind entlang der 0. Achse von links nach rechts mit 0 bis 4 beschriftet. Von Block Null aus geht ein blauer Pfeil zu Block drei, der blau markiert ist.\"}\n\nslicing von Marc Fehr ist lizensiert unter [CC-BY-4.0](https://github.com/bausteine-der-datenanalyse/w-python-numpy-grundlagen#CC-BY-4.0-1-ov-file) und abrufbar auf [GitHub](https://github.com/bausteine-der-datenanalyse/w-python-numpy-grundlagen). Die Grafik wurde auf den gezeigten Teil beschnitten und die obenstehende Beschriftung entfernt. 2024\n:::\n\n&nbsp;\n\nBeispiele eindimensionaler Datensätze sind ein Einkaufszettel oder die Urliste eines Würfelexperiments. Über den Index kann beispielsweise das Würfelergebnis an der Indexposition 2 ausgegeben werden.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nprint( *( Augen := [6, 2, 1, 2] ) )\n\nprint(f\"Das Würfelergebnis an Indexposition 2 lautet: {Augen[2]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n6 2 1 2\nDas Würfelergebnis an Indexposition 2 lautet: 1\n```\n:::\n:::\n\n\n### Eindimensionale Daten einlesen mit Python\nAn dieser Stelle eine kleine Wiederholung aus dem [Werkzeugbaustein Python](https://bausteine-der-datenanalyse.github.io/w-python/output/book/):  \nDie Pythonbasis greift über Dateiobjekte auf Dateien zu. Die Funktionen und Methoden haben Sie im Werkzeugbaustein Python kennengelernt. Der Zugriff auf Dateien über die Pythonbasis ist eine verlässliche Rückfalloption und darüber hinaus nützlich, um die Enkodierung einer Datei zu bestimmen.\n\n::: {#tip-openundco .callout-tip collapse=\"false\"}\n## Kleine Wiederholung: Funktionen und Methoden der Pythonbasis\n\n- Die Funktion `os.getcwd()` aus dem Modul os gibt das aktuelle Arbeitsverzeichnis aus, mit der Funktion `os.cwd(pfad)` kann es gewechselt werden.\n- Die Funktion `open(dateipfad, mode = 'r')` öffnet eine Datei im Lesemodus und gibt ein Dateiobjekt zurück.\n- Informationen zum Dateiobjekt können durch Ausgabe verschiedener Attribute abgerufen werden: `dateiobjekt.name`, `os.path.basename(dateiobjekt.name)`, `dateiobjekt.closed`, `dateiobjekt.mode`, `dateiobjekt.encoding` \n- Das Dateiobjekt kann mit Methoden wie `dateiobjekt.read()`, `dateiobjekt.readline()`, `dateiobjekt.readlines()` oder der Funktion `list(dateiobjekt)` ausgelesen werden.\n- Die Methode `dateiobjekt.close()` schließt die Datei und gibt sie somit wieder für andere Programme frei.\n\n:::\n\n**Lesen Sie die Datei \"python.txt\" unter dem dateipfad \"skript/01-daten/\" ein.**\n\n - Bestimmen Sie die Enkodierung der Datei.\n\n - Entfernen Sie die die erste Zeile aus dem Text und geben Sie den Text mit Python aus.\n\n - Wie kann der Text korrekt dargestellt werden?\n\n::: {#tip-pythonbasis .callout-tip collapse=\"true\"}\n## Musterlösung python.txt\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndateipfad = \"01-daten/\" + \"python.txt\"\ndateiobjekt = open(dateipfad, mode = 'r')\n\n# Enkodierung der Datei bestimmen\nprint(f\"Die Enkodierung der Datei lautet: {dateiobjekt.encoding}\")\n\n# Text ausgeben\ntext_als_liste = list(dateiobjekt)\n\nfor i in range(1, len(text_als_liste)):\n  print(text_als_liste[i])\n\n# Datei schließen.\ndateiobjekt.close()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDie Enkodierung der Datei lautet: UTF-8\n\n\nPython ist eine universell nutzbare, üblicherweise interpretierte, höhere Programmiersprache.[14] Sie hat den Anspruch, einen gut lesbaren, knappen Programmierstil zu fördern.[15] So werden beispielsweise Blöcke nicht durch geschweifte Klammern, sondern durch Einrückungen strukturiert. \n\nPython wurde mit dem Ziel größter Einfachheit und Übersichtlichkeit entworfen. Dies wird vor allem durch zwei Maßnahmen erreicht. Zum einen kommt die Sprache mit relativ wenigen Schlüsselwörtern aus.[49] Zum anderen ist die Syntax reduziert und auf Übersichtlichkeit optimiert. Dadurch lassen sich Python-basierte Skripte deutlich knapper formulieren als in anderen Sprachen.[50]\n\nVan Rossum legte bei der Entwicklung großen Wert auf eine Standardbibliothek, die überschaubar und leicht erweiterbar ist. Dies war Ergebnis seiner schlechten Erfahrung mit der Sprache ABC, in der das Gegenteil der Fall ist.[51] Dieses Konzept ermöglicht, in Python Module aufzurufen, die in anderen Programmiersprachen geschrieben wurden, etwa um Schwächen von Python auszugleichen. Beispielsweise können für zeitkritische Teile in maschinennäheren Sprachen wie C implementierte Routinen aufgerufen werden.\n\n\n\nAuszug aus https://de.wikipedia.org/wiki/Python_(Programmiersprache), abgerufen am 20.02.2025\n```\n:::\n:::\n\n\nEnkodierung UTF-8 auswählen.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Mit europäischen Sonderzeichen kompatible Enkodierung UTF-8 wählen\ndateiobjekt = open(dateipfad, mode = 'r', encoding = 'utf-8')\n\n# Text ausgeben\ntext_als_liste = list(dateiobjekt)\n\nfor i in range(1, len(text_als_liste)):\n  print(text_als_liste[i])\n\n# Datei schließen.\ndateiobjekt.close()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\nPython ist eine universell nutzbare, üblicherweise interpretierte, höhere Programmiersprache.[14] Sie hat den Anspruch, einen gut lesbaren, knappen Programmierstil zu fördern.[15] So werden beispielsweise Blöcke nicht durch geschweifte Klammern, sondern durch Einrückungen strukturiert. \n\nPython wurde mit dem Ziel größter Einfachheit und Übersichtlichkeit entworfen. Dies wird vor allem durch zwei Maßnahmen erreicht. Zum einen kommt die Sprache mit relativ wenigen Schlüsselwörtern aus.[49] Zum anderen ist die Syntax reduziert und auf Übersichtlichkeit optimiert. Dadurch lassen sich Python-basierte Skripte deutlich knapper formulieren als in anderen Sprachen.[50]\n\nVan Rossum legte bei der Entwicklung großen Wert auf eine Standardbibliothek, die überschaubar und leicht erweiterbar ist. Dies war Ergebnis seiner schlechten Erfahrung mit der Sprache ABC, in der das Gegenteil der Fall ist.[51] Dieses Konzept ermöglicht, in Python Module aufzurufen, die in anderen Programmiersprachen geschrieben wurden, etwa um Schwächen von Python auszugleichen. Beispielsweise können für zeitkritische Teile in maschinennäheren Sprachen wie C implementierte Routinen aufgerufen werden.\n\n\n\nAuszug aus https://de.wikipedia.org/wiki/Python_(Programmiersprache), abgerufen am 20.02.2025\n```\n:::\n:::\n\n\n:::\n\n### Zweidimensionale Datensätze\nZweidimensionale Datensätze organisieren Werte in einer aus Zeilen und Spalten bestehenden **Matrix** oder einem **Dataframe**. Eine Matrix enthält nur einen Datentyp (bspw. Zahlen), ein Dataframe kann unterschiedliche Datentypen enthalten (bspw. Zahlen und Wahrheitswerte). In Python stellt das Modul Pandas die DataFrame-Struktur bereit (siehe [Werkzeugbaustein Pandas](https://bausteine-der-datenanalyse.github.io/w-pandas/output/book/)).\n\n::: {.border}\n![zweidimensionaler Datensatz](00-bilder/zweidimensionaler-datensatz-slicing-mf-mp.png){width=\"45%\" fig-alt=\"Dargestellt ist ein zweidimensionaler Block, der einen zweidimensionalen Datensatz repräsentiert. Pfeile repräsentieren die zwei Achsen. Die nullte Achse entspricht der Länge (von oben nach unten) und die erste Achse der Breite des Datensatzes.\"}\n\nslicing von Marc Fehr ist lizensiert unter [CC-BY-4.0](https://github.com/bausteine-der-datenanalyse/w-python-numpy-grundlagen#CC-BY-4.0-1-ov-file) und abrufbar auf [GitHub](https://github.com/bausteine-der-datenanalyse/w-python-numpy-grundlagen). Die Grafik wurde auf den gezeigten Teil beschnitten und die obenstehende Beschriftung entfernt. 2024\n:::\n\n&nbsp;\n\nTypischerweise entspricht in zweidimensionalen Datensätzen jede Spalte einer **Variablen** und jede Zeile einer **Beobachtung**. Variablen speichern alle Werte eines Merkmals, zum Beispiel des Würfelergebnisses. Beobachtungen speichern alle Werte, die für eine Beobachtungseinheit gemessen wurden, z. B. für eine Person. [@Wickham-2014, S. 3]\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport pandas as pd\n\nmessung1 = pd.DataFrame({'Name': ['Hans', 'Elke', 'Jean', 'Maya'], 'Geburtstag': ['26.02.', '14.03.', '30.12.', '07.09.'], 'Würfelfarbe': ['rosa', 'rosa', 'blau', 'gelb'], 'Summe Augen': [17, 12, 8, 23]})\n\nmessung1\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Geburtstag</th>\n      <th>Würfelfarbe</th>\n      <th>Summe Augen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hans</td>\n      <td>26.02.</td>\n      <td>rosa</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Elke</td>\n      <td>14.03.</td>\n      <td>rosa</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Jean</td>\n      <td>30.12.</td>\n      <td>blau</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Maya</td>\n      <td>07.09.</td>\n      <td>gelb</td>\n      <td>23</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n&nbsp;\n\nÜber die Angabe der Indizes entlang der 0. (Zeilen) und der 1. Achse (Spalten) kann die Summe der gewürfelten Augen einer Person ausgegeben werden. \n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nprint(f\"Jean würfelte {messung1.iloc[2, 3]} Augen\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nJean würfelte 8 Augen\n```\n:::\n:::\n\n\nEs ist aber auch möglich, zunächst eine Spalte auszuwählen und dann wie bei einem eindimensionalen Datensatz den Wert an einer Indexposition aufzurufen. Dies wird verkettete Indexierung genannt.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nprint(f\"Jean würfelte {messung1['Summe Augen'][2]} Augen\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nJean würfelte 8 Augen\n```\n:::\n:::\n\n\n::: {#wrn-chainedassignment .callout-warning appearance=\"simple\"}\n## Verkettete Indexierung\n\nDie verkettete Indexierung erzeugt in Pandas abhängig vom Kontext eine Kopie des Objekts oder greift auf den Speicherbereich des Objekts zu. Mit Pandas 3.0 wird die verkettete Indexierung nicht mehr unterstützt, das Anlegen einer Kopie wird zum Standard werden. Weitere Informationen erhalten Sie im zitierten Link.\n\n:::: {.border layout=\"[5, 90, 5]\"}\n\n&nbsp;\n\n\"Whether a copy or a reference is returned for a setting operation, may depend on the context. This is sometimes called `chained assignment` and should be avoided. See [Returning a View versus Copy](https://pandas.pydata.org/docs/user_guide/indexing.html#indexing-view-versus-copy).\"\n\n&nbsp;\n\n::::\n\n([Pandas Dokumentation](https://pandas.pydata.org/docs/user_guide/indexing.html))\n:::\n\n### long- und wide-Format\nZweidimensionale Datensätze werden zumeist in einer aus Zeilen und Spalten bestehenden Matrix dargestellt. Den zeilenweise eingetragenen Beobachtungen werden Werte für die in den Spalten organisierten Variablen zugeordnet. Diese Art Daten darzustellen, wird wide-Format genannt: Mit jeder zusätzlich gemessenen Variablen wird der Datensatz breiter.\n\nEine andere Art Daten zu organisieren und über Daten nachzudenken, ist die Darstellung im long-Format. Einige Programme und Pakete erfordern Daten im long-Format oder profitieren zumindest davon beispielsweise bei der Erstellung von Grafiken. Schauen wir uns zunächst noch einmal den Datensatz messung1 im wide-Format an. Welche Beobachtungseinheiten gibt es? Welche Variablen wurden erhoben?\n\n::: {.cell execution_count=7}\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Geburtstag</th>\n      <th>Würfelfarbe</th>\n      <th>Summe Augen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hans</td>\n      <td>26.02.</td>\n      <td>rosa</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Elke</td>\n      <td>14.03.</td>\n      <td>rosa</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Jean</td>\n      <td>30.12.</td>\n      <td>blau</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Maya</td>\n      <td>07.09.</td>\n      <td>gelb</td>\n      <td>23</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n&nbsp;\n\nVermutlich werden Sie davon ausgehen, dass die Beobachtungseinheiten Hans, Elke, Jean und Maya sind und die Variablen Geburtstag, Würfelfarbe und Summe Augen. Es ist aber auch denkbar, dass die Beobachtungseinheit Person mit 0, 1, 2 und 3 kodiert wurde (dem Zeilenindex des Datensatzes) und die Spalte Name ebenfalls eine der erhobenen Variablen ist. Ebenso könnte es nur zwei Variablen, Würfelfarbe und Summe Augen, geben, während die Spalten Name und Geburtstag die beobachteten Personen kodieren. Stellen Sie sich vor, es gäbe eine zweite Person mit dem Namen Hans. Dann könnten die Würfelergebnisse der Personen mit dem Namen Hans nur über den Geburtstag am 26.02. oder 11.11. korrekt zugeordnet werden.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nmessung1 = pd.DataFrame({'Name': ['Hans', 'Elke', 'Jean', 'Maya', 'Hans'], 'Geburtstag': ['26.02.', '14.03.', '30.12.', '07.09.', '11.11.'], 'Würfelfarbe': ['rosa', 'rosa', 'blau', 'gelb', 'rosa'], 'Summe Augen': [12, 17, 8, 23, 7]})\n\nmessung1\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Geburtstag</th>\n      <th>Würfelfarbe</th>\n      <th>Summe Augen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hans</td>\n      <td>26.02.</td>\n      <td>rosa</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Elke</td>\n      <td>14.03.</td>\n      <td>rosa</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Jean</td>\n      <td>30.12.</td>\n      <td>blau</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Maya</td>\n      <td>07.09.</td>\n      <td>gelb</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Hans</td>\n      <td>11.11.</td>\n      <td>rosa</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n&nbsp;\n\nDas long-Format macht diese Überlegungen explizit, indem identifizierende Variablen (identification variables, kurz: id vars) und gemessene Variablen (measure variables oder value vars) unterschieden werden. Die Transformation eines Datensatzes aus dem wide-Format ins long-Format wird melting (schmelzen) genannt. Das Modul Pandas bietet die Funktion `pd.melt(frame, id_vars = None)`. Diese erwartet einen DataFrame. Im optionalen Argument `id_vars` wird angegeben, welche Spalten die identifizierenden Variablen sind.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nmessung1_long = pd.melt(messung1, id_vars = ['Name', 'Geburtstag'])\n\nmessung1_long\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Geburtstag</th>\n      <th>variable</th>\n      <th>value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hans</td>\n      <td>26.02.</td>\n      <td>Würfelfarbe</td>\n      <td>rosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Elke</td>\n      <td>14.03.</td>\n      <td>Würfelfarbe</td>\n      <td>rosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Jean</td>\n      <td>30.12.</td>\n      <td>Würfelfarbe</td>\n      <td>blau</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Maya</td>\n      <td>07.09.</td>\n      <td>Würfelfarbe</td>\n      <td>gelb</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Hans</td>\n      <td>11.11.</td>\n      <td>Würfelfarbe</td>\n      <td>rosa</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Hans</td>\n      <td>26.02.</td>\n      <td>Summe Augen</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Elke</td>\n      <td>14.03.</td>\n      <td>Summe Augen</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Jean</td>\n      <td>30.12.</td>\n      <td>Summe Augen</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Maya</td>\n      <td>07.09.</td>\n      <td>Summe Augen</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Hans</td>\n      <td>11.11.</td>\n      <td>Summe Augen</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n&nbsp;\n\nIm long-Format werden die gemessenen Variablen in der Spalte variable aufgeführt und deren Wert in der Spalte value eingetragen. Mit jeder zusätzlich erhobenen Variablen wird der Datensatz länger.\n\nWenn Sie die Unterscheidung von identifizierenden und gemessenen Variablen zu Ende denken, kann der Variablenname selbst als eine identifizierende Variable für den Wert in der Spalte value aufgefasst werden. Ein Datensatz kann als eine Struktur verstanden werden, die genau eine gemessene Variable, nämlich value, und eine Anzahl identifizierender Variablen besitzt. Dies kann im long-Format wie folgt dargestellt werden.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nmessung1_all_id = pd.melt(messung1, id_vars = ['Name', 'Geburtstag', 'Würfelfarbe'])\n\nmessung1_all_id\n```\n:::\n\n\nIn dieser Darstellung wird beispielsweise der erste Wert 12 durch Name = Hans, Geburtstag = 26.02., Würfelfarbe = rosa und variable = Summe Augen identifiziert.\n\n\n::: {layout=\"[70, 30]\"}\n\n::: {.cell execution_count=11}\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Geburtstag</th>\n      <th>Würfelfarbe</th>\n      <th>variable</th>\n      <th>value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hans</td>\n      <td>26.02.</td>\n      <td>rosa</td>\n      <td>Summe Augen</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Elke</td>\n      <td>14.03.</td>\n      <td>rosa</td>\n      <td>Summe Augen</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Jean</td>\n      <td>30.12.</td>\n      <td>blau</td>\n      <td>Summe Augen</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Maya</td>\n      <td>07.09.</td>\n      <td>gelb</td>\n      <td>Summe Augen</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Hans</td>\n      <td>11.11.</td>\n      <td>rosa</td>\n      <td>Summe Augen</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n![](00-bilder/5f489ffabc91dec1ec2192dc4e993e00.jpg){width=\"90%\" fig-alt=\"dekoratives Bild eines staunenden Hundes\"} \n\n::: \n\n::: {.border}\nMuch wow. Such architecture. von Dmitry Kudryavtsev ist verfügbar unter <https://yieldcode.blog/post/bloat-in-software-engineering/>.\n:::\n\n**Was passiert, wenn auch die Variable `Summe Augen` dem Argument `id_vars` übergeben wird?**\n\n::: {#tip-Antwort-all-id .callout-tip collapse=\"true\"}\n## Antwort\n\nDer Befehl `messung1_all_id = pd.melt(messung1, id_vars = ['Name', 'Geburtstag', 'Würfelfarbe', 'Summe Augen'])` produziert einen leeren Dataframe, weil keine gemessenen Werte verbleiben.\n:::\n\nAuch der umgekehrte Fall ist möglich: Werden beim melting keine id_vars angegeben, werden alle Spalten als gemessene Variablen behandelt.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nmessung1_no_id = pd.melt(messung1)\n\nmessung1_no_id\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>variable</th>\n      <th>value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Name</td>\n      <td>Hans</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Name</td>\n      <td>Elke</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Name</td>\n      <td>Jean</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Name</td>\n      <td>Maya</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Name</td>\n      <td>Hans</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Geburtstag</td>\n      <td>26.02.</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Geburtstag</td>\n      <td>14.03.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Geburtstag</td>\n      <td>30.12.</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Geburtstag</td>\n      <td>07.09.</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Geburtstag</td>\n      <td>11.11.</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Würfelfarbe</td>\n      <td>rosa</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Würfelfarbe</td>\n      <td>rosa</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Würfelfarbe</td>\n      <td>blau</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Würfelfarbe</td>\n      <td>gelb</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Würfelfarbe</td>\n      <td>rosa</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Summe Augen</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>Summe Augen</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Summe Augen</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>Summe Augen</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Summe Augen</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n&nbsp;\n\nDie Umkehroperation zum melting wird casting (gießen) oder pivoting (schwenken) genannt. Dabei wird ein im long-Format vorliegender Datensatz in das wide-Format konvertiert. Die Pandas Funktion `pd.pivot(data, columns, index)` nimmt einen melted DataFrame entgegen und konveriert diesen aus den einzigartigen Werten in columns (= Spaltennamen des DataFrame im wide-Format) und den einzigartigen Werten in index (= Zeilenindex des DataFrame im wide-Format). Wird der Funktion keine Spalte für index übergeben, wird der bestehende Index des melted DataFrame verwendet (der mit 20 Zeilen natürlich viel zu lang ist.) Da das Objekt messung1_no_id keine geeignete Indexspalte besitzt, muss diese vor dem casting erzeugt werden. Dies ist mit der Methode `messung1_no_id.groupby('variable').cumcount()` möglich, die die Anzahl jeder Ausprägung in der übergebenen Spalte bei 0 beginnend durchzählt. (Ein direktes Ersetzen des Index ist auf diese Weise nicht möglich, da der Index des an `pd.pivot(data, columns, index)` übergebenen DataFrames keine Doppelungen enthalten darf.)\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n# pd.pivot() benötigt einen Index oder benutzt den bestehenden Index, des melted_df, der zu lang ist\n# Deshalb eine zusätzliche Spalte in messung1_no_id einfügen\n## einfach: messung1_no_id['new_index'] = list(range(0, 5)) * 4 \n## allgemein: messung1_no_id['new_index'] = messung1_no_id.groupby('variable').cumcount()\n\n# Spalte new_index einfügen\nmessung1_no_id['new_index'] = messung1_no_id.groupby('variable').cumcount()\nprint (f\"Der Datensatz im long-Format mit zusätzlicher Spalte new_index:\\n{messung1_no_id}\")\n\n# casting\nmessung1_cast = pd.pivot(messung1_no_id, index = 'new_index', columns = 'variable', values = 'value')\nprint(f\"\\nDer Datensatz im wide-Format:\\n{messung1_cast}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDer Datensatz im long-Format mit zusätzlicher Spalte new_index:\n       variable   value  new_index\n0          Name    Hans          0\n1          Name    Elke          1\n2          Name    Jean          2\n3          Name    Maya          3\n4          Name    Hans          4\n5    Geburtstag  26.02.          0\n6    Geburtstag  14.03.          1\n7    Geburtstag  30.12.          2\n8    Geburtstag  07.09.          3\n9    Geburtstag  11.11.          4\n10  Würfelfarbe    rosa          0\n11  Würfelfarbe    rosa          1\n12  Würfelfarbe    blau          2\n13  Würfelfarbe    gelb          3\n14  Würfelfarbe    rosa          4\n15  Summe Augen      12          0\n16  Summe Augen      17          1\n17  Summe Augen       8          2\n18  Summe Augen      23          3\n19  Summe Augen       7          4\n\nDer Datensatz im wide-Format:\nvariable  Geburtstag  Name Summe Augen Würfelfarbe\nnew_index                                         \n0             26.02.  Hans          12        rosa\n1             14.03.  Elke          17        rosa\n2             30.12.  Jean           8        blau\n3             07.09.  Maya          23        gelb\n4             11.11.  Hans           7        rosa\n```\n:::\n:::\n\n\nDas Ergebnis entspricht noch nicht dem ursprünglichen Datensatz im wide-Format. Um das Ausgangsformat wiederherzustellen, müssen die Spalten in die ursprüngliche Reihenfolge gebracht sowie der Index und dessen Beschriftung zurückgesetzt werden.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\n# Spalten anordnen, Index zurücksetzen\nmessung1_cast = messung1_cast[['Name', 'Geburtstag', 'Würfelfarbe', 'Summe Augen']]\nmessung1_cast.reset_index(drop = True, inplace = True)\nmessung1_cast.rename_axis(None, axis = 1, inplace = True)\n\nprint(f\"\\nDer Datensatz im wide-Format mit zurückgesetztem Index:\\n\\n{messung1_cast}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nDer Datensatz im wide-Format mit zurückgesetztem Index:\n\n   Name Geburtstag Würfelfarbe Summe Augen\n0  Hans     26.02.        rosa          12\n1  Elke     14.03.        rosa          17\n2  Jean     30.12.        blau           8\n3  Maya     07.09.        gelb          23\n4  Hans     11.11.        rosa           7\n```\n:::\n:::\n\n\n::: {#tip-idvars .callout-tip collapse=\"false\"}\n## identifizierende und gemessene Variablen\n\nAuch wenn Sie mit Datensätzen im wide-Format arbeiten, ist die Unterscheidung identifizierender und gemessener Variablen nützlich, um Datensätze zu organisieren. [siehe @sec-tidydata]\n\n:::\n\n### Übung zweidimensionale Datensätze\nOben wurde das Objekt messung1_long mit dem Befehl `messung1_long = pd.melt(messung1, id_vars = ['Name', 'Geburtstag'])` angelegt.  \n**Benutzen Sie die Funktion** `pd.DataFrame.pivot()`, **um den Datensatz messung1 wieder ins wide-Format zu transformieren.**\n\n::: {.cell execution_count=15}\n\n::: {.cell-output .cell-output-display execution_count=15}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Geburtstag</th>\n      <th>variable</th>\n      <th>value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hans</td>\n      <td>26.02.</td>\n      <td>Würfelfarbe</td>\n      <td>rosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Elke</td>\n      <td>14.03.</td>\n      <td>Würfelfarbe</td>\n      <td>rosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Jean</td>\n      <td>30.12.</td>\n      <td>Würfelfarbe</td>\n      <td>blau</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Maya</td>\n      <td>07.09.</td>\n      <td>Würfelfarbe</td>\n      <td>gelb</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Hans</td>\n      <td>11.11.</td>\n      <td>Würfelfarbe</td>\n      <td>rosa</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Hans</td>\n      <td>26.02.</td>\n      <td>Summe Augen</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Elke</td>\n      <td>14.03.</td>\n      <td>Summe Augen</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Jean</td>\n      <td>30.12.</td>\n      <td>Summe Augen</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Maya</td>\n      <td>07.09.</td>\n      <td>Summe Augen</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Hans</td>\n      <td>11.11.</td>\n      <td>Summe Augen</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#tip-pivoting .callout-tip collapse=\"true\"}\n## Musterlösung zweidimensionale Datensätze\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n# Spalte new_index einfügen\nmessung1_long['new_index'] = messung1_long.groupby('variable').cumcount()\n\n# casting\nmessung1_long_cast = pd.pivot(messung1_long, index = 'new_index', columns = 'variable', values = 'value')\n\n# Spalten anordnen, Index zurücksetzen\nmessung1_long_cast = messung1_cast[['Name', 'Geburtstag', 'Würfelfarbe', 'Summe Augen']]\nmessung1_long_cast.reset_index(drop = True, inplace = True)\nmessung1_long_cast.rename_axis(None, axis = 1, inplace = True)\n\nmessung1_long_cast\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Geburtstag</th>\n      <th>Würfelfarbe</th>\n      <th>Summe Augen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hans</td>\n      <td>26.02.</td>\n      <td>rosa</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Elke</td>\n      <td>14.03.</td>\n      <td>rosa</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Jean</td>\n      <td>30.12.</td>\n      <td>blau</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Maya</td>\n      <td>07.09.</td>\n      <td>gelb</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Hans</td>\n      <td>11.11.</td>\n      <td>rosa</td>\n      <td>7</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n:::\n\n### Drei- und mehrdimensionale Datensätze\nDrei- oder mehrdimensionale Datensätze organisieren komplexe Datenstrukturen in sogenannten **Arrays**. Arrays sind n-dimensionale Datenstrukturen und damit zugleich ein Oberbegriff. So ist eine Liste ein eindimensionales Array, eine Matrix ein zweidimensionales Array und eine Excel-Datei mit mehreren Arbeitsblättern für jährlich erhobene Umfragedaten ein 3-dimensionales Array (Arbeitsblätter, Zeilen, Spalten). Abhängig vom verwendeten Modul können Arrays ein oder mehrere Datentypen enthalten.\n\n<!-- ggf. ergänzen: Modul xarray <https://docs.xarray.dev/en/stable/user-guide/pandas.html> -->\n\n::: {.border}\n![dreidimensionale Datensätze](00-bilder/dreidimensionaler-datensatz-slicing-mf-mp.png){width=\"50%\" fig-alt=\"Dargestellt ist ein dreidimensionaler Block, der einen dreidimensionalen Datensatz repräsentiert. Pfeile repräsentieren die drei Achsen. Die nullte Achse entspricht der Tiefe, die erste Achse der Länge (von oben nach unten) und die zweite Achse der Breite des Datensatzes.\"}\n\nslicing von Marc Fehr ist lizensiert unter [CC-BY-4.0](https://github.com/bausteine-der-datenanalyse/w-python-numpy-grundlagen#CC-BY-4.0-1-ov-file) und abrufbar auf [GitHub](https://github.com/bausteine-der-datenanalyse/w-python-numpy-grundlagen). Die Grafik wurde auf den gezeigten Teil beschnitten und die obenstehende Beschriftung entfernt. 2024\n\n:::\n\n&nbsp;\n\nFür drei- und mehrdimensionale Datenstrukturen werden häufig spezialisierte Datenformate verwendet, die im Abschnitt @sec-spezialformate behandelt werden. Dies hat unter anderem den Grund, dass so leichter verschiedene Datentypen verarbeitet und mit Metadaten (siehe @sec-metadaten) dokumentiert werden können.\n\n#### Bilddaten einlesen\n\n::: {.border}\nDigitale Bilder liegen in Form eines dreidimensionalen Datensatzes vor. In Zeilen und Spalten liegen für jeden Pixel Farbwerte (Rot, Grün, Blau) und gegebenenfalls ein Alphawert vor (Rot, Grün, Blau, Alpha). Die Farbwerte liegen entweder im Bereich von 0 bis 1 oder von 0 bis 255 (8-Bit).\n\n```\n# Farbwerte für einen Pixel\n[Rotwert, Grünwert, Blauwert]\n\n# Eine Bildzeile mit drei Pixeln\n[[Rotwert, Grünwert, Blauwert], [Rotwert, Grünwert, Blauwert], [Rotwert, Grünwert, Blauwert]]\n\n# Ein Bild aus drei Zeilen und Spalten\n[[[Rotwert, Grünwert, Blauwert], [Rotwert, Grünwert, Blauwert], [Rotwert, Grünwert, Blauwert]],\n[[Rotwert, Grünwert, Blauwert], [Rotwert, Grünwert, Blauwert], [Rotwert, Grünwert, Blauwert]],\n[[Rotwert, Grünwert, Blauwert], [Rotwert, Grünwert, Blauwert], [Rotwert, Grünwert, Blauwert]]]\n```\n\nBilddateien können mit der Funktion `plt.imread()` aus dem Modul `matplotlib.pyplot` eingelesen werden. \n\n:::: {.border}\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nlogo = plt.imread(fname = '00-bilder/python-logo-and-wordmark-cc0-tm.png')\n\nplt.imshow(logo)\n```\n\n::: {.cell-output .cell-output-display}\n![](einlesen-strukturierter-datensaetze_files/figure-pdf/cell-18-output-1.png){fig-pos='H'}\n:::\n:::\n\n\nPython Logo von Python Software Foundation steht unter der [GPLv3](https://www.gnu.org/licenses/gpl-3.0.html). Die Wort-Bild-Marke ist markenrechtlich geschützt: <https://www.python.org/psf/trademarks/>. Das Werk ist abrufbar auf [wikimedia](https://de.m.wikipedia.org/wiki/Datei:Python_logo_and_wordmark.svg). 2008\n\n:::: \n\n&nbsp;\n\nDie Struktur des Datensatzes kann mit dem Attribut `.shape` abgerufen werden.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nprint(type(logo), \"\\n\")\n\nprint(logo.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'numpy.ndarray'> \n\n(144, 486, 4)\n```\n:::\n:::\n\n\nDie Daten wurden als NumPy.ndarray eingelesen. Das Logo hat 144 Zeilen, 486 Spalten und liegt im RGBA-Farbraum vor. Ein Ausschnitt der Daten sieht so aus:\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nprint(logo[50:52, 50:52, : ])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[[0.21568628 0.44705883 0.63529414 1.        ]\n  [0.21568628 0.44705883 0.63529414 1.        ]]\n\n [[0.21568628 0.44705883 0.63529414 1.        ]\n  [0.21176471 0.44313726 0.6313726  1.        ]]]\n```\n:::\n:::\n\n\n[@Arnold-2023-numpy-dateien]\n\n:::\n\n### Übung dreidimensionale Datensätze\nÜber den Index der dritten Dimension können die Farbkanäle Rot, Grün und Blau ausgewählt und mit der Funktion `plt.imshow(cmap = 'Greys_r')` einzeln dargestellt werden. Das Argument `cmap = 'Greys_r'` weist die Funktion an, die invertierte Grauskala benutzen. Dadurch werden hohe Farbwerte hell und niedrige Farbwerte dunkel dargestellt. **Stellen Sie die Farbkanäle Rot, Grün und Blau des Pythonlogos einzeln mit der Funktion `plt.imshow(cmap = 'Greys_r')` dar.**\n\n::: {#tip-logo .callout-tip collapse=\"true\"}\n## Musterlösung dreidimensionale Datensätze\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nkanal = [\"Rotkanal\", \"Grünkanal\", \"Blaukanal\"]\n\nplt.figure(figsize = (9, 6))\n\nfor i in range(3):\n\n  plt.subplot(1, 4, i + 1)\n  plt.imshow(logo[ :, :, i], cmap = 'Greys_r')\n  plt.title(label = kanal[i])\n\nplt.colorbar(shrink = 0.15)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Farbkanäle des Pythonlogos](einlesen-strukturierter-datensaetze_files/figure-pdf/cell-21-output-1.png){fig-alt='Dargestellt sind die drei Farbkanäle des Pythonlogos.' fig-pos='H'}\n:::\n:::\n\n\nMöglicherweise wundern Sie sich, warum der Bildhintergrund in jedem Farbkanal schwarz ist. Die Ursache finden Sie im nächsten Tipp.\n\n:::: {#tip-logo .callout-tip collapse=\"true\"}\n## Erklärung Bildhintergrund\nDer Bildhintergrund hat in allen Kanälen, auch im Alphakanal, den Farbwert 0. Dieser Teil des Bildes ist deshalb vollständig transparent und wird vom Hintergrund der Internetseite ausgefüllt. Der Bildhintergrund des Logos wirkt deshalb weiß.\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\n# Alphakanal\nplt.imshow(logo[ :, :, 3], cmap = 'Greys_r')\nplt.title(label = 'Alphakanal')\nplt.colorbar(shrink = 0.4)\n\nplt.show()\n\n# Die ersten zwei Zeilen und Spalten des Logos\nprint(logo[0:2, 0:2, : ])\n```\n\n::: {.cell-output .cell-output-display}\n![Alphakanal des Pythonlogos](einlesen-strukturierter-datensaetze_files/figure-pdf/cell-22-output-1.png){fig-alt='Dargestellt ist der Alphakanal des Pythonlogos. Der Bildhintergrund hat den Farbwert 0.' fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[[[0. 0. 0. 0.]\n  [0. 0. 0. 0.]]\n\n [[0. 0. 0. 0.]\n  [0. 0. 0. 0.]]]\n```\n:::\n:::\n\n\n::::\n:::\n\n## Datentyp {#sec-datentyp}\nDer Datentyp gibt an, wie die in einem Datensatz einhaltenen Werte von Python interpretiert werden sollen. Beispielsweise kann der Wert \"1\" ein Zeichen, eine Ganzzahl, einen Wahrheitswert, den Monat Januar oder die Ausprägung einer kategorialen Variablen repräsentieren. Python unterstützt als vielseitig einsetzbare Programmiersprache zahlreiche Datentypen, die den Kategorien: numerics, sequences, mappings, classes, instances and exceptions zugeordnet sind. Nähere Informationen dazu finden Sie in der [Dokumentation](https://docs.python.org/3/library/stdtypes.html).\n\n::: {.border}\n![Datentypen in Python](00-bilder/python3-standard-type-hierarchy.png){width=\"60%\" fig-alt=\"Dargestellt ist eine Kategorisierung der Standardtypen in Python. Die Kategorisierung ist nicht vollständig deckungsgleich zu den in der Dokumentation genannten Kategorien von Datentypen. Der Typ None für Nullwerte hat keine weitere Unterteilung. Die Kategorie Numbers unterteilt sich in Zahlenwerte (Ganzzahlen, boolsche Wahrheitswerte), reele Zahlen (floats) und komplexe Zahlen. Die Kategorie Sequences unterteilt sich in Unveränderliche (Strings, Tuple, Bytes) und Veränderliche (Listen, Byte Arrays). Die Kategorie Set Types unterteilt sich in Sets (Mengen) und Frozen Sets. Die Kategorie Mappings enthält Dictionaries (Wörterbücher). Die Kategorie Callable umfasst Funktionen, Methoden und Klassen. Außerdem gibt es die Kategorie Module.\"}\n\nPython 3. The standard type hierarchy. von Максим Пе ist lizensiert unter [CC BY SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/deed.de) und abrufbar auf [wikimedia](https://commons.wikimedia.org/wiki/File:Python_3._The_standard_type_hierarchy.png). 2018\n:::\n\n&nbsp;\n\nDurch Module werden weitere Datentypen hinzugefügt. In der Datenanalyse häufig verwendete Datentypen sind:\n\n  - Zahlen: Ganzzahl, Fließkommazahlen\n\n  - Wahrheitswerte\n\n  - Zeichenketten\n\n  - Datums- und Uhrzeitangaben\n\n  - Kategorie<!-- Faktor in R--> (aus dem Modul [Pandas](https://pandas.pydata.org/docs/user_guide/categorical.html))\n\nDer Datentyp bestimmt zum einen den zulässigen Wertebereich einer Variablen. Beispielsweise sind 0 und 13 zulässige Ganzzahlen, aber keine gültigen Kodierungen des Monats. Zum anderen definiert der Datentyp, welche Operationen mit den Werten zulässig sind und wie diese von Python ausgeführt werden. Dies betrifft Operatoren und Funktionen. Python enthält Funktionen, um den Datentyp eines Werts zu bestimmen und ggf. umzuwandeln (siehe w-Python).\n\n::: {#nte-operation-nach-datentyp .callout-note}\n# Datentypabhängige Operationen und Funktionen\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\n# Der Operator + bewirkt die Addition von Zahlen\nprint(1 + 13)\n\n# Der Operator + bewirkt auch das Verketten von strings\nprint(str(1) + str(13))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n14\n113\n```\n:::\n:::\n\n\nDie Sortierfunktion arbeitet abhängig vom Datentyp.\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\n# Liste von Monatskürzeln erstellen\ndates = pd.Series([ '07.06.2000', '12.01.2000', '11.02.2000', '04.09.2000', '10.03.2000', '03.10.2000', '09.04.2000', '08.05.2000', '06.07.2000', '05.08.2000', '02.11.2000', '01.12.2000'])\ndates = pd.to_datetime(dates, format = '%d.%m.%Y');\n\nprint(f\"Eine unsortierte Liste von Monatskürzeln:\\n{list(dates.dt.strftime('%b'))}\")\n\nprint(f\"\\nDie Liste alphabetisch sortiert:\\n{sorted(list(dates.dt.strftime('%b')))}\")\n\nprint(f\"\\nDie Liste als datetime-Objekt sortiert:\\n{list(dates.sort_values().dt.strftime('%b'))}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEine unsortierte Liste von Monatskürzeln:\n['Jun', 'Jan', 'Feb', 'Sep', 'Mar', 'Oct', 'Apr', 'May', 'Jul', 'Aug', 'Nov', 'Dec']\n\nDie Liste alphabetisch sortiert:\n['Apr', 'Aug', 'Dec', 'Feb', 'Jan', 'Jul', 'Jun', 'Mar', 'May', 'Nov', 'Oct', 'Sep']\n\nDie Liste als datetime-Objekt sortiert:\n['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n```\n:::\n:::\n\n\n:::\n\n::: {#tip-datatype .callout-tip collapse=\"false\"}\n## Datentyp kontollieren und plausibilisieren\nBeim Einlesen von Datensätzen ist es wichtig, die korrekte Erkennung der Datentypen zu kontrollieren bzw. aktiv zu steuern. Weitere Methoden für die formale Prüfung des Datentyps und für die Kontrolle des Wertebereichs werden in \n[@sec-numpypandas] vorgestellt.\n::: \n\n### Fehlende Werte {#sec-missing}\nEin besonderer Datentyp ist der zur Repräsentation fehlender Werte. In Python wird zwischen nicht existenten und nicht definierten Werten unterschieden.\n\n#### Nullwert None\nDer sogenannte Nullwert in Python ist `None`, das zu den definierten Schlüsselwörtern in Python gehört.\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nprint(type(None))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'NoneType'>\n```\n:::\n:::\n\n\n`None` repräsentiert nicht existente Werte und Objekte. Leere (aber existente) Objekte gehören nicht zum Datentyp `None`.\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nleere_liste = []\nleere_liste == None\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\nFalse\n```\n:::\n:::\n\n\n`None` kann Funktionen als Argument übergeben oder von diesen als Rückgabewert ausgegeben werden. Operationen sind mit `None` jedoch nicht möglich. \n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\n# Operationen mit None führen zu Fehlermeldungen\ntry:\n  print(None + 1)\nexcept TypeError as error:\n  print(\"Der übergebene Wert führt zu der Fehlermeldung:\\n\", error)\nelse:\n  print(None + 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDer übergebene Wert führt zu der Fehlermeldung:\n unsupported operand type(s) for +: 'NoneType' and 'int'\n```\n:::\n:::\n\n\nEine Ausnahme ist die Umwandlung in eine Zeichenkette.\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\n# Eine Ausnahme ist die Umwandlung in strings\na = None\nprint(\"\\nprint(a) gibt den Nullwert zurück:\\n\", a, sep = \"\")\n\nprint(\"\\nstr(a) gibt eine Zeichenkette zurück:\")\nstr(a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nprint(a) gibt den Nullwert zurück:\nNone\n\nstr(a) gibt eine Zeichenkette zurück:\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\n'None'\n```\n:::\n:::\n\n\n#### NaN\nUm mit fehlenden Werten innerhalb eines Datensatzes arbeiten zu können, gibt es den Wert `NaN`, der zur Klasse der Fließkommazahlen gehört. `NaN` steht für Not a Number und repräsentiert undefinierte oder nicht darstellbare Werte. Beispielsweise berechnet die Methode `pd.diff()` die Differenz jedes Werts zu seinem Vorgänger. Da der erste Wert keinen Vorgänger hat, wird `NaN` erzeugt.\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\nmy_series = pd.Series([1, 2, 4, 8])\nmy_series.diff()\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```\n0    NaN\n1    1.0\n2    2.0\n3    4.0\ndtype: float64\n```\n:::\n:::\n\n\nAnders als `None` ist `NaN` kein Standardschlüsselwort in Python. Der Wert `NaN` wird erzeugt mit `float('nan')` oder `float('NaN')`, die Groß- und Kleinschreibung spielt keine Rolle. `NaN` hat also den Datentyp Fließkommazahl. Die Module math und NumPy bieten mit `math.nan` und `np.nan` ebenfalls Funktionen, um `NaN` zu erzeugen.\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\nprint(type(float('NaN')))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'float'>\n```\n:::\n:::\n\n\nMit dem Wert 'NaN' können Operationen ausgeführt werden. Das Ergebnis ist immer `NaN`.\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\nprint(float('NaN') + 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnan\n```\n:::\n:::\n\n\nEinige Funktionen können mit `NaN` als Platzhalter für fehlende Werte umgehen.\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\n# Python-Basis\nprint(\"sum():\", sum([1, 2, float('NaN'), 4]), \"\\n\")\nprint(\"max():\", max([1, 2, float('NaN'), 4]), \"\\n\")\nprint(\"any():\", any([1, 2, float('NaN'), 4]), \"\\n\")\n\n# Pandas\ndaten_mit_nan = pd.Series([1, 2, float('NaN'), 4])\nprint(daten_mit_nan + 1)\nprint(\"\\nSumme des Datensates:\", daten_mit_nan.sum())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nsum(): nan \n\nmax(): 4 \n\nany(): True \n\n0    2.0\n1    3.0\n2    NaN\n3    5.0\ndtype: float64\n\nSumme des Datensates: 7.0\n```\n:::\n:::\n\n\n::: {#wrn-logicbasepython .callout-warning appearance=\"simple\" collapse=\"false\"}\n## Achtung Logik!\n\nDie logische Abfrage fehlender Werte unterscheidet sich für `None` und `NaN`. \n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\nbool_values = [None, float('NaN')]\n\nfor element in bool_values:\n  bool_value = bool(element)\n  print(\"Wahrheitswert von\", element, \"ist\", bool_value)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWahrheitswert von None ist False\nWahrheitswert von nan ist True\n```\n:::\n:::\n\n\nDies gilt auch für die Wertgleichheit.\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\nfor element in bool_values:\n  result = element == element\n  print(\"Wertgleichheit von\", element, \"ist\", result)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWertgleichheit von None ist True\nWertgleichheit von nan ist False\n```\n:::\n:::\n\n\n:::\n\n### Fehlende Werte in der Praxis\n`None` und `NaN` sind pythonspezifische Repräsentationen für nicht existente oder nicht definierte Werte. In der Praxis werden fehlende Werte in Datensätzen auf unterschiedliche Weise gekennzeichnet. \n\nIn Datensätzen übliche Werte sind:\n\n  - kein Eintrag, beispielsweise in kommaseparierten Dateien eine leere Zeichenkette `\"\"`\n\n  - definierte Zeichenfolge: `NA` in der Programmiersprache R, `NULL` in der Datenbanksprache SQL, `.` in der Statistik-Software Stata\n  \n  - (mehrere) manuell gewählte Zeichen oder Ziffern außerhalb des zulässigen Wertebereichs wie -1, -88, -99 (häufig bei Umfragedaten)\n\nDie Art der Kennzeichnung ist jeweils mit Vor- und Nachteilen verbunden. Eine definierte Zeichenfolge für fehlende Werte hilft dabei, Lücken im Datensatz von Fehlern bei der Datenerfassung zu unterscheiden. Dazu ist eine definierte Zeichenfolge wie \"NA\" besser als eine leere Zeichenkette geeignet. Manuell gewählte Werte erlauben es, bei der automatischen Auswertung eines Datensatzes abhängig von der Situation ein bestimmtes Verhalten für jede Variable festzulegen (z. B. Unterscheidung von nicht zutreffend, Aussage verweigert, weiß nicht, Interview abgebrochen keine Antwort). \n\n::: {#tip-missingvalues .callout-tip collapse=\"false\"}\n## fehlende Werte\n\nDie Identifizierung und ggf. Bereinigung fehlender Werte ist ein wichtiger Schritt beim Einlesen strukturierter Datensätze. Dabei hilft es, die gängigen Kennzeichnungen für fehlende Werte zu kennen und sich über die Konventionen des jeweiligen Dateiformats bzw. der jeweiligen Disziplin zu informieren. Dennoch ist manchmal ein gewisser Spürsinn unerlässlich. Geeignete Funktionen zur Identifizierung fehlender Werte werden in @sec-numpypandas vorgestellt.\n\n:::\n\n## Metadaten {#sec-metadaten}\nMetadaten sind beschreibende Informationen eines Datensatzes. Metadaten geben beispielsweise an:\n\n  - welche Datentypen ein Datensatz enthält,\n\n  - verwendete Kodierschemen, Skalen oder mimimal und maximal zulässige Werte,\n\n  - die Bedingungen, unter denen die Daten erhoben wurden,\n\n  - Herkunft der Daten,\n\n  - Beziehungen zwischen Variablen und Datensätzen,\n  \n  - urheberrechtliche Informationen und Lizenzhinweise.\n\n(vgl. [The HDF Group Help Desk](https://docs.hdfgroup.org/archive/support/HDF5/doc/Advanced/HDF5_Metadata/index.html))\n\nSpezialisierte Dateiformate wie netCDF oder HDF deklarieren Metadaten explizit in dafür vorgesehenen Feldern. Vielen Dateiformaten fehlt eine solche Funktion. Relevante Metadaten stehen deshalb häufig im Dateinamen, in Spaltenbeschriftungen, in zusätzlichen Tabellenblättern oder in separaten Dokumenten (die nicht immer zur Verfügung stehen).\n\n::: {#tip-metadata .callout-tip collapse=\"false\"}\n## Metadaten\nInsbesondere vor dem Einlesen komplexer Datensätze sollten Begleitmaterialien, sofern vorhanden, studiert werden. \n\n:::\n\n## Tidy data {#sec-tidydata}\nDatensätze werden mit verschiedenen Zielstellungen angelegt, etwa dass eine bequeme Dateneingabe möglich ist. Dies führt aber häufig dazu, dass Datensätze für die skriptbasierte Datenanalyse zunächst aufwändig aufgeräumt werden müssen.\n\n::: {.border layout=\"[5, 90, 5]\"}\n\n&nbsp;\n\n“Tidy datasets are all alike, but every messy dataset is messy in its own way.” [@R-for-Data-Science, Kapitel 5 Data tidying]\n\n&nbsp;\n\n:::\n\nTidy data ist ein System von Hadley Wickham, das dabei hilft, Datensätze in ein aufgeräumtes (tidy) Format zu bringen. Das Aufräumen von Datensätzen ist eine vorbereitende Tätigkeit mit dem Ziel, während der eigentlichen Datenanlyse möglichst wenig Zeit für das Umformen von Datenstrukturen aufwenden zu müssen. Dadurch soll ein größerer Fokus auf den inhaltlichen Aspekt der Datenanalyse ermöglicht werden. [@R-for-Data-Science, Kapitel 5 Data tidying]\n\n::: {#imp-tidy-data .callout-important}\n## tidy data\n\n:::: {.border layout=\"[[5, 90, 5], [1]]\"}\n\n&nbsp;\n\n::::: {}\n\"Das System tidy data besteht aus drei Regeln:\n\n1. Jede Variable ist eine Spalte; jede Spalte ist eine Variabe.\n\n2. Jede Beobachtung ist eine Zeile; jede Zeile ist eine Beobachtung.\n\n3. Jeder Wert ist eine Zelle; jede Zelle ist ein einzelner Wert.\"\n:::::\n\n&nbsp;\n\n[@R-for-Data-Science, Kapitel 5 Data tidying, eigene Übersetzung]\n::::\n:::\n\nTidy data bezieht sich auf zweidimensionale Datensätze, bietet aber auch darüber hinaus eine Orientierung, um unterschiedlich aufgebaute Datensätze strukturiert einzulesen und für die Datenanalyse vorzubereiten. Tidy data ist kein strikt zu befolgendes Regelwerk. Es ist völlig in Ordnung, eine andere Struktur zu wählen, wenn die Datenanalyse damit leichter durchgeführt werden kann.\n\n# Die Module NumPy und Pandas {#sec-numpypandas}\nDie Module NumPy und Pandas erlauben ein effizientes Arbeiten mit Datensätzen. Insbesondere das Lesen- und Schreiben von Dateien und die Verwaltung von Datentypen ist erheblich einfacher als mit der Python-Basis. Außerdem sind die vektorisierten Operationen vielfach schneller als Operationen mit Python. Das Modul Pandas basiert auf NumPy. In den folgenden Abschnitten werden beide Module behandelt.\n\nEine kurze Übersicht der Vor- und Nachteile:\n\n  * [NumPy](https://bausteine-der-datenanalyse.github.io/w-python-numpy-grundlagen/output/book/): n-dimensionale Array-Struktur mit Unterstützung der am häufigsten verwendeten Datentypen sowie zahlreicher numerischer Formate für spezialisierte wissenschaftliche Berechnungen ([siehe Dokumentation](https://numpy.org/devdocs/reference/arrays.scalars.html)). Ein Array kann immer nur einen Datentyp haben und die Größe von Arrays ist unveränderlich. Dafür werden Operationen etwas schneller als in der DataFrame-Struktur von Pandas ausgeführt.\n\n    - Spaltennamen sind mit einem strukturierten dtype möglich ([siehe Dokumentation](https://numpy.org/doc/stable/user/basics.io.genfromtxt.html#setting-the-names))\n\n  * [Pandas](https://bausteine-der-datenanalyse.github.io/w-pandas/output/book/): 2-dimensionale DataFrame-Struktur im long- und wide-Format. DataFrames können mehrere Datentypen enthalten und die Größe von DataFrames ist veränderlich. Unterstützung von alphanummerischen Spalten- und Indexbeschriftungen. Direktes Abrufen von Dateien aus dem Internet möglich.\n\n    - dreidimensionale DataFrames sind mit einem Multiindex möglich --> das widerspricht aber dem Konzept von Tidy Data\n\nFür beide Module haben sich diese Kürzel etabliert:\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\n\n# Deklarieren der Anzahl der Nachkommastellen\npd.set_option(\"display.precision\", 2)\n```\n:::\n\n\n::: {#tip-pypandas .callout-tip collapse=\"false\"}\n## Arbeiten mit NumPy und Pandas\nOb Sie mit NumPy oder mit Pandas arbeiten, hängt von dem vorliegenden Datensatz und persönlichen Präferenzen ab. \n\nDas Paket Pandas erlaubt es, Daten aus verschiedenen Quellen wie CSV-Dateien oder Excel-Tabellen und mit unterschiedlichen Datentypen in einen DataFrame zu laden. Anschließend können diese mit wenigen Befehlen untersucht und umstrukturiert werden. Komplexe Operationen wie das Umformen von Datensätzen, das Gruppieren und Aggregieren von Daten sowie das Filtern und Sortieren sind effizient möglich.\n\nBis auf wenige Ausnahmen sind Pandas und NumPy zueinander kompatibel. Es spricht nichts dagegen, Ihre Daten mit Pandas vorzubereiten und anschließend mit NumPy auszuwerten.\n\n:::\n\n## Datentypen\nNumPy unterstützt folgende Datentypen:\n\n|      Datentyp NumPy-Array  |      Datentyp in Python |\n|---|---|\n|     int_    |     int    |\n|     double    |     float    |\n|     cdouble    |     complex    |\n|     bytes_    |     bytes    |\n|     str_    |     str    |\n|     bool_    |     bool    |\n|     datetime64    |     datetime.datetime    |\n|     timedelta64    |     datetime.timedelta    |\n\n[Dokumentation NumPy](https://numpy.org/devdocs/reference/arrays.scalars.html)\n\nIn den meisten Fällen verwendet das Modul Pandas die NumPy-Datentypen. Pandas führt aber auch einige zusätzliche Datentypen ein. Eine vollständige Liste finden Sie in der [Pandas Dokumentation](https://pandas.pydata.org/docs/reference/arrays.html). Die wichtigsten zusätzlichen Datentypen sind:\n\n  - [Kategorie](https://pandas.pydata.org/docs/user_guide/categorical.html) `dtype = 'category'`\n\n  - [Zeitzonenbewusstes Datumsformat](https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.html#pandas.Timestamp) `dtype = 'datetime64[ns, US/Eastern]'`\n\n## Dateien lesen und schreiben\nIn den Werkzeugbausteinen NumPy und Pandas haben Sie die Funktionen zum Lesen und Schreiben von Dateien kennengelernt.\n\n::: {.panel-tabset}\n## NumPy\nIn NumPy können Dateien mit der Funktion `np.loadtxt()` gelesen und mit der Funktion `np.savetxt()` geschrieben werden. \n\n  - `np.loadtxt(fname = data.txt, delimiter = \";\", skiprows= #Reihen)`  \n\n  - `np.savetxt(fname = dateipfad, X = daten, header = kommentar, fmt='%5.2f')`\n\n## Pandas\nIn Pandas werden Dateien mit einer Reihe spezialisierter Funktionen gelesen und geschrieben, die einem einheitlichen Schema folgen. Funktionen zum Lesen von Dateien werden in der Form `pd.read_csv` und Funktionen zum Schreiben in der Form `pd.to_csv` aufgerufen. Mit Pandas können auch Dateien aus dem Internet abgerufen werden `pd.read_csv(URL)`.\n\n:::: {.border}\n| Format Type | Data Description | Reader | Writer |\n|:---:|:---:|:---:|:---:|\n| text | CSV | read_csv | to_csv |\n| text | Fixed-Width Text File | read_fwf | NA |\n| text | JSON | read_json | to_json |\n| text | HTML | read_html | to_html |\n| text | LaTeX | Styler.to_latex | NA |\n| text | XML | read_xml | to_xml |\n| text | Local clipboard | read_clipboard | to_clipboard |\n| binary | MS Excel | read_excel | to_excel |\n| binary | OpenDocument | read_excel | NA |\n| binary | HDF5 Format | read_hdf | to_hdf |\n| binary | Feather Format | read_feather | to_feather |\n| binary | Parquet Format | read_parquet | to_parquet |\n| binary | ORC Format | read_orc | to_orc |\n| binary | Stata | read_stata | to_stata |\n| binary | SAS | read_sas | NA |\n| binary | SPSS | read_spss | NA |\n| binary | Python Pickle Format | read_pickle | to_pickle |\n| SQL | SQL | read_sql | to_sql |\n\n([Pandas Dokumentation](https://pandas.pydata.org/docs/user_guide/io.html))\n::::\n::: \n\n## Datentypen erkennen und festlegen\nDer Datentyp bestimmt, wie bereits ausgeführt, den zulässigen Wertebereich einer Variablen, zulässige Operationen und die Ausführung von Operatoren und Funktionen in Python. Die Module NumPy und Pandas bieten eine Reihe von Funktionen, um den Datentyp von Variablen zu kontrollieren und festzulegen. \n\n*Hinweis: Der Datentyp datetime wird in [@sec-zeitreihen] behandelt.*\n\n### NumPy\nMit NumPy kann der Datentyp eines Arrays beim Einlesen einer Datei mit dem Argument `dtype` festgelegt werden `np.loadtxt(fname = data.txt, dtype = 'float')`. Das Argument `dtype` akzeptiert die Angabe eines Datentyps, Schlüsselwörter oder Kürzel. Weiter Informationen erhalten Sie in der [NumPy Dokumentation](https://numpy.org/doc/stable/reference/arrays.dtypes.html).\n\n| Datentyp | Schlüsselwort | Kürzel | dtype |\n|---|---|---|---|\n| Fließkommazahl | float | f8 | float64 |\n| Ganzzahl | int | i | int32 |\n| Wahrheitswert | bool | ? | bool |\n| Datum | datetime64 | M | datetime64 |\n| Zeichenkette | str | U | U + Ziffer zur Angabe der benötigten Bytes |\n\nDer Datentyp eines Arrays kann mit dem Attribut `np.dtype` bestimmt werden. Der Datentyp eines Objekts kann mit der Methode `np.array = np.array.astype()` geändert werden.\n\n\nFolgende Datei ist Ihnen aus dem w-NumPy bekannt.\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\ndateipfad = '01-daten/TC01.csv'\ndaten = np.loadtxt(dateipfad)\n```\n:::\n\n\n**Prüfen Sie den dtype der Datei und legen Sie eine Kopie des Objekts mit Datentyp Ganzzahl an. Wie kann überprüft werden, ob bei der Umwandlung in Ganzzahlen Nachkommastellen abgeschnitten wurden?**\n\n::: {#tip-numpydatentyp .callout-tip collapse=\"true\"}\n## Musterlösung Datentypumwandlung\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\n# Ausgabe des Datentyps\nprint(daten.dtype)\n\n# Umwandlung in Ganzzahl\ndaten_int = daten.astype('int')\n\n# Prüfen auf Datenverlust\nprüfsumme = daten - daten_int\nprint(f\"Differenz daten - daten_int: {prüfsumme.sum()}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nfloat64\nDifferenz daten - daten_int: 664.0\n```\n:::\n:::\n\n\n:::\n\n### Pandas\nDas Modul Pandas ist auf den Umgang mit unterschiedlichen Datentypen spezialisiert. Den Funktionen zum Einlesen von Daten kann mit dem Argument `dtype` der Datentyp übergeben werden. Für mehrere Spalten ist dies in Form eines Dictionaries in der Form `{'Spaltenname': 'dtype'}` möglich.  \nDas Atrribut zur Ausgabe des Datentyps heißt passenderweise `pd.DataFrame.dtypes` (angefügtes s beachten). Der Datentyp eines Pandas-Datenobjekts kann analog zu NumPy mit `pd.Series = pd.Series.astype()` geändert werden.\n\n#### Zahnwachstum bei Meerschweinchen\nIn einer Gruppe von 60 Meerschweinchen (**1. Spalte ohne Beschriftung**) wurde die Länge der zahnbildenden Zellen (Odontoblasten) in Micron gemessen (**len**). Den Tieren wurde zuvor Vitamin C in Form von Ascorbinsäure (VC) oder Orangensaft (VC) verabreicht (**supp**). Die Meerschweinchen erhielten Dosen von 0.5, 1 oder 2 Milligramm Vitamin C pro Tag  (**dose**). Die Messdaten sind in der Datei ToothGrowth.csv gespeichert (Crampton 1947.)\n\n::: {.border}\nCrampton, E. W. 1947. „THE GROWTH OF THE ODONTOBLASTS OF THE INCISOR TOOTH AS A CRITERION OF THE VITAMIN C INTAKE OF THE GUINEA PIG“. The Journal of Nutrition 33 (5): 491–504. <https://doi.org/10.1093/jn/33.5.491> \n:::\n\n&nbsp;\n\n **Lesen Sie die Datei wie folgt ein:**\n\n  - Die Spaltenbeschriftung der 1. Spalte soll mit der Beschriftung 'ID' ersetzt werden (ohne Anführungszeichen).\n  \n  - Die Spalten len und dose sollen mit geeigneten numerischen Datentypen, die Spalte supp als Kategorie eingelesen werden.\n\n::: {#tip-meerschweinchen .callout-tip collapse=\"true\"}\n## Musterlösung Meerschweinchen\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\ndateipfad = \"01-daten/ToothGrowth.csv\"\nmeerschweinchen = pd.read_csv(filepath_or_buffer = dateipfad, sep = ',', header = 0, \\\n  names = ['ID', 'len', 'supp', 'dose'], dtype = {'ID': 'int', 'len': 'float', 'dose': 'float', 'supp': 'category'})\n\n# Ausgabe jedes sechsten Werts\nmeerschweinchen.iloc[meerschweinchen.index % 6 == 0]\n```\n\n::: {.cell-output .cell-output-display execution_count=37}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>len</th>\n      <th>supp</th>\n      <th>dose</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>4.2</td>\n      <td>VC</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>11.2</td>\n      <td>VC</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>13</td>\n      <td>15.2</td>\n      <td>VC</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>19</td>\n      <td>18.8</td>\n      <td>VC</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>25</td>\n      <td>26.4</td>\n      <td>VC</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>31</td>\n      <td>15.2</td>\n      <td>OJ</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>37</td>\n      <td>8.2</td>\n      <td>OJ</td>\n      <td>0.5</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>43</td>\n      <td>23.6</td>\n      <td>OJ</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>49</td>\n      <td>14.5</td>\n      <td>OJ</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>54</th>\n      <td>55</td>\n      <td>24.8</td>\n      <td>OJ</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\nprint(meerschweinchen.dtypes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nID         int64\nlen      float64\nsupp    category\ndose     float64\ndtype: object\n```\n:::\n:::\n\n\n:::\n\n#### Nützliche Funktionen für die deskriptive Datenanalyse\nPandas bietet einige praktische Funktionen, um den Aufbau eines Datensatzes zu beschreiben.\n\nDas Attribut `.columns` gibt die Spaltenbeschriftungen als Liste zurück. Ebenfalls ist darüber ein Schreibzugriff möglich.\n\n::: {.cell execution_count=39}\n``` {.python .cell-code}\nprint(meerschweinchen.columns)\nmeerschweinchen.columns = ['ID', 'Länge', 'Verabreichung', 'Dosis']\nprint(meerschweinchen.columns)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIndex(['ID', 'len', 'supp', 'dose'], dtype='object')\nIndex(['ID', 'Länge', 'Verabreichung', 'Dosis'], dtype='object')\n```\n:::\n:::\n\n\nDie Methode `pd.DataFrame.describe()` erzeugt eine beschreibende Statistik für einen DataFrame. Standardmäßig werden alle numerischen Spalten berücksichtigt. Mit dem Argument `include` können die zu berücksichtigenden Spalten vorgegeben werden. `include = all` berücksichtigt alle Spalten, was nicht unbedingt sinnvoll ist. Alternativ kann eine Liste zu berücksichtigender Datentypen übergeben werden. Das Argument `exclude` schließt auf die gleiche Weise Datentypen von der Ausgabe aus.\n\n::: {.cell execution_count=40}\n``` {.python .cell-code}\nprint(meerschweinchen.describe(), \"\\n\")\n\nprint(meerschweinchen.describe(include = 'all'), \"\\n\")\n\nprint(meerschweinchen.describe(include = ['float']), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          ID  Länge  Dosis\ncount  60.00  60.00  60.00\nmean   30.50  18.81   1.17\nstd    17.46   7.65   0.63\nmin     1.00   4.20   0.50\n25%    15.75  13.07   0.50\n50%    30.50  19.25   1.00\n75%    45.25  25.27   2.00\nmax    60.00  33.90   2.00 \n\n           ID  Länge Verabreichung  Dosis\ncount   60.00  60.00            60  60.00\nunique    NaN    NaN             2    NaN\ntop       NaN    NaN            OJ    NaN\nfreq      NaN    NaN            30    NaN\nmean    30.50  18.81           NaN   1.17\nstd     17.46   7.65           NaN   0.63\nmin      1.00   4.20           NaN   0.50\n25%     15.75  13.07           NaN   0.50\n50%     30.50  19.25           NaN   1.00\n75%     45.25  25.27           NaN   2.00\nmax     60.00  33.90           NaN   2.00 \n\n       Länge  Dosis\ncount  60.00  60.00\nmean   18.81   1.17\nstd     7.65   0.63\nmin     4.20   0.50\n25%    13.07   0.50\n50%    19.25   1.00\n75%    25.27   2.00\nmax    33.90   2.00 \n\n```\n:::\n:::\n\n\nDie Methode `pd.DataFrame.count()` zählt alle vorhandenen Werte in jeder Spalte oder  mit `pd.DataFrame.count(axis = 'columns')` in jeder Zeile.\n\n::: {.cell execution_count=41}\n``` {.python .cell-code}\nmeerschweinchen.count(axis = 'rows') # der Standardwert von axis ist 'rows'\n```\n\n::: {.cell-output .cell-output-display execution_count=41}\n```\nID               60\nLänge            60\nVerabreichung    60\nDosis            60\ndtype: int64\n```\n:::\n:::\n\n\nDie Methode `pd.DataFrame.info()` erzeugt eine Beschreibung des Datensatzes. \n\n::: {.cell execution_count=42}\n``` {.python .cell-code}\nmeerschweinchen.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 60 entries, 0 to 59\nData columns (total 4 columns):\n #   Column         Non-Null Count  Dtype   \n---  ------         --------------  -----   \n 0   ID             60 non-null     int64   \n 1   Länge          60 non-null     float64 \n 2   Verabreichung  60 non-null     category\n 3   Dosis          60 non-null     float64 \ndtypes: category(1), float64(2), int64(1)\nmemory usage: 1.7 KB\n```\n:::\n:::\n\n\nDie Methode `pd.unique()` listet alle einzigartigen Werte auf.\n\n::: {.cell execution_count=43}\n``` {.python .cell-code}\nmeerschweinchen['Dosis'].unique()\n```\n\n::: {.cell-output .cell-output-display execution_count=43}\n```\narray([0.5, 1. , 2. ])\n```\n:::\n:::\n\n\n::: {#tip-pandasinfo .callout-tip collapse=\"false\"}\n## Nützliche Funktionen\nPandas bietet einige praktische Funktionen, um eine eingelesene Datei zu kontrollieren. Machen Sie sich die Verwendung von `pd.dtypes` oder `pd.DataFrame.info()` zur Angewohnheit. \n:::\n\n### Aufgabe Datentypen\nDas britische Energieministerium veröffentlicht Daten zu den Industriestrompreisen in den Mitgliedsändern der Internationalen Energieagentur.  \n**Lesen Sie Tabellenblatt \"5.3.1 (excl. taxes)\" aus der Excel-Datei 'skript/01-daten/table_531.xlsx' mit Pandas ein. Schauen Sie in der Dokumentation der Funktion [pd.read_excel](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html) nach, wie Sie das korrekte Tabellenblatt auswählen können. Stellen Sie sicher, dass alle Spalten mit einem numerischen Datentyp eingelesen werden.**\n\n::: {.border}\nDepartment for Energy Security & Net Zero. 2024. Energy Prices International Comparisons. Industrial electricity prices in the IEA. <https://www.gov.uk/government/uploads/system/uploads/attachment_data/file/670121/table_531.xls>\n:::\n\n::: {#tip-taxes .callout-tip collapse=\"true\"}\n## Musterlösung 5.3.1 (excl. taxes)\n\nÜberspringen der führenden Zeilen mit dem Argument `header = 8`. Auswahl des Tabellenblatts mit `sheet_name = \"5.3.1 (excl. taxes)\"` und Kontrolle der erkannten Datentypen mit `taxes.dtypes` \n\n::: {.cell execution_count=44}\n``` {.python .cell-code}\ndateipfad = '01-daten/table_531.xlsx'\n\ntaxes = pd.read_excel(io = dateipfad, sheet_name = \"5.3.1 (excl. taxes)\", \\\n  header = 8)\n\ntaxes.dtypes\n```\n\n::: {.cell-output .cell-output-display execution_count=44}\n```\nYear                            int64\nAustria                       float64\nBelgium                       float64\nDenmark                       float64\nFinland                       float64\nFrance                        float64\nGermany                       float64\nGreece                        float64\nIreland                       float64\nItaly                         float64\nLuxembourg                    float64\nNetherlands                   float64\nPortugal                      float64\nSpain                         float64\nSweden                        float64\nUnited Kingdom                float64\nAustralia                     float64\nCanada                        float64\nCzech Republic                float64\nHungary                       float64\nJapan                         float64\nKorea                         float64\nNew Zealand                   float64\nNorway                        float64\nPoland                        float64\nSlovakia                      float64\nSwitzerland                   float64\nRepublic of Türkiye            object\nUSA                           float64\nIEA median                    float64\nUK relative to IEA median%    float64\nUK relative to IEA rank         int64\nUK relative to G7 rank          int64\ndtype: object\n```\n:::\n:::\n\n\nWerte in Spalte 'Republic of Türkiye' mit `pd.unique()` ansehen.\n\n::: {.cell execution_count=45}\n``` {.python .cell-code}\ntaxes['Republic of Türkiye'].unique()\n```\n\n::: {.cell-output .cell-output-display execution_count=45}\n```\narray(['..', 2.0436081749999997, 3.3248584439999993, 3.2947581129644483,\n       3.5628243387317866, 3.998334312, 3.838962582401693,\n       4.2138469457789975, 3.775503630575527, 3.2804905218375238,\n       3.783413840344277, 4.139259596071514, 4.196890742949158,\n       4.658509330911754, 5.552842625063031, 4.316920402166109,\n       4.1586205264300675, 4.765321921741988, 4.060617948410105,\n       3.9191433658651307, 4.223710389549368, 4.481407237836746,\n       4.629981488840797, 5.2657882806931235, 5.109009847145703,\n       4.585007793872617, 4.769921255774284, 4.419433670846949,\n       4.428906151745361, 6.171573537217762, 7.192920543071899,\n       7.962417550086158, 7.035941949054445, 7.622058781522502,\n       7.644892388451444, 6.47006818181818, 5.968380462724936,\n       6.379514692256784, 5.537541821623266, 5.248709303933227,\n       6.9100519994521274, 6.670900808798327, 5.864171132090749,\n       13.928251887312259, 11.123594768114717], dtype=object)\n```\n:::\n:::\n\n\nZeichenkette '..' entfernen und Datentyp mit Methode `pd.astype('float64')` ändern.\n\n  - Variante 1: als fehlenden Wert beim Einlesen deklarieren. \n  \n  - Variante 2: Nach dem Einlesen Indexposition bestimmen und Wert ersetzen. Das verkettete Slicing `df[\"col\"][row_indexer] = value` wird mit der Pandas Version 3.0 nicht mehr unterstützt und gibt deshalb eine Fehlermeldung aus. Künftig ist folgende Syntax zu verwenden: `df.loc[row_indexer, \"col\"] = value`.\n\n::: {.cell execution_count=46}\n``` {.python .cell-code}\n# Variante 1: '..' als fehlenden Wert deklarieren\n# taxes = pd.read_excel(io = dateipfad, sheet_name = \"5.3.1 (excl. taxes)\", \\\n#   header = 8, na_values = ['..'])\n\n# Variante 2: Index des Werts bestimmen und mit np.nan überschreiben\nindexposition = taxes['Republic of Türkiye'] == '..'\n\ntaxes.loc[indexposition, 'Republic of Türkiye'] = np.nan\ntaxes['Republic of Türkiye'] = taxes['Republic of Türkiye'].astype('float64')\n\ntaxes.dtypes\n```\n\n::: {.cell-output .cell-output-display execution_count=46}\n```\nYear                            int64\nAustria                       float64\nBelgium                       float64\nDenmark                       float64\nFinland                       float64\nFrance                        float64\nGermany                       float64\nGreece                        float64\nIreland                       float64\nItaly                         float64\nLuxembourg                    float64\nNetherlands                   float64\nPortugal                      float64\nSpain                         float64\nSweden                        float64\nUnited Kingdom                float64\nAustralia                     float64\nCanada                        float64\nCzech Republic                float64\nHungary                       float64\nJapan                         float64\nKorea                         float64\nNew Zealand                   float64\nNorway                        float64\nPoland                        float64\nSlovakia                      float64\nSwitzerland                   float64\nRepublic of Türkiye           float64\nUSA                           float64\nIEA median                    float64\nUK relative to IEA median%    float64\nUK relative to IEA rank         int64\nUK relative to G7 rank          int64\ndtype: object\n```\n:::\n:::\n\n\n:::\n\n## Umgang mit fehlenden Werten\nEine unerwartet als string oder object eingelesene Spalte weist häufig auf fehlende Werte hin, die durch Sonderzeichen gekennzeichnet sind. Die Module NumPy und Pandas bieten Funktionen, um fehlende Werte bereits beim Einlesen zu erkennen und umzuwandeln.\n\n*Hinweis: Maskierte NumPy-Arrays werden in [@sec-ma] behandelt.*\n\n### NumPy\nDie NumPy-Funktion `np.loadtxt()` wird verwendet, um vollständige Datensätze einzulesen. Fehlende Werte im Datensatz können problematisch sein, da diese entweder zu Fehlermeldungen bezüglich des Datentyps führen oder übersprungen werden, sodass das NumPy-Array kürzer als der eingelesene Datensatz ist. Da NumPy-Arrays immer nur einen Datentyp und eine feste Länge haben, kann das bei der Durchführung von Operationen mit mehreren Arrays zu Fehlern führen.\n\nFolgende Datei ist Ihnen aus dem w-NumPy bekannt.\n\n::: {.cell execution_count=47}\n``` {.python .cell-code}\ndateipfad = '01-daten/TC01.csv'\ndaten_ohne_fehlende_werte = np.loadtxt(dateipfad)\n\nprint(\"Daten:\", daten_ohne_fehlende_werte)\nprint(\"Struktur:\", daten_ohne_fehlende_werte.shape, \"dtype:\", daten_ohne_fehlende_werte.dtype)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDaten: [20.1 20.1 20.1 ... 24.3 24.2 24.2]\nStruktur: (1513,) dtype: float64\n```\n:::\n:::\n\n\nAngenommen, Sie haben eine zweite Messung durchgeführt und möchten die Differenz beider Datensätze berechnen. In der zweiten Messung haben Sensorfehler zu fehlenden Werten geführt, die mit `--` markiert sind. Die Funktion `np.loadtxt()` kann jedoch mit fehlenden Werten nicht umgehen und gibt eine Fehlermeldung zurück. \n\n::: {.cell execution_count=48}\n``` {.python .cell-code}\ndateipfad = '01-daten/TC01_double_hyphen.csv'\n\ntry:\n  daten_double_hypen = np.loadtxt(dateipfad)\nexcept ValueError as error:\n  print(\"Die Eingabe führt zu der Fehlermeldung:\\n\", error)\nelse:\n  print(\"Daten mit fehlenden Werten '--':\", daten_double_hypen, \"dtype:\", daten_double_hypen.dtype) \n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDie Eingabe führt zu der Fehlermeldung:\n could not convert string '--' to float64 at row 1, column 1.\n```\n:::\n:::\n\n\n#### Die Funktion np.genfromtxt()\nUm Datensätze mit fehlenden Werten einzulesen, wird die Funktion `np.genfromtxt(fname, delimiter = None, missing_values = None, filling_values = None)` verwendet. Dieses durchläuft den Datensatz `fname` in zwei Schleifen, weshalb die Funktion langsamer als `np.loadtxt()` ist. Die erste Schleife teilt den Datensatz zeilenweise am optional übergebenen Trennzeichen `delimiter` in eine Zeichenkette auf. Die zweite Schleife konvertiert jede Zeichenkette in den passenden Datentyp. Mit den optionalen Argumenten `missing_values` und `filling_values` können der Funktion Zeichenfolgen übergeben werden, mit der fehlende Werte markiert sind bzw. ersetzt werden sollen. ([NumPy Dokumentation](https://numpy.org/doc/stable/user/basics.io.genfromtxt.html))\n\n::: {.cell execution_count=49}\n``` {.python .cell-code}\ndateipfad = '01-daten/TC01_double_hyphen.csv'\ndaten_double_hypen = np.genfromtxt(dateipfad, missing_values = '--', filling_values = np.nan)\n\nprint(\"\\nDaten mit fehlenden Werten '--':\", daten_double_hypen)\nprint(\"Struktur:\", daten_double_hypen.shape, \"dtype:\", daten_double_hypen.dtype)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nDaten mit fehlenden Werten '--': [20.1  nan 20.1 ... 24.3 24.2 24.2]\nStruktur: (1513,) dtype: float64\n```\n:::\n:::\n\n\nDurch die Umwandlung fehlender Werte in `nan`, sind Operationen mit gleichlangen NumPy-Arrays möglich.\n\n::: {.cell execution_count=50}\n``` {.python .cell-code}\ndaten_differenz = daten_ohne_fehlende_werte - daten_double_hypen\nprint(daten_differenz)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[ 0. nan  0. ...  0.  0.  0.]\n```\n:::\n:::\n\n\nDie Funktion `np.genfromtxt()` kann beliebige Zeichenketten als fehlenden Wert verarbeiten. Lediglich leere Zellen können problematisch sein, da deren Inhalt `'\\n'` als Zeilentrenner verarbeitet wird.\n\n::: {#nte-npgenfromtxt .callout-note collapse=\"true\"}\n## Leere Zellen mit np.genfromtxt()\n\nEnthält eine Datei leere Zellen, können diese nicht eingelesen werden, da diese automatisch übersprungen werden.\n\n::: {.cell execution_count=51}\n``` {.python .cell-code}\n# Datei ohne Markierung fehlender Werte\ndateipfad = '01-daten/TC01_empty_lines.csv'\ndaten_empty_lines = np.genfromtxt(dateipfad, missing_values = '', filling_values = np.nan) \n\nprint(\"\\nDaten mit fehlenden Werten '':\", daten_empty_lines)\nprint(\"Struktur:\", daten_empty_lines.shape, \"dtype:\", daten_empty_lines.dtype)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nDaten mit fehlenden Werten '': [20.1 20.1 20.1 ... 24.3 24.2 24.2]\nStruktur: (1511,) dtype: float64\n```\n:::\n:::\n\n\nDas Array ist zwei Elemente kürzer. Die Subtraktion von einem längeren NumPy-Array scheitert mit einer Fehlermeldung.\n\n::: {.cell execution_count=52}\n``` {.python .cell-code}\ntry:\n  result = daten_ohne_fehlende_werte - daten_empty_lines\nexcept ValueError as error:\n  print(\"Die Eingabe führt zu der Fehlermeldung:\\n\", error)\nelse:\n  print(result)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDie Eingabe führt zu der Fehlermeldung:\n operands could not be broadcast together with shapes (1513,) (1511,) \n```\n:::\n:::\n\n\nIn diesem Fall muss auf die Stringbearbeitung aus der Python-Basis zurückgegriffen werden. Die bearbeitete Liste kann wie gewohnt mit `np.genfromtxt()` eingelesen werden.\n\n::: {.cell execution_count=53}\n``` {.python .cell-code}\n# Einlesen über Datenobjekt\ndatenobjekt_empty_lines = open(dateipfad, 'r', encoding = 'utf-8')\ndaten_empty_lines = datenobjekt_empty_lines.readlines()\ndatenobjekt_empty_lines.close()\n\nprint(\"Das ausgelesene Datenobjekt (Ausschnitt):\\n\", daten_empty_lines[0:10])\n\n# Stringbearbeitung mit replace('\\n', '')\nfor i in range(len(daten_empty_lines)):\n\n  if daten_empty_lines[i] == '\\n':\n    daten_empty_lines[i] = 'platzhalter'\n  else:\n    daten_empty_lines[i] = daten_empty_lines[i].replace('\\n', '')\n\nprint(\"\\nNach der Stringbearbeitung (Ausschnitt):\\n\", daten_empty_lines[0:10])\n\n# Einlesen mit np.genfromtxt\ndaten_empty_lines = np.genfromtxt(daten_empty_lines, missing_values = 'platzhalter', filling_values = np.nan)\nprint(\"\\nDaten mit fehlenden Werten '':\", daten_empty_lines)\nprint(\"Struktur:\", daten_empty_lines.shape, \"dtype:\", daten_empty_lines.dtype)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDas ausgelesene Datenobjekt (Ausschnitt):\n ['# Temperatur in C\\n', '20.1\\n', '\\n', '20.1\\n', '20.1\\n', '20.1\\n', '\\n', '20.1\\n', '20.1\\n', '20.1\\n']\n\nNach der Stringbearbeitung (Ausschnitt):\n ['# Temperatur in C', '20.1', 'platzhalter', '20.1', '20.1', '20.1', 'platzhalter', '20.1', '20.1', '20.1']\n\nDaten mit fehlenden Werten '': [20.1  nan 20.1 ... 24.3 24.2 24.2]\nStruktur: (1513,) dtype: float64\n```\n:::\n:::\n\n\n:::\n\nBesonders bei Dateien mit mehreren Spalten führen leere Zellen schnell zu Fehlern. Hier ist es erforderlich, den Zeichentrenner mit dem Argument `delimiter` zu spezifizieren. Aus der Dokumentation:  \n\"When spaces are used as delimiters, or when no delimiter has been given as input, there should not be any missing data between two fields.\" ([NumPy Dokumentation](https://numpy.org/doc/stable/reference/generated/numpy.genfromtxt.html))\n\n::: {#nte-npgenfromtxt .callout-note collapse=\"true\"}\n## Leere Zellen in mehreren Spalten mit np.genfromtxt()\nOhne Spezifikation des Arguments `delimiter` wird nur eine Spalte eingelesen, die ausschließlich `np.nan` enthält.\n\n::: {.cell execution_count=54}\n``` {.python .cell-code}\n# ohne Spezifikation von delimiter\ndateipfad = '01-daten/TC01_missing_values_multi_column.csv'\ndaten_empty_lines2 = np.genfromtxt(dateipfad, missing_values = '', filling_values = np.nan, ndmin = 2)\n\nprint(\"Struktur:\", daten_empty_lines2.shape, \"dtype:\", daten_empty_lines2.dtype)\nprint(\"Die ersten drei Zeilen:\\n\", daten_empty_lines2[0:3])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStruktur: (1513, 1) dtype: float64\nDie ersten drei Zeilen:\n [[nan]\n [nan]\n [nan]]\n```\n:::\n:::\n\n\nWird das Argument `delimiter = ','` übergeben, wird die Datei korrekt eingelesen.\n\n::: {.cell execution_count=55}\n``` {.python .cell-code}\n# mit Spezifikation von delimiter\ndaten_empty_lines2 = np.genfromtxt(dateipfad, delimiter = ',', missing_values = '', filling_values = np.nan, ndmin = 2)\n\nprint(\"Struktur:\", daten_empty_lines2.shape, \"dtype:\", daten_empty_lines2.dtype)\nprint(\"\\nDaten mit fehlenden Werten '':\\n\", daten_empty_lines2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStruktur: (1513, 2) dtype: float64\n\nDaten mit fehlenden Werten '':\n [[20.1 20.1]\n [ nan  nan]\n [20.1 20.1]\n ...\n [24.3 24.3]\n [24.2 24.2]\n [24.2 24.2]]\n```\n:::\n:::\n\n\n:::\n\n#### Fehlende Werte in NumPy erzeugen, prüfen, finden, ersetzen, löschen\nDas Modul NumPy bietet Funktionen, um mit fehlenden Werten zu arbeiten.\n\n  - `np.nan` erzeugt einen fehlenden Wert.\n\n  - `np.isnan()` prüft auf einen fehlenden Wert und gibt einen Wahrheitswert bzw. ein NumPy-Array mit dtype bool zurück.\n\n  - `np.nonzero(np.isnan(array))` gibt ein Tuple zurück, das ein Array mit den Indexpositionen der Elemente mit dem Wert 'nan' enthält. Auf das Array kann mit `np.nonzero(np.isnan(array))[0]` zugegriffen werden. Je nach Situation kann die Umwandlung in eine Liste nützlich sein `np.nonzero(np.isnan(array))[0].tolist()`.  \n  Eine ähnliche Funktion ist `np.argwhere(np.isnan(array))`, deren Ausgabe aber nicht für das Slicing mehrdimensionaler Arrays geeignet ist (siehe folgendes Beispiel).  \n\n::: {#nte-npargwhere .callout-note collapse=\"true\"}\n## Die Funktion np.argwhere()\n\nEine andere Funktion, um die Indexposition eines Werts zu bestimmen, ist die Funktion `np.argwhere()`. Der Aufruf der Funktion `np.argwhere(np.isnan(array))` gibt ein NumPy-Array mit den Indexposition Elemente mit dem Wert `nan` zurück. \n\n::: {.cell execution_count=56}\n``` {.python .cell-code}\narray = np.array([[1, np.nan, np.nan], [4, 5, np.nan]])\nprint(array)\n\nnp.argwhere(np.isnan(array))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[ 1. nan nan]\n [ 4.  5. nan]]\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=56}\n```\narray([[0, 1],\n       [0, 2],\n       [1, 2]])\n```\n:::\n:::\n\n\nDas mit `np.argwhere()` erzeugte Array ist aber nicht geeignet, um Arraybereiche auszuwählen.\n\n::: {.cell execution_count=57}\n``` {.python .cell-code}\ntry:\n  array[np.argwhere(np.isnan(array))]\nexcept IndexError as error:\n  print(\"Die Eingabe führt zu der Fehlermeldung:\\n\", error)\nelse:\n  print(array[np.argwhere(np.isnan(array))]) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDie Eingabe führt zu der Fehlermeldung:\n index 2 is out of bounds for axis 0 with size 2\n```\n:::\n:::\n\n\nZum Vergleich mit `np.nonzero()`\n\n::: {.cell execution_count=58}\n``` {.python .cell-code}\ntry:\n  array[np.nonzero(np.isnan(array))]\nexcept IndexError as error:\n  print(\"Die Eingabe führt zu der Fehlermeldung:\\n\", error)\nelse:\n  print(array[np.nonzero(np.isnan(array))]) \n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[nan nan nan]\n```\n:::\n:::\n\n\n:::: {#wrn-npargwhere .callout-warning appearance=\"simple\" collapse=\"false\"}\n## Die Funktion np.arghwhere()\nDie Auswahl von Array-Bereichen mit `np.argwhere()` funktioniert für eindimensionale Arrays.\n\n::: {.cell execution_count=59}\n``` {.python .cell-code}\narray = np.array([1, np.nan, np.nan, 4, 5])\n\ntry:\n  array[np.argwhere(np.isnan(array))]\nexcept IndexError as error:\n  print(\"Die Eingabe führt zu der Fehlermeldung:\\n\", error)\nelse:\n  print(array[np.argwhere(np.isnan(array))]) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[nan]\n [nan]]\n```\n:::\n:::\n\n\n::::\n:::\n\n  - `nan_to_num(x = array, nan = 0.0)` ersetzt im Array x `nan` durch den Wert 0.0 oder durch den im Argument `nan` übergebenen Wert. (Hinweis: `nan_to_num()` ersetzt standardmäßig auch `np.inf` durch große positive sowie `-np.inf` durch große negative Zahlen.)  \n  Die Ersetzung eines bestimmten Werts ist auch mit einem logischen Vektor möglich (siehe folgendes Beispiel).\n\n::: {#nte-vectorslicing .callout-note collapse=\"true\"}\n## Wertzuweisung mit logischem Vektor\nDie Ersetzung eines bestimmten Werts ist auch durch die Auswahl bestimmter Array-Bereiche durch einen logischen Vektor möglich.\n\n::: {.cell execution_count=60}\n``` {.python .cell-code}\na = np.array([1, 2, 3, np.nan, 5, 6, np.nan])\n\nb = np.isnan(a)\n\nprint(b)\n\na[b] = 0\n\nprint(a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[False False False  True False False  True]\n[1. 2. 3. 0. 5. 6. 0.]\n```\n:::\n:::\n\n\nDabei können mehrere Bedingungen mit der Funktion `np.logical_or(x1, x2)` als logisches ODER kombiniert werden.\n\n::: {.cell execution_count=61}\n``` {.python .cell-code}\na = np.array([1, 2, 3, np.nan, 5, 6, np.nan])\n\nbedingung1 = np.isnan(a)\n\nbedingung2 = a >= 5\n\nbedingung = np.logical_or(bedingung1, bedingung2)\n\na[bedingung] = 0\n\nprint(a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1. 2. 3. 0. 0. 0. 0.]\n```\n:::\n:::\n\n\nAuch ein logisches UND ist möglich (aber in Verbindung mit np.nan nicht sinnvoll). Der Operator * bewirkt das gleiche wie der logische Operator `and` oder die Funktion `np.logical_and(x1, x2)`.\n\n::: {.cell execution_count=62}\n``` {.python .cell-code}\na = np.array([1, 2, 3, np.nan, 5, 6, np.nan])\n\nbedingung1 = a < 4\n\nbedingung2 = a >= 1\n\nbedingung = bedingung1 * bedingung2\n\na[bedingung] = 0\n\nprint(a)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[ 0.  0.  0. nan  5.  6. nan]\n```\n:::\n:::\n\n\n:::\n\n  - `np.delete(arr = array, obj)` gibt ein neues (kürzeres) Array ohne die im Parameter obj spezifizierten Array-Bereiche zurück. Alle Elemente mit dem Wert `nan` werden so gelöscht: `np.delete(array, obj = np.nonzero(np.isnan(array)))`  \n\nNumPy wandelt `None` nicht automatisch in `nan` um. NumPy kann den Datentyp des Objekts deshalb nicht bestimmen und gibt `dtype=object` aus:\n\n::: {.cell execution_count=63}\n``` {.python .cell-code}\nnp_array_with_none = np.array([1, 2, None, 4])\nprint(np_array_with_none, np_array_with_none.dtype)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1 2 None 4] object\n```\n:::\n:::\n\n\n**Aufgabe: Wie kann im Array np_array_with_none `None` durch `np.nan` ersetzt werden?**\n\n::: {#tip-numpynone .callout-tip collapse=\"true\"}\n## Lösung\nEine logische Abfrage von `None` ist möglich. Auf diese Weise kann ein logisches Array erzeugt werden, das zur Auswahl der Indexpositionen verwendet wird, deren Werte ersetzt werden sollen.\n\n::: {.cell execution_count=64}\n``` {.python .cell-code}\nnp_array_with_none = np.array([1, 2, None, 4])\nprint(np_array_with_none)\n\nnp_array_with_nan = np_array_with_none.copy()\n\nprint(f\"\\nArray mit logischer Abfrage von None:\\n{np_array_with_none == None}\")\nnp_array_with_nan[np_array_with_none == None] = np.nan\nprint(f\"\\nArray mit None ersetzt durch nan:\\n{np_array_with_nan, np_array_with_nan.dtype}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1 2 None 4]\n\nArray mit logischer Abfrage von None:\n[False False  True False]\n\nArray mit None ersetzt durch nan:\n(array([1, 2, nan, 4], dtype=object), dtype('O'))\n```\n:::\n:::\n\n\n::: \n\n&nbsp;\n\n#### Operationen mit fehlenden Werten\nOperationen mit `nan` ergeben immer `nan`. Deshalb gibt es in NumPy viele Funktionen, die `nan` automatisch ignorieren bzw. durch einen geeigneten Wert ersetzen. Diese sind bereits am Funktionsnamen erkennbar. Beispielsweise liefern `np.nansum()` und `np.nancumsum()` die Summe bzw. die kumulierte Summe eines Arrays. In der kumulierten Summe werden `nan` durch das laufende Ergebnis ersetzt. Eine vollständige Liste der NumPy-Funktionen finden Sie in der [Dokumentation](https://numpy.org/doc/stable/reference/routines.html).\n\n::: {.cell execution_count=65}\n``` {.python .cell-code}\nprint(f\"Array mit nan:\\n{np_array_with_nan}\\n\")\n\nprint(f\"Summe des Arrays:\\n{np.sum(np_array_with_nan)}\\n\")\n\nprint(f\"nan-Summe des Arrays:\\n{np.nansum(np_array_with_nan)}\\n\")\n\nprint(f\"kumulierte Summe des Arrays:\\n{np.cumsum(np_array_with_nan)}\\n\")\n\nprint(f\"kumulierte nan-Summe des Arrays:\\n{np.nancumsum(np_array_with_nan)}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nArray mit nan:\n[1 2 nan 4]\n\nSumme des Arrays:\nnan\n\nnan-Summe des Arrays:\n7\n\nkumulierte Summe des Arrays:\n[1 3 nan nan]\n\nkumulierte nan-Summe des Arrays:\n[1 3 3 7]\n\n```\n:::\n:::\n\n\n### Pandas\nDie Pandas-Funktionen zum Lesen von Dateien können mit fehlenden Werten umgehen. Standardmäßig werden folgende Werte als fehlende Werte erkannt:  \n`['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN', '#N/A N/A', '#N/A', 'N/A', 'n/a', 'NA', '<NA>', '#NA', 'NULL', 'null', 'NaN', '-NaN', 'nan', '-nan', 'None', '']`\n\nWeitere Werte können mit dem Argument `na_values = []` als fehlende Werte definiert werden. Mit dem Argument `keep_default_na = False` kann festgelegt werden, dass ausschließlich die in `na_values = []` übergebenen Werte als fehlende Werte interpretiert werden sollen. Standardmäßig werden mit dem Argument `na_filter = True` auch leere Zellen als NA eingelesen. Vollständig leere Zeilen werden jedoch standardmäßig übersprungen. Dies kann mit dem Argument `skip_blank_lines = False` geändert werden. ([Pandas Dokumentation](https://pandas.pydata.org/docs/user_guide/io.html#io-navaluesconst))  \n\n::: {.cell execution_count=66}\n``` {.python .cell-code}\ndateipfad = '01-daten/TC01_double_hyphen.csv'\n\ntry:\n  daten_double_hypen = pd.read_csv(dateipfad, na_values = ['--'])\nexcept ValueError as error:\n  print(\"Die Eingabe führt zu der Fehlermeldung:\\n\", error)\nelse:\n  print(\"Daten mit fehlenden Werten '--':\\n\", daten_double_hypen, daten_double_hypen.shape) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDaten mit fehlenden Werten '--':\n       # Temperatur in C\n0                  20.1\n1                   NaN\n2                  20.1\n3                  20.1\n4                  20.1\n...                 ...\n1508               24.3\n1509               24.3\n1510               24.3\n1511               24.2\n1512               24.2\n\n[1513 rows x 1 columns] (1513, 1)\n```\n:::\n:::\n\n\nMit dem Argument `skip_blank_lines = False` werden leere Zeilen ebenfalls eingelesen. \n\n::: {.cell execution_count=67}\n``` {.python .cell-code}\ndateipfad = '01-daten/TC01_empty_lines.csv'\n\ntry:\n  daten_empty_lines = pd.read_csv(dateipfad, skip_blank_lines = False)\nexcept ValueError as error:\n  print(\"Die Eingabe führt zu der Fehlermeldung:\\n\", error)\nelse:\n  print(\"Daten mit fehlenden Werten '':\\n\", daten_empty_lines, daten_empty_lines.shape) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDaten mit fehlenden Werten '':\n       # Temperatur in C\n0                  20.1\n1                   NaN\n2                  20.1\n3                  20.1\n4                  20.1\n...                 ...\n1508               24.3\n1509               24.3\n1510               24.3\n1511               24.2\n1512               24.2\n\n[1513 rows x 1 columns] (1513, 1)\n```\n:::\n:::\n\n\nPandas verwendet abhängig vom Datentyp verschiedene Werte zur Kennzeichnung fehlender Werte.\n\n  - `numpy.nan` für NumPy-Datentypen. Hierbei wird der Datentyp automatisch in `np.float64` oder `object` konvertiert.\n\n  - `pd.NA` für Zeichenketten und Ganzzahlen. Der Datentyp bleibt erhalten.\n\nEinlesen der Datei TC01_empty_lines.csv als string:\n\n::: {.cell execution_count=68}\n``` {.python .cell-code}\ndateipfad = '01-daten/TC01_empty_lines.csv'\n\ntry:\n  daten_empty_lines = pd.read_csv(dateipfad, skip_blank_lines = False, dtype = 'string')\nexcept ValueError as error:\n  print(\"Die Eingabe führt zu der Fehlermeldung:\\n\", error)\nelse:\n  print(\"Daten mit fehlenden Werten '':\\n\", daten_empty_lines, daten_empty_lines.shape) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDaten mit fehlenden Werten '':\n      # Temperatur in C\n0                 20.1\n1                 <NA>\n2                 20.1\n3                 20.1\n4                 20.1\n...                ...\n1508              24.3\n1509              24.3\n1510              24.3\n1511              24.2\n1512              24.2\n\n[1513 rows x 1 columns] (1513, 1)\n```\n:::\n:::\n\n\n`NA` kann zwar auch als fehlender Wert für Gleitkommazahlen und andere NumPy Datentypen verwendet werden. Allerdings wird dafür ein Pandas-Datentyp benötigt (siehe das folgende Beispiel).\n\n::: {#nte-pdNA .callout-note collapse=\"true\"}\n## pd.Series mit np.nan und pd.NA\nEine pd.Series mit `np.nan` wird automatisch in `dtype: float64` umgewandelt: \n\n::: {.cell execution_count=69}\n``` {.python .cell-code}\ntry:\n  test = pd.Series([1, 2, np.nan])\nexcept TypeError as error:\n  print(\"Die Eingabe führt zu der Fehlermeldung:\\n\", error)\nelse:\n  print(test) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0    1.0\n1    2.0\n2    NaN\ndtype: float64\n```\n:::\n:::\n\n\nEine pd.Series mit `pd.NA` wird als `dtype: object` eingelesen: \n\n::: {.cell execution_count=70}\n``` {.python .cell-code}\ntry:\n  test = pd.Series([1, 2, pd.NA])\nexcept TypeError as error:\n  print(\"Die Eingabe führt zu der Fehlermeldung:\\n\", error)\nelse:\n  print(test) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0       1\n1       2\n2    <NA>\ndtype: object\n```\n:::\n:::\n\n\nDer `dtype` kann für eine Series mit `pd.NA` festgelegt werden: \n\n::: {.cell execution_count=71}\n``` {.python .cell-code}\ntry:\n  test = pd.Series([1, 2, pd.NA], dtype = 'Int32')\nexcept TypeError as error:\n  print(\"Die Eingabe führt zu der Fehlermeldung:\\n\", error)\nelse:\n  print(test) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0       1\n1       2\n2    <NA>\ndtype: Int32\n```\n:::\n:::\n\n\nAbhängig vom Datentyp kommt es auf den korrekten `dtype` (NumPy oder Pandas) an, erkennbar an der Groß- und Kleinschreibung. `pd.NA` mit Numpy-Fließkommazahl: \n\n::: {.cell execution_count=72}\n``` {.python .cell-code}\ntry:\n  test = pd.Series([1, 2, pd.NA], dtype = 'float64')\nexcept TypeError as error:\n  print(\"Die Eingabe führt zu der Fehlermeldung:\\n\", error)\nelse:\n  print(test) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDie Eingabe führt zu der Fehlermeldung:\n float() argument must be a string or a real number, not 'NAType'\n```\n:::\n:::\n\n\n`pd.NA` mit Pandas-Fließkommazahl:\n\n::: {.cell execution_count=73}\n``` {.python .cell-code}\ntry:\n  test = pd.Series([1, 2, pd.NA], dtype = 'Float64')\nexcept TypeError as error:\n  print(\"Die Eingabe führt zu der Fehlermeldung:\\n\", error)\nelse:\n  print(test) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0     1.0\n1     2.0\n2    <NA>\ndtype: Float64\n```\n:::\n:::\n\n\n`np.nan` mit Numpy-Fließkommazahl:\n\n::: {.cell execution_count=74}\n``` {.python .cell-code}\ntry:\n  test = pd.Series([1, 2, np.nan], dtype = 'float64')\nexcept TypeError as error:\n  print(\"Die Eingabe führt zu der Fehlermeldung:\\n\", error)\nelse:\n  print(test) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0    1.0\n1    2.0\n2    NaN\ndtype: float64\n```\n:::\n:::\n\n\n`np.nan` mit Pandas-Fließkommazahl:\n\n::: {.cell execution_count=75}\n``` {.python .cell-code}\ntry:\n  test = pd.Series([1, 2, np.nan], dtype = 'Float64')\nexcept TypeError as error:\n  print(\"Die Eingabe führt zu der Fehlermeldung:\\n\", error)\nelse:\n  print(test) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0     1.0\n1     2.0\n2    <NA>\ndtype: Float64\n```\n:::\n:::\n\n\n:::\n\n&nbsp;\n\n::: {#wrn-logicpandas .callout-warning appearance=\"simple\" collapse=\"false\"}\n## Achtung Logik!\n\nDie logische Abfrage fehlender Werte unterscheidet sich für `None`, `np.nan` und `pd.NA`. \n\n::: {.cell execution_count=76}\n``` {.python .cell-code}\nbool_values = [None, float('nan'), pd.NA]\n\nfor element in bool_values:\n  try:\n    bool_value = bool(element)\n  except TypeError as error:\n      print(error)\n  else:\n    print(\"Wahrheitswert von\", element, \"ist\", bool_value)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWahrheitswert von None ist False\nWahrheitswert von nan ist True\nboolean value of NA is ambiguous\n```\n:::\n:::\n\n\nDies gilt auch für die Wertgleichheit.\n\n::: {.cell execution_count=77}\n``` {.python .cell-code}\nbool_values = [None, float('nan'), pd.NA]\n\nfor element in bool_values:\n  try:\n    result = element == element\n  except TypeError as error:\n      print(error)\n  else:\n    print(\"Wertgleichheit von\", element, \"ist\", result)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWertgleichheit von None ist True\nWertgleichheit von nan ist False\nWertgleichheit von <NA> ist <NA>\n```\n:::\n:::\n\n\n::: \n\n([Pandas Dokumentation](https://pandas.pydata.org/docs/user_guide/missing_data.html))\n\n#### Fehlende Werte in Pandas erzeugen, prüfen, finden, ersetzen, löschen\nDas Modul Pandas wandelt `None` automatisch in `nan` um. Das Modul Pandas bietet wie das Modul NumPy verschiedene Funktionen, um mit fehlenden Werten zu arbeiten.\n\n  - `pd.NA` erzeugt einen fehlenden Wert (Groß- und Kleinschreibung beachten: `pd.na` funktioniert nicht)\n  \n  - Die Funktionen `pd.isnull()` und `pd.isna()` prüfen auf einen fehlenden Wert und geben einen Wahrheitswert bzw. ein NumPy-Array mit dtype bool zurück. Die Funktionen `pd.notna()` und `pd.notnull()` prüfen den umgekehrten Fall.\n\n  - Die Funktion `np.nonzero(pd.isna())` verwendet die NumPy-Funktion `np.nonzero()` und gibt ein Array mit den Indexpositionen der Elemente mit fehlenden Werten zurück (die Pandas-Funktion pd.nonzero() wird nicht mehr unterstützt).\n\n  - `pd.Series.fillna(value = 0)` ersetzt fehlende Werte mit dem im Argument `value` übergebenen Wert. Die Methoden `pd.ffill()` und `pd.bfill()` ersetzen fehlende Werte mit dem letzten bzw. dem nächsten gültigen Wert. Die Methode `pd.Series.interpolate()` ersetzt fehlende Werte durch Interpolation, wofür ein Datentyp definiert sein muss (`dtype = object` funktioniert nicht). Standardmäßig wird linear interpoliert, es stehen aber verschiedene Methoden zur Verfügung (siehe [Pandas Dokumentation](https://pandas.pydata.org/docs/reference/api/pandas.Series.interpolate.html))\n\n  - Die Methode `pd.Series.dropna()` gibt eine neue (kürzere) Series ohne fehlende Wert zurück.\n\n#### Operationen mit fehlenden Werten\nOperationen mit `pd.NA` ergeben in der Regel `pd.NA`. Es gibt jedoch einige Ausnahmen:\n\n::: {.cell execution_count=78}\n``` {.python .cell-code}\nprint(pd.NA ** 0)\nprint(1 ** pd.NA)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1\n1\n```\n:::\n:::\n\n\nDie Methode `pd.Series.sum()` behandelt `pd.NA` als 0, die Methode `pd.Series.prod()` als 1.\n\n::: {.cell execution_count=79}\n``` {.python .cell-code}\nprint(pd.Series([pd.NA]).sum())\nprint(pd.Series([pd.NA]).prod())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0\n1\n```\n:::\n:::\n\n\nReduzierende Methoden wie `pd.Series.min()` oder `pd.Series.mean()` sowie zusammenfassende Methoden wie `pd.Series.cumsum()` oder `pd.Series.cumprod()` überspringen `pd.NA`.\n\n::: {.cell execution_count=80}\n``` {.python .cell-code}\nprint(pd.Series([pd.NA]).min())\nprint(pd.Series([pd.NA]).mean())\nprint(pd.Series([pd.NA]).cumsum())\nprint(pd.Series([pd.NA]).cumprod())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnan\nnan\n0    NaN\ndtype: object\n0    NaN\ndtype: object\n```\n:::\n:::\n\n\nDas Verhalten von Methoden wie `pd.Series.sum()` und von Methoden wie `pd.Series.min()` hat für Datenreihen einen vergleichbaren Effekt, produziert für einzelne Werte jedoch unterschiedliche Ergebnisse.\n\n### Aufgabe fehlende Werte\nDer Deutsche Wetterdienst misst deutschlandweit verschiedene Wetterdaten. In der Datei 'produkt_st_stunde_20230831_20240630_01303.txt' sind stündliche Stationsmessungen der Solarstrahlung in Essen-Bredeney gespeichert.\n\n| Spaltenname | Beschreibung |\n|---|---|\n| STATIONS_ID | Stationsnummer |\n| QN_592 | Qualitätsniveau der Daten |\n| ATMO_LBERG | Stundensumme der atmosphärischen Gegenstrahlung |\n| FD_LBERG | Stundensumme der diffusen solaren Strahlung |\n| FG_LBERG | Stundensumme der Globalstrahlung |\n| SD_LBERG | Stundensumme der Sonnenscheindauer |\n| ZENIT | Zenitwinkel der Sonne 0 - 180 Grad |\n\n::: {.border}\nDeutscher Wetterdienst. 2024. Stündliche Stationsmessung der Solarstrahlung (global/diffus) und der atmosphärischen Gegenstrahlung für Deutschland. <https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/solar/stundenwerte_ST_01303_row.zip> Die Spalten MESS_DATUM, MESS_DATUM_WOZ und eor wurden entfernt.\n:::\n\n&nbsp;\n\n**Bestimmen Sie die Kodierung fehlender Werte und ersetzen Sie diese durch `np.nan` bzw. `pd.NA`. Wie viele Werte wurden ersetzt?**  \n\n::: {#tip-musterlösungfehlendewerte .callout-tip collapse=\"true\"}\n## Musterlösung fehlende Werte\n\nMit der Methode `df.info()` ist erkennbar, dass der Datensatz vollständig ist.\n\n::: {.cell execution_count=81}\n``` {.python .cell-code}\ndateipfad = \"01-daten/produkt_st_stunde_20230831_20240630_01303.txt\"\nsolar = pd.read_csv(dateipfad, sep = \";\")\n\nsolar.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7296 entries, 0 to 7295\nData columns (total 7 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   STATIONS_ID  7296 non-null   int64  \n 1   QN_592       7296 non-null   int64  \n 2   ATMO_LBERG   7296 non-null   float64\n 3   FD_LBERG     7296 non-null   float64\n 4   FG_LBERG     7296 non-null   float64\n 5   SD_LBERG     7296 non-null   int64  \n 6   ZENIT        7296 non-null   float64\ndtypes: float64(4), int64(3)\nmemory usage: 399.1 KB\n```\n:::\n:::\n\n\nMit der Methode `df.describe()` wird die deskriptive Statistik für numerische Spalten erstellt.  \n\n::: {.cell execution_count=82}\n``` {.python .cell-code}\nsolar.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=82}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>STATIONS_ID</th>\n      <th>QN_592</th>\n      <th>ATMO_LBERG</th>\n      <th>FD_LBERG</th>\n      <th>FG_LBERG</th>\n      <th>SD_LBERG</th>\n      <th>ZENIT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>7296.0</td>\n      <td>7296.0</td>\n      <td>7296.00</td>\n      <td>7296.00</td>\n      <td>7296.00</td>\n      <td>7296.00</td>\n      <td>7296.00</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1303.0</td>\n      <td>1.0</td>\n      <td>111.80</td>\n      <td>-31.23</td>\n      <td>-16.46</td>\n      <td>9.00</td>\n      <td>92.65</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>89.42</td>\n      <td>222.34</td>\n      <td>229.74</td>\n      <td>18.66</td>\n      <td>30.02</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1303.0</td>\n      <td>1.0</td>\n      <td>-999.00</td>\n      <td>-999.00</td>\n      <td>-999.00</td>\n      <td>0.00</td>\n      <td>28.56</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1303.0</td>\n      <td>1.0</td>\n      <td>112.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>70.61</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1303.0</td>\n      <td>1.0</td>\n      <td>121.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>92.40</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1303.0</td>\n      <td>1.0</td>\n      <td>127.25</td>\n      <td>25.00</td>\n      <td>33.00</td>\n      <td>3.00</td>\n      <td>115.97</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1303.0</td>\n      <td>1.0</td>\n      <td>150.00</td>\n      <td>182.00</td>\n      <td>348.00</td>\n      <td>60.00</td>\n      <td>151.44</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nDrei Spalten weisen als minimalen Wert -999 auf, der inhaltlich nicht sinnvoll ist. Wie oft kommt der Wert -999 in den Spalten vor?\n\n::: {.cell execution_count=83}\n``` {.python .cell-code}\ncounting_df = solar[['ATMO_LBERG', 'FD_LBERG', 'FG_LBERG']] == -999\nprint(counting_df.sum())\nprint(\"Summe:\\t\\t \", counting_df.sum().sum())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nATMO_LBERG     46\nFD_LBERG      359\nFG_LBERG      353\ndtype: int64\nSumme:\t\t  758\n```\n:::\n:::\n\n\n:::\n\n# Zeitreihen {#sec-zeitreihen}\n\n## Datum und Zeit in NumPy und Pandas\nDie Module NumPy und Pandas nutzen den Datentyp `datetime64`, um Datums- und Zeitinformationen zu verarbeiten.\n\n::: {.panel-tabset}\n## NumPy\n`datetime64`-Objekte werden mit der Funktion `np.datetime64()` angelegt, der Datentyp wird in der Ausgabe von Python auch durch den Buchstaben M repräsentiert. `datetime64`-Objekte können auf zwei Arten angelegt werden:\n\n  - Eine Zeichenkette nach [ISO 8601](https://www.iso.org/iso-8601-date-and-time-format.html) als Repräsentation eines Datums in der festgelegten Reihenfolge Jahr, Monat, Tag, Stunde, Minute, Sekunde, Millisekunde im Format `YYYY-MM-DD 12:00:00.000`. Als Zeichentrenner zwischen Datum und Uhrzeit sind ein Leerzeichen oder der Buchstabe T zulässig. Der Datentyp und die kleinste verwendete Einheit werden im Attribut `dtype` gespeichert.\n\n::: {.cell execution_count=84}\n``` {.python .cell-code}\nprint(np.datetime64('2024'), np.datetime64('2024').dtype)\nprint(np.datetime64('2024-10-31'), np.datetime64('2024-10-31').dtype)\nprint(np.datetime64('2024-10-31T12:24:59.999'), np.datetime64('2024-10-31T12:24:59.999').dtype)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2024 datetime64[Y]\n2024-10-31 datetime64[D]\n2024-10-31T12:24:59.999 datetime64[ms]\n```\n:::\n:::\n\n\n  - Als Zahl relativ zur Epoche und unter Angabe einer Zeiteinheit. Die verfügbaren Zeiteinheiten sind years ('Y'), months ('M'), weeks ('W'), days ('D') sowie hours ('h'), minutes ('m'), seconds ('s'), milliseconds ('ms') und weitere sekundenbasierte Einheiten bis zur Attosekunde (siehe [NumPy Dokumentation](https://numpy.org/doc/stable/reference/arrays.datetime.html#datetime-units)).\n\n::: {.cell execution_count=85}\n``` {.python .cell-code}\nprint(np.datetime64(10 * 1000, 'D'), np.datetime64(10 * 1000, 'D').dtype)\nprint(np.datetime64(1000 * 1000, 'h'), np.datetime64(1000 * 1000, 'h').dtype)\nprint(np.datetime64(1000 * 1000 * 1000, 's'), np.datetime64(1000 * 1000 * 1000, 's').dtype)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1997-05-19 datetime64[D]\n2084-01-29T16 datetime64[h]\n2001-09-09T01:46:40 datetime64[s]\n```\n:::\n:::\n\n\nAußerdem können Datetime-Formate anderer Module in `np.datetime64()` umgewandelt werden.\n\nBeim Anlegen eines Arrays, kann die Zeiteinheit gewählt werden.\n\n::: {.cell execution_count=86}\n``` {.python .cell-code}\nmy_array = np.array(['2007-07-13', '2006-01-13', '2010-08-13'], dtype = 'datetime64[s]')\nprint(my_array, my_array.dtype)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['2007-07-13T00:00:00' '2006-01-13T00:00:00' '2010-08-13T00:00:00'] datetime64[s]\n```\n:::\n:::\n\n\nDer Datentyp `datetime64` ist mit den meisten NumPy-Funktionen kompatibel.\n\n::: {.cell execution_count=87}\n``` {.python .cell-code}\nnp.arange('2005-02', '2005-03', dtype = 'datetime64[D]')\n```\n\n::: {.cell-output .cell-output-display execution_count=87}\n```\narray(['2005-02-01', '2005-02-02', '2005-02-03', '2005-02-04',\n       '2005-02-05', '2005-02-06', '2005-02-07', '2005-02-08',\n       '2005-02-09', '2005-02-10', '2005-02-11', '2005-02-12',\n       '2005-02-13', '2005-02-14', '2005-02-15', '2005-02-16',\n       '2005-02-17', '2005-02-18', '2005-02-19', '2005-02-20',\n       '2005-02-21', '2005-02-22', '2005-02-23', '2005-02-24',\n       '2005-02-25', '2005-02-26', '2005-02-27', '2005-02-28'],\n      dtype='datetime64[D]')\n```\n:::\n:::\n\n\n([NumPy Dokumentation](https://numpy.org/doc/stable/reference/arrays.datetime.html))\n\n## Pandas\n\nIn Pandas werden `datetime64`-Objekte mit den Funktionen `pd.to_datetime()` oder `pd.date_range()` angelegt.  \n*Hinweis: Eine weitere Möglichkeit ist die Funktion `pd.Timestamp()`, die umfangreichere Möglichkeiten zur Erzeugung eines Zeitpunkts bietet, aber kein string-parsing unterstützt.*\n\n`pd.to_datetime()` erzeugt Werte des Datentyps `datetime64[ns]` (mit `pd.to_datetime()` erzeugte Skalare (Einzelwerte) werden als Timestamp (Zeitpunkt) ausgegeben, die kein Attribut `dtype` haben). Die Funktion `pd.to_datetime()` akzeptiert als Eingabewerte:\n\n  - datetime-Objekte anderer Module.\n\n  - Zahlen und eine Zeiteinheit `pd.to_datetime(1, unit = None)` (Standard sind Nanosekunden). Das Argument `unit` nimmt die Werte 'ns', 'ms', 's', 'm', 'h', 'D', 'W' für Nanosekunde, Millisekunde, Sekunde, Minute, Stunde, Tag, Woche und 'MS' oder 'ME' für einen Monat (für Monatsanfang oder Monatsende) sowie für das Jahr 'YS' oder 'YE' (für Jahresanfang oder Jahresende) entgegen. Erzeugt wird ein Zeitpunkt relativ zur Epoche.\n\n::: {.cell execution_count=88}\n``` {.python .cell-code}\nprint(pd.to_datetime(1000, unit = 'D'))\nprint(pd.to_datetime(1000 * 1000, unit = 'h'))\nprint(pd.to_datetime(1000 * 1000 * 1000, unit = 's'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1972-09-27 00:00:00\n2084-01-29 16:00:00\n2001-09-09 01:46:40\n```\n:::\n:::\n\n\n  - Zeichenketten, die ein Datum oder ein Datum mit Uhrzeit ausdrücken, formatiert nach [ISO 8601](https://www.iso.org/iso-8601-date-and-time-format.html).\n\n::: {.cell execution_count=89}\n``` {.python .cell-code}\nprint(pd.to_datetime('2017'))\nprint(pd.to_datetime('2017-01-01T00'))\nprint(pd.to_datetime('2017-01-01 00:00:00'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2017-01-01 00:00:00\n2017-01-01 00:00:00\n2017-01-01 00:00:00\n```\n:::\n:::\n\n\n  - Anders formatierte Zeichenketten mit dem Argument format = `\"%d/%m/%Y\"` (siehe [Dokumentation strftime zur string-Formatierung](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior)).\n\n::: {.cell execution_count=90}\n``` {.python .cell-code}\nprint(pd.to_datetime('Monday, 12. August `24', format = \"%A, %d. %B `%y\"))\nprint(pd.to_datetime('Monday, 12. August 2024, 12:15 Uhr CET', format = \"%A, %d. %B %Y, %H:%M Uhr %Z\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2024-08-12 00:00:00\n2024-08-12 12:15:00+02:00\n```\n:::\n:::\n\n\n  - Dictionary oder DataFrame.\n\n::: {.cell execution_count=91}\n``` {.python .cell-code}\nprint(pd.to_datetime({'year':[2020, 2024], 'month': [1, 11], 'day': [1, 21]}), \"\\n\")\nprint(pd.to_datetime(pd.DataFrame({'year':[2020, 2024], 'month': [1, 11], 'day': [1, 21]})))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0   2020-01-01\n1   2024-11-21\ndtype: datetime64[ns] \n\n0   2020-01-01\n1   2024-11-21\ndtype: datetime64[ns]\n```\n:::\n:::\n\n\nDie Funktion `pd.date_range()` erzeugt ein Array vom Typ `DatetimeIndex` mit dtype `datetime64`. Genau drei der folgenden vier Argumente sind für die Erzeugung erforderlich: \n\n  - start: Beginn der Reihe.\n\n  - end: Ende der Reihe (inklusiv)\n\n  - freq: Schrittweite (bspw. Jahr, Tag, Geschäftstag, Stunde oder Vielfache wie '6h' - siehe [Liste verfügbarer strings](https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases))\n\n  - periods: Anzahl der zu erzeugenden Werte.\n\n::: {.cell execution_count=92}\n``` {.python .cell-code}\nprint(pd.date_range(start = '2017', end = '2024', periods = 3), \"\\n\")\n\nprint(pd.date_range(start = '2017', end = '2024', freq = 'YE'), \"\\n\")\n\nprint(pd.date_range(end = '2024', freq = 'h', periods = 3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDatetimeIndex(['2017-01-01', '2020-07-02', '2024-01-01'], dtype='datetime64[ns]', freq=None) \n\nDatetimeIndex(['2017-12-31', '2018-12-31', '2019-12-31', '2020-12-31',\n               '2021-12-31', '2022-12-31', '2023-12-31'],\n              dtype='datetime64[ns]', freq='YE-DEC') \n\nDatetimeIndex(['2023-12-31 22:00:00', '2023-12-31 23:00:00',\n               '2024-01-01 00:00:00'],\n              dtype='datetime64[ns]', freq='h')\n```\n:::\n:::\n\n\n:::\n\nZeitdifferenzen werden über einen eigenen Datentyp dargestellt (siehe folgendes Beispiel).  \n<!-- **Notiz für mich: Anwendungsfall Zeitzonen verschieben unabhängig von einer Zeitzone (und ohne die Zeitzonensyntax)** -->\n\n::: {.callout-note collapse=\"true\"}\n## Zeitdifferenzen in NumPy und Pandas\n\n:::: {.panel-tabset}\n\n## NumPy\nZeitdifferenzen werden mit dem Datentyp `timedelta64` abgebildet. Dieser wird wie `datetime64` durch Angabe einer Ganzzahl und einer Zeiteinheit angelegt.\n\n::: {.cell execution_count=93}\n``` {.python .cell-code}\nnp.timedelta64(1, 'D')\n```\n\n::: {.cell-output .cell-output-display execution_count=93}\n```\nnp.timedelta64(1,'D')\n```\n:::\n:::\n\n\nObjekte der Typen `datetime64` und `timedelta64` ermöglichen es, Operationen mit Datum und Zeit durchzuführen (weitere Beispiele in der [NumPy-Dokumentation](https://numpy.org/doc/stable/reference/arrays.datetime.html#datetime-and-timedelta-arithmetic)).\n\n::: {.cell execution_count=94}\n``` {.python .cell-code}\nprint(np.datetime64('today') - np.datetime64('2000-01-01', 'D'))\nprint(np.datetime64('now') - np.datetime64('2000-01-01', 'h'))\n\nprint(\"\\n\\nEine einfache Zeitverschiebung:\", np.datetime64('now') - np.timedelta64(1, 'h'))\nprint(\"Wie viele Tage hat die Woche?\", np.timedelta64(1,'W') / np.timedelta64(1,'D'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n9552 days\n825326748 seconds\n\n\nEine einfache Zeitverschiebung: 2026-02-25T08:25:48\nWie viele Tage hat die Woche? 7.0\n```\n:::\n:::\n\n\n## Pandas\nZeitdifferenzen können zum einen wie in NumPy durch Angabe einer Ganzzahl und einer Zeiteinheit angelegt werden. Außerdem ist die Übergabe mit Argumenten möglich (zulässige Argumente sind: weeks, days, hours, minutes, seconds, milliseconds, microseconds, nanoseconds).\n\n::: {.cell execution_count=95}\n``` {.python .cell-code}\npd.Timedelta(1, 'D')\npd.Timedelta(days = 1, hours = 1)\n```\n\n::: {.cell-output .cell-output-display execution_count=95}\n```\nTimedelta('1 days 01:00:00')\n```\n:::\n:::\n\n\n**Wichtig:** Anders als in NumPy werden Zeitdifferenzen in Monaten und Jahren nicht mehr von Pandas unterstützt.\n\n::: {.cell execution_count=96}\n``` {.python .cell-code}\ntry:\n  print( pd.Timedelta(1, 'YE'))\nexcept ValueError as error:\n  print(error)\nelse:\n  print( pd.Timedelta(1, 'YE'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ninvalid unit abbreviation: YE\n```\n:::\n:::\n\n\nZum anderen können Zeitdifferenzen mit einer Zeichenkette erzeugt werden.\n\n::: {.cell execution_count=97}\n``` {.python .cell-code}\nprint(pd.Timedelta('10sec'))\nprint(pd.Timedelta('10min'))\nprint(pd.Timedelta('10hours'))\nprint(pd.Timedelta('10days'))\nprint(pd.Timedelta('10w'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0 days 00:00:10\n0 days 00:10:00\n0 days 10:00:00\n10 days 00:00:00\n70 days 00:00:00\n```\n:::\n:::\n\n\nMit Hilfe einer Zeitdifferenz können Zeitreihen leicht verschoben werden.\n\n::: {.cell execution_count=98}\n``` {.python .cell-code}\npd.date_range(start = '2024-01-01T00:00', end = '2024-01-01T02:00', freq = '15min') + pd.Timedelta('30min')\n```\n\n::: {.cell-output .cell-output-display execution_count=98}\n```\nDatetimeIndex(['2024-01-01 00:30:00', '2024-01-01 00:45:00',\n               '2024-01-01 01:00:00', '2024-01-01 01:15:00',\n               '2024-01-01 01:30:00', '2024-01-01 01:45:00',\n               '2024-01-01 02:00:00', '2024-01-01 02:15:00',\n               '2024-01-01 02:30:00'],\n              dtype='datetime64[ns]', freq='15min')\n```\n:::\n:::\n\n\n::::\n\n&nbsp;\n\n**Wie alt sind Sie in Tagen? Wie alt in Sekunden? Rechnen Sie mit NumPy oder Pandas.**\n\n:::: {#tip-alter .callout-tip collapse=\"true\"}\n## Tipp Pandas und Musterlösung Alter\n\nFür eine elegante Lösung in Pandas schauen Sie sich die verfügbaren Methoden und Attribute von Timedelta-Objekten an.\n\n``` {.raw}\ndir(pd.Timedelta(0))\n```\n\n::::: {#tip-loesungalter .callout-tip collapse=\"true\"}\n## Musterlösung\n\nErsetzen sie in der Lösung die Zeichenkette 'YYYY-MM-DD' bzw., wenn Sie die Uhrzeit Ihrer Geburt kennen, die Zeichenkette 'YYYY-MM-DDTHH:MM' durch ihren Geburtstag.\n\n#### NumPy\nIn NumPy können die Schlüsselwörter `np.datetime64('today')` und `np.datetime64('now')` verwendet werden. Die Ausgabe ist in Tagen bzw. in Sekunden aufgelöst.\n\n``` {.raw}\nprint(np.datetime64('today') - np.datetime64('YYYY-MM-DD', 'D'))\nprint(np.datetime64('now') - np.datetime64('YYYY-MM-DDTHH:MM', 's'))\n```\n\n#### Pandas\nIn Pandas werden die Schlüsselwörter `pd.to_datetime('today')` und `pd.to_datetime('now')` in Nanosekunden aufgelöst.  \n\n``` {.raw}\n(pd.to_datetime('today') - pd.to_datetime('YYYY-MM-DD')).days\n(pd.to_datetime('now') - pd.to_datetime('YYYY-MM-DDTHH:MM')).total_seconds()\n```\n\n:::::\n::::\n:::\n\n## Zeitreihen einlesen\nInsbesondere das Modul Pandas bietet effiziente Möglichkeiten, um Datumsformate korrekt einzulesen. Im Folgenden wird das Einlesen von Zeitreihen mit NumPy und Pandas anhand von Strommarktdaten demonstriert. In @sec-übungenzeitreihen stehen verschiedene Übungsaufgaben zur Verfügung.\n\nDie Bundesnetzagentur veröffentlicht verschiedene Strommarktdaten, darunter die Großhandelspreise. Die Strommarktdaten der Bundesnetzagentur müssen manuell auf [https://www.smard.de/](https://www.smard.de/home/downloadcenter/download-marktdaten/) heruntergeladen werden. In diesem Skript werden Daten für das Jahr 2023 benutzt.\n\n::: {style=\"font-size: 90%;\"}\n| Daten | Dateiname |\n|---|---|\n| Großhandelspreise 2023 | Gro_handelspreise_202301010000_202401010000_Stunde.csv |\n| Großhandelspreise 2023 (Englisch) | Day-ahead_prices_202301010000_202401010000_Hour.csv |\n:::\n\n::: {#wrn-SMARD .callout-warning appearance=\"simple\" collapse=\"true\"}\n## SMARD Daten herunterladen\n\n:::: {layout=\"[[50, 50], [50, 50], [1]]\"}\n\nBeim der Auswahl des Zeitraums auf Akzeptieren klicken. \n\nDaten in Originalauflösung auswählen und auf Download klicken.\n\n![&nbsp;](00-bilder/smard-accept.png)\n\n![&nbsp;](00-bilder/smard-day-ahead-prices.png)\n\nDas Datumsformat der Dateien ist abhängig von der auf der Internetseite eingestellten Sprache (Deutsch/English).\n::::\n:::\n\n::: {.panel-tabset}\n\n### NumPy\nDer Versuch, die Datei mit `np.loadtxt()` einzulesen, führt zu verschiedenen Fehlermeldungen (Datentyp ist nicht numerisch, Spaltenzahl kann nicht ermittelt werden). Diesen wird durch Spezifizierung des Datentyps `dtype = str` und der Beschränkung auf die erste Zeile `max_rows = 1` begegnet.\n\n::: {.cell execution_count=99}\n``` {.python .cell-code}\ndateipfad = \"01-daten/Gro_handelspreise_202301010000_202401010000_Stunde.csv\"\npreise = np.loadtxt(fname = dateipfad, dtype = 'str', max_rows = 1)\npreise\n```\n\n::: {.cell-output .cell-output-display execution_count=99}\n```\narray(['\\ufeffDatum', 'von;Datum', 'bis;Deutschland/Luxemburg', '[€/MWh]',\n       'Originalauflösungen;∅', 'Anrainer', 'DE/LU', '[€/MWh]',\n       'Originalauflösungen;Belgien', '[€/MWh]',\n       'Originalauflösungen;Dänemark', '1', '[€/MWh]',\n       'Originalauflösungen;Dänemark', '2', '[€/MWh]',\n       'Originalauflösungen;Frankreich', '[€/MWh]',\n       'Originalauflösungen;Niederlande', '[€/MWh]',\n       'Originalauflösungen;Norwegen', '2', '[€/MWh]',\n       'Originalauflösungen;Österreich', '[€/MWh]',\n       'Originalauflösungen;Polen', '[€/MWh]',\n       'Originalauflösungen;Schweden', '4', '[€/MWh]',\n       'Originalauflösungen;Schweiz', '[€/MWh]',\n       'Originalauflösungen;Tschechien', '[€/MWh]',\n       'Originalauflösungen;DE/AT/LU', '[€/MWh]',\n       'Originalauflösungen;Italien', '(Nord)', '[€/MWh]',\n       'Originalauflösungen;Slowenien', '[€/MWh]',\n       'Originalauflösungen;Ungarn', '[€/MWh]', 'Originalauflösungen'],\n      dtype='<U31')\n```\n:::\n:::\n\n\nAuf diese Weise kann die erste Zeile eingelesen und das Semikolon als Zeichentrenner identifiziert werden. Außerdem sind Fehler mit der Zeichenkodierung auffällig. Deshalb werden der Zeichentrenner mit `delimiter = ';'` und die Kodierung der Datei mit `encoding = 'UTF-8'` übergeben.\n\n::: {.cell execution_count=100}\n``` {.python .cell-code}\ndateipfad = \"01-daten/Gro_handelspreise_202301010000_202401010000_Stunde.csv\"\npreise = np.loadtxt(fname = dateipfad, dtype = 'str', max_rows = 1, delimiter = ';', encoding = 'UTF-8')\npreise\n```\n\n::: {.cell-output .cell-output-display execution_count=100}\n```\narray(['\\ufeffDatum von', 'Datum bis',\n       'Deutschland/Luxemburg [€/MWh] Originalauflösungen',\n       '∅ Anrainer DE/LU [€/MWh] Originalauflösungen',\n       'Belgien [€/MWh] Originalauflösungen',\n       'Dänemark 1 [€/MWh] Originalauflösungen',\n       'Dänemark 2 [€/MWh] Originalauflösungen',\n       'Frankreich [€/MWh] Originalauflösungen',\n       'Niederlande [€/MWh] Originalauflösungen',\n       'Norwegen 2 [€/MWh] Originalauflösungen',\n       'Österreich [€/MWh] Originalauflösungen',\n       'Polen [€/MWh] Originalauflösungen',\n       'Schweden 4 [€/MWh] Originalauflösungen',\n       'Schweiz [€/MWh] Originalauflösungen',\n       'Tschechien [€/MWh] Originalauflösungen',\n       'DE/AT/LU [€/MWh] Originalauflösungen',\n       'Italien (Nord) [€/MWh] Originalauflösungen',\n       'Slowenien [€/MWh] Originalauflösungen',\n       'Ungarn [€/MWh] Originalauflösungen'], dtype='<U49')\n```\n:::\n:::\n\n\nEs verbleibt die Zeichenkette \"\\\\ufeff\" am Beginn des Arrays. Diese kennzeichnet die [Byte-Reihenfolge](https://de.wikipedia.org/wiki/Byte-Reihenfolge) der Datei. Diese kann mit der Übergabe der Kodierung `encoding = 'UTF-8-sig'` übersprungen werden ([Mark Tolonen auf stackoverflow.com](https://stackoverflow.com/a/72466627), [Python Dokumentation](https://docs.python.org/3/library/codecs.html)). Auf diese Weise wird die erste Zeile korrekt eingelesen, sodass die Anzahl der einzulesenden Zeilen mit `max_rows = 2` erweitert werden kann, um die Datentypen zu identifizieren.\n\n::: {.cell execution_count=101}\n``` {.python .cell-code}\npreise = np.loadtxt(fname = dateipfad, dtype = 'str', max_rows = 2, delimiter = ';', encoding = 'UTF-8-sig')\npreise\n```\n\n::: {.cell-output .cell-output-display execution_count=101}\n```\narray([['Datum von', 'Datum bis',\n        'Deutschland/Luxemburg [€/MWh] Originalauflösungen',\n        '∅ Anrainer DE/LU [€/MWh] Originalauflösungen',\n        'Belgien [€/MWh] Originalauflösungen',\n        'Dänemark 1 [€/MWh] Originalauflösungen',\n        'Dänemark 2 [€/MWh] Originalauflösungen',\n        'Frankreich [€/MWh] Originalauflösungen',\n        'Niederlande [€/MWh] Originalauflösungen',\n        'Norwegen 2 [€/MWh] Originalauflösungen',\n        'Österreich [€/MWh] Originalauflösungen',\n        'Polen [€/MWh] Originalauflösungen',\n        'Schweden 4 [€/MWh] Originalauflösungen',\n        'Schweiz [€/MWh] Originalauflösungen',\n        'Tschechien [€/MWh] Originalauflösungen',\n        'DE/AT/LU [€/MWh] Originalauflösungen',\n        'Italien (Nord) [€/MWh] Originalauflösungen',\n        'Slowenien [€/MWh] Originalauflösungen',\n        'Ungarn [€/MWh] Originalauflösungen'],\n       ['01.01.2023 00:00', '01.01.2023 01:00', '-5,17', '13,85',\n        '-4,39', '2,01', '2,01', '0,00', '-3,61', '119,32', '12,06',\n        '18,09', '2,01', '0,03', '4,84', '-', '195,90', '13,31', '19,76']],\n      dtype='<U49')\n```\n:::\n:::\n\n\nDie ersten beiden Spalten enthalten Datums- und Zeitinformationen, die folgenden numerische Werte, wobei eine Spalte mit '-' kodierte fehlende Werte enthält. Als Dezimaltrennzeichen wird das Komma verwendet. Da NumPy-Arrays immer nur einen Datentyp enthalten können, muss der Datensatz entsprechend aufgeteilt werden. Für die viertletzte Spalte ist zu prüfen, ob diese ausschließlich fehlende Werte enthält.\n\nZunächst wird der Datensatz vollständig als string eingelesen, die Spaltenbeschriftungen werden mit `skiprows = 1` übersprungen.\n\n::: {.cell execution_count=102}\n``` {.python .cell-code}\npreise = np.loadtxt(fname = dateipfad, dtype = 'str', delimiter = ';', encoding = 'UTF-8-sig', skiprows = 1)\n```\n:::\n\n\nAnschließend werden im ersten Schritt die Datumsspalten isoliert. NumPy unterstützt keine String-Formatierung, die Zeitstempel müssen deshalb manuell von '01.01.2023 00:00' in die Formatierung nach ISO 8601 'YYYY-MM-DDThh:mm' konvertiert werden.\n\n::: {.cell execution_count=103}\n``` {.python .cell-code}\n# Datumsspalten isolieren\npreise_date = preise[ : , 0:2]\n\n# Zeichenkette manuell ins Format ISO 8601 bringen\n## Spalte 0\n### neues Array anlegen\npreise_datumvon = np.array([], dtype = 'datetime64')\n\nfor element in preise_date[ : , 0]:\n\n  # string umstellen\n  neues_element = element[6:10] + '-' + \\\n  element[3:5] + '-' + \\\n  element[0:2] + 'T' + \\\n  element[11:13] +  ':' + \\\n  element[14:]\n\n  # in datetime64 konvertieren\n  neues_element = np.datetime64(neues_element)\n\n  # anhängen\n  preise_datumvon = np.append(preise_datumvon, neues_element)\n\n## Spalte 1\n### neues Array anlegen\npreise_datumbis = np.array([], dtype = 'datetime64')\n\nfor element in preise_date[ : , 1]:\n\n  # string umstellen\n  neues_element = element[6:10] + '-' + \\\n  element[3:5] + '-' + \\\n  element[0:2] + 'T' + \\\n  element[11:13] +  ':' + \\\n  element[14:]\n\n  # in datetime64 konvertieren\n  neues_element = np.datetime64(neues_element)\n\n  # anhängen\n  preise_datumbis = np.append(preise_datumbis, neues_element)\n\n# die letzten 4 Elemente angucken\nprint(preise_datumvon[-4:], preise_datumvon.dtype)\nprint(preise_datumbis[-4:], preise_datumbis.dtype)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['2023-12-31T20:00' '2023-12-31T21:00' '2023-12-31T22:00'\n '2023-12-31T23:00'] datetime64[m]\n['2023-12-31T21:00' '2023-12-31T22:00' '2023-12-31T23:00'\n '2024-01-01T00:00'] datetime64[m]\n```\n:::\n:::\n\n\nIm zweiten Schritt wird geprüft, ob die viertletzte Spalte ausschließlich fehlende Werte enthält. Die Position der Spalte ist zwar bekannt, wird aber dennoch mit der Funktion `np.argwhere()` ermittelt. Mit der Funktion `len(np.unique())` werden die einzigartigen Werte abgezählt. \n\n::: {.cell execution_count=104}\n``` {.python .cell-code}\n# numerische Spalten isolieren\npreise_numeric = preise[ : , 2:]\n\n# Position der Spalte mit fehlendem Wert '-' in der nullten Zeile finden\nposition = np.argwhere(preise_numeric[0, : ] == '-')\nprint(\"Spaltenindex:\", position)\n\n# prüfen, welche Werte in der Spalte vorkommen\nprint(\"Anzahl einzigartiger Werte:\", len(np.unique(preise_numeric[:, position])))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSpaltenindex: [[13]]\nAnzahl einzigartiger Werte: 1\n```\n:::\n:::\n\n\nDa die viertletzte Spalte ausschließlich das Zeichen '-' enthält, kann die Spalte entfernt werden. Anschließend kann der Datentyp als Fließkommazahl deklariert werden. Dazu ist es erforderlich, mit `np.char.replace(preise_numeric, ',', '.')` das Dezimaltrennzeichen Komma durch den Punkt zu ersetzen. Die Spaltennamen müssen separat gespeichert werden.\n\n::: {.cell execution_count=105}\n``` {.python .cell-code}\n# Spalte mit fehlenden Werten entfernen\npreise_numeric = np.delete(arr = preise_numeric, obj = position, axis = 1) # axis 1 = columns\n\n# Dezimaltrennzeichen ersetzen\npreise_numeric = np.char.replace(preise_numeric, ',', '.')\npreise_numeric = preise_numeric.astype('float64')\n\n# Spaltennamen speichern\npreise_numeric_colnames = np.loadtxt(fname = dateipfad, dtype = 'str', delimiter = ';', encoding = 'UTF-8-sig', max_rows = 1)\npreise_numeric_colnames = preise_numeric_colnames[2:] # Datumsspalten entfernen\npreise_numeric_colnames = np.delete(arr = preise_numeric_colnames, obj = position)\n\nprint(preise_numeric_colnames, \"\\n\")\nprint(preise_numeric[0:2, :], preise_numeric.dtype)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['Deutschland/Luxemburg [€/MWh] Originalauflösungen'\n '∅ Anrainer DE/LU [€/MWh] Originalauflösungen'\n 'Belgien [€/MWh] Originalauflösungen'\n 'Dänemark 1 [€/MWh] Originalauflösungen'\n 'Dänemark 2 [€/MWh] Originalauflösungen'\n 'Frankreich [€/MWh] Originalauflösungen'\n 'Niederlande [€/MWh] Originalauflösungen'\n 'Norwegen 2 [€/MWh] Originalauflösungen'\n 'Österreich [€/MWh] Originalauflösungen'\n 'Polen [€/MWh] Originalauflösungen'\n 'Schweden 4 [€/MWh] Originalauflösungen'\n 'Schweiz [€/MWh] Originalauflösungen'\n 'Tschechien [€/MWh] Originalauflösungen'\n 'Italien (Nord) [€/MWh] Originalauflösungen'\n 'Slowenien [€/MWh] Originalauflösungen'\n 'Ungarn [€/MWh] Originalauflösungen'] \n\n[[-5.1700e+00  1.3850e+01 -4.3900e+00  2.0100e+00  2.0100e+00  0.0000e+00\n  -3.6100e+00  1.1932e+02  1.2060e+01  1.8090e+01  2.0100e+00  3.0000e-02\n   4.8400e+00  1.9590e+02  1.3310e+01  1.9760e+01]\n [-1.0700e+00  9.7900e+00 -1.7500e+00  1.3800e+00  1.3800e+00 -1.0000e-01\n  -1.4600e+00  1.0883e+02 -1.0000e-01  5.7500e+00  1.3800e+00 -7.2500e+00\n  -3.5000e-01  1.9109e+02 -7.0000e-02  1.9000e-01]] float64\n```\n:::\n:::\n\n\n### Pandas\nZunächst wird die Datei der Großhandelspreise mit der Funktion `pd.read_csv()` eingelesen und der Erfolg durch Aufruf der Funktionen `pd.info()` kontrolliert.\n\n::: {.cell execution_count=106}\n``` {.python .cell-code}\ndateipfad = \"01-daten/Gro_handelspreise_202301010000_202401010000_Stunde.csv\"\npreise = pd.read_csv(filepath_or_buffer = dateipfad)\nprint(preise.info())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nMultiIndex: 8760 entries, ('01.01.2023 00:00;01.01.2023 01:00;-5', '17;13', '85;-4', '39;2', '01;2', '01;0', '00;-3', '61;119', '32;12', '06;18', '09;2', '01;0', '03;4', '84;-;195', '90;13', '31;19') to ('31.12.2023 23:00;01.01.2024 00:00;2', '44;19', '26;3', '17;26', '87;26', '87;3', '64;3', '17;59', '31;9', '35;35', '70;26', '87;9', '51;7', '44;-;106', '12;11', '02;14')\nData columns (total 1 columns):\n #   Column                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Non-Null Count  Dtype\n---  ------                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          --------------  -----\n 0   Datum von;Datum bis;Deutschland/Luxemburg [€/MWh] Originalauflösungen;∅ Anrainer DE/LU [€/MWh] Originalauflösungen;Belgien [€/MWh] Originalauflösungen;Dänemark 1 [€/MWh] Originalauflösungen;Dänemark 2 [€/MWh] Originalauflösungen;Frankreich [€/MWh] Originalauflösungen;Niederlande [€/MWh] Originalauflösungen;Norwegen 2 [€/MWh] Originalauflösungen;Österreich [€/MWh] Originalauflösungen;Polen [€/MWh] Originalauflösungen;Schweden 4 [€/MWh] Originalauflösungen;Schweiz [€/MWh] Originalauflösungen;Tschechien [€/MWh] Originalauflösungen;DE/AT/LU [€/MWh] Originalauflösungen;Italien (Nord) [€/MWh] Originalauflösungen;Slowenien [€/MWh] Originalauflösungen;Ungarn [€/MWh] Originalauflösungen  8760 non-null   int64\ndtypes: int64(1)\nmemory usage: 4.3+ MB\nNone\n```\n:::\n:::\n\n\nEs wird nur eine Spalte erkannt, da im Datensatz das Semikolon als Zeichentrenner verwendet wird, das nun mit dem Argument `sep = ';'` übergeben wird (Standardwert ist das Komma). Durch Aufruf der Funktionen `pd.info()` und `pd.head()` wird der Erfolg kontrolliert.\n\n::: {.cell execution_count=107}\n``` {.python .cell-code}\ndateipfad = \"01-daten/Gro_handelspreise_202301010000_202401010000_Stunde.csv\"\npreise = pd.read_csv(filepath_or_buffer = dateipfad, sep = ';')\nprint(preise.info())\npreise.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 8760 entries, 0 to 8759\nData columns (total 19 columns):\n #   Column                                             Non-Null Count  Dtype \n---  ------                                             --------------  ----- \n 0   Datum von                                          8760 non-null   object\n 1   Datum bis                                          8760 non-null   object\n 2   Deutschland/Luxemburg [€/MWh] Originalauflösungen  8760 non-null   object\n 3   ∅ Anrainer DE/LU [€/MWh] Originalauflösungen       8760 non-null   object\n 4   Belgien [€/MWh] Originalauflösungen                8760 non-null   object\n 5   Dänemark 1 [€/MWh] Originalauflösungen             8760 non-null   object\n 6   Dänemark 2 [€/MWh] Originalauflösungen             8760 non-null   object\n 7   Frankreich [€/MWh] Originalauflösungen             8760 non-null   object\n 8   Niederlande [€/MWh] Originalauflösungen            8760 non-null   object\n 9   Norwegen 2 [€/MWh] Originalauflösungen             8760 non-null   object\n 10  Österreich [€/MWh] Originalauflösungen             8760 non-null   object\n 11  Polen [€/MWh] Originalauflösungen                  8760 non-null   object\n 12  Schweden 4 [€/MWh] Originalauflösungen             8760 non-null   object\n 13  Schweiz [€/MWh] Originalauflösungen                8760 non-null   object\n 14  Tschechien [€/MWh] Originalauflösungen             8760 non-null   object\n 15  DE/AT/LU [€/MWh] Originalauflösungen               8760 non-null   object\n 16  Italien (Nord) [€/MWh] Originalauflösungen         8760 non-null   object\n 17  Slowenien [€/MWh] Originalauflösungen              8760 non-null   object\n 18  Ungarn [€/MWh] Originalauflösungen                 8760 non-null   object\ndtypes: object(19)\nmemory usage: 1.3+ MB\nNone\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=107}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Datum von</th>\n      <th>Datum bis</th>\n      <th>Deutschland/Luxemburg [€/MWh] Originalauflösungen</th>\n      <th>∅ Anrainer DE/LU [€/MWh] Originalauflösungen</th>\n      <th>Belgien [€/MWh] Originalauflösungen</th>\n      <th>Dänemark 1 [€/MWh] Originalauflösungen</th>\n      <th>Dänemark 2 [€/MWh] Originalauflösungen</th>\n      <th>Frankreich [€/MWh] Originalauflösungen</th>\n      <th>Niederlande [€/MWh] Originalauflösungen</th>\n      <th>Norwegen 2 [€/MWh] Originalauflösungen</th>\n      <th>Österreich [€/MWh] Originalauflösungen</th>\n      <th>Polen [€/MWh] Originalauflösungen</th>\n      <th>Schweden 4 [€/MWh] Originalauflösungen</th>\n      <th>Schweiz [€/MWh] Originalauflösungen</th>\n      <th>Tschechien [€/MWh] Originalauflösungen</th>\n      <th>DE/AT/LU [€/MWh] Originalauflösungen</th>\n      <th>Italien (Nord) [€/MWh] Originalauflösungen</th>\n      <th>Slowenien [€/MWh] Originalauflösungen</th>\n      <th>Ungarn [€/MWh] Originalauflösungen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>01.01.2023 00:00</td>\n      <td>01.01.2023 01:00</td>\n      <td>-5,17</td>\n      <td>13,85</td>\n      <td>-4,39</td>\n      <td>2,01</td>\n      <td>2,01</td>\n      <td>0,00</td>\n      <td>-3,61</td>\n      <td>119,32</td>\n      <td>12,06</td>\n      <td>18,09</td>\n      <td>2,01</td>\n      <td>0,03</td>\n      <td>4,84</td>\n      <td>-</td>\n      <td>195,90</td>\n      <td>13,31</td>\n      <td>19,76</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>01.01.2023 01:00</td>\n      <td>01.01.2023 02:00</td>\n      <td>-1,07</td>\n      <td>9,79</td>\n      <td>-1,75</td>\n      <td>1,38</td>\n      <td>1,38</td>\n      <td>-0,10</td>\n      <td>-1,46</td>\n      <td>108,83</td>\n      <td>-0,10</td>\n      <td>5,75</td>\n      <td>1,38</td>\n      <td>-7,25</td>\n      <td>-0,35</td>\n      <td>-</td>\n      <td>191,09</td>\n      <td>-0,07</td>\n      <td>0,19</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>01.01.2023 02:00</td>\n      <td>01.01.2023 03:00</td>\n      <td>-1,47</td>\n      <td>8,91</td>\n      <td>-1,46</td>\n      <td>0,09</td>\n      <td>0,09</td>\n      <td>-1,33</td>\n      <td>-1,52</td>\n      <td>102,39</td>\n      <td>-0,66</td>\n      <td>5,27</td>\n      <td>0,09</td>\n      <td>-3,99</td>\n      <td>-0,97</td>\n      <td>-</td>\n      <td>187,95</td>\n      <td>-0,47</td>\n      <td>0,07</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>01.01.2023 03:00</td>\n      <td>01.01.2023 04:00</td>\n      <td>-5,08</td>\n      <td>6,58</td>\n      <td>-5,27</td>\n      <td>0,08</td>\n      <td>0,08</td>\n      <td>-4,08</td>\n      <td>-5,00</td>\n      <td>92,36</td>\n      <td>-1,99</td>\n      <td>5,74</td>\n      <td>0,08</td>\n      <td>-7,71</td>\n      <td>-1,93</td>\n      <td>-</td>\n      <td>187,82</td>\n      <td>-1,56</td>\n      <td>0,01</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>01.01.2023 04:00</td>\n      <td>01.01.2023 05:00</td>\n      <td>-4,49</td>\n      <td>5,42</td>\n      <td>-4,41</td>\n      <td>0,05</td>\n      <td>0,05</td>\n      <td>-4,16</td>\n      <td>-4,60</td>\n      <td>82,66</td>\n      <td>-2,42</td>\n      <td>5,22</td>\n      <td>0,05</td>\n      <td>-9,71</td>\n      <td>-3,07</td>\n      <td>-</td>\n      <td>187,74</td>\n      <td>-1,94</td>\n      <td>-0,77</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n&nbsp;\n\nIn der Ausgabe ist am Datentyp object erkennbar, dass für keine Spalte der Datentyp erkannt wurde. In der Darstellung der ersten Zeilen des Datensatzes ist das Komma als Dezimaltrennzeichen zu sehen, der Standardwert der Funktion `pd.read_csv()` ist aber der Punkt. Nach Übergabe des Dezimaltrennzeichens sollten die numerischen Spalten korrekt erkannt werden.\n\n::: {.cell execution_count=108}\n``` {.python .cell-code}\npreise = pd.read_csv(filepath_or_buffer = dateipfad, sep = ';', decimal = ',')\nprint(preise.info())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 8760 entries, 0 to 8759\nData columns (total 19 columns):\n #   Column                                             Non-Null Count  Dtype  \n---  ------                                             --------------  -----  \n 0   Datum von                                          8760 non-null   object \n 1   Datum bis                                          8760 non-null   object \n 2   Deutschland/Luxemburg [€/MWh] Originalauflösungen  8760 non-null   float64\n 3   ∅ Anrainer DE/LU [€/MWh] Originalauflösungen       8760 non-null   float64\n 4   Belgien [€/MWh] Originalauflösungen                8760 non-null   float64\n 5   Dänemark 1 [€/MWh] Originalauflösungen             8760 non-null   float64\n 6   Dänemark 2 [€/MWh] Originalauflösungen             8760 non-null   float64\n 7   Frankreich [€/MWh] Originalauflösungen             8760 non-null   float64\n 8   Niederlande [€/MWh] Originalauflösungen            8760 non-null   float64\n 9   Norwegen 2 [€/MWh] Originalauflösungen             8760 non-null   float64\n 10  Österreich [€/MWh] Originalauflösungen             8760 non-null   float64\n 11  Polen [€/MWh] Originalauflösungen                  8760 non-null   float64\n 12  Schweden 4 [€/MWh] Originalauflösungen             8760 non-null   float64\n 13  Schweiz [€/MWh] Originalauflösungen                8760 non-null   float64\n 14  Tschechien [€/MWh] Originalauflösungen             8760 non-null   float64\n 15  DE/AT/LU [€/MWh] Originalauflösungen               8760 non-null   object \n 16  Italien (Nord) [€/MWh] Originalauflösungen         8760 non-null   float64\n 17  Slowenien [€/MWh] Originalauflösungen              8760 non-null   float64\n 18  Ungarn [€/MWh] Originalauflösungen                 8760 non-null   float64\ndtypes: float64(16), object(3)\nmemory usage: 1.3+ MB\nNone\n```\n:::\n:::\n\n\nDer Datentyp der Spalte DE/AT/LU [€/MWh] Originalauflösungen wird nicht als float64 erkannt. In der Ausgabe ist zu sehen, dass wenigstens in den ersten Zeilen fehlende Werte durch '-' markiert sind. Mittels der Methode `.describe()` kann überprüft werden, ob die Spalte überhaupt numerische Werte enthält.\n\n::: {.cell execution_count=109}\n``` {.python .cell-code}\npreise['DE/AT/LU [€/MWh] Originalauflösungen'].describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=109}\n```\ncount     8760\nunique       1\ntop          -\nfreq      8760\nName: DE/AT/LU [€/MWh] Originalauflösungen, dtype: object\n```\n:::\n:::\n\n\nDa dies nicht der Fall ist, kann die Spalte entfernt werden. Anschließend können die ersten beiden Spalten mit der Funktion `pd.to_datetime()` in ein Datumsformat konvertiert werden. Eine Zelle enthält Zeichenketten im Schema '01.01.2023 00:00'. Mit Hilfe der [strftime-Dokumentation](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior) kann der Funktion das Datumsformat übergeben werden.\n\n::: {.cell execution_count=110}\n``` {.python .cell-code}\npreise.drop(labels = 'DE/AT/LU [€/MWh] Originalauflösungen', axis = 'columns', inplace = True)\n\n## Datumsspalten konvertieren\npreise['Datum von'] = pd.to_datetime(preise['Datum von'], format = \"%d.%m.%Y %H:%M\")\npreise['Datum bis'] = pd.to_datetime(preise['Datum bis'], format = \"%d.%m.%Y %H:%M\")\nprint(preise.info())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 8760 entries, 0 to 8759\nData columns (total 18 columns):\n #   Column                                             Non-Null Count  Dtype         \n---  ------                                             --------------  -----         \n 0   Datum von                                          8760 non-null   datetime64[ns]\n 1   Datum bis                                          8760 non-null   datetime64[ns]\n 2   Deutschland/Luxemburg [€/MWh] Originalauflösungen  8760 non-null   float64       \n 3   ∅ Anrainer DE/LU [€/MWh] Originalauflösungen       8760 non-null   float64       \n 4   Belgien [€/MWh] Originalauflösungen                8760 non-null   float64       \n 5   Dänemark 1 [€/MWh] Originalauflösungen             8760 non-null   float64       \n 6   Dänemark 2 [€/MWh] Originalauflösungen             8760 non-null   float64       \n 7   Frankreich [€/MWh] Originalauflösungen             8760 non-null   float64       \n 8   Niederlande [€/MWh] Originalauflösungen            8760 non-null   float64       \n 9   Norwegen 2 [€/MWh] Originalauflösungen             8760 non-null   float64       \n 10  Österreich [€/MWh] Originalauflösungen             8760 non-null   float64       \n 11  Polen [€/MWh] Originalauflösungen                  8760 non-null   float64       \n 12  Schweden 4 [€/MWh] Originalauflösungen             8760 non-null   float64       \n 13  Schweiz [€/MWh] Originalauflösungen                8760 non-null   float64       \n 14  Tschechien [€/MWh] Originalauflösungen             8760 non-null   float64       \n 15  Italien (Nord) [€/MWh] Originalauflösungen         8760 non-null   float64       \n 16  Slowenien [€/MWh] Originalauflösungen              8760 non-null   float64       \n 17  Ungarn [€/MWh] Originalauflösungen                 8760 non-null   float64       \ndtypes: datetime64[ns](2), float64(16)\nmemory usage: 1.2 MB\nNone\n```\n:::\n:::\n\n\nWenn der innere Aufbau einer Datei bekannt ist, können die notwendigen Parameter auch direkt beim Einlesen mit `pd.read_csv` übergeben werden (Argumente `usecols`, `parse_dates` und `date_format`).\n\n::: {.cell execution_count=111}\n``` {.python .cell-code}\npreise = pd.read_csv(filepath_or_buffer = dateipfad, sep = ';', decimal = ',',\n                     usecols = list(range(0, 15)) + list(range(16, 19)), # Auswahl der einzulesenden Spalten\n                     parse_dates = ['Datum von', 'Datum bis'], date_format = \"%d.%m.%Y %H:%M\") # Formatierung des Datums\nprint(preise.info())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 8760 entries, 0 to 8759\nData columns (total 18 columns):\n #   Column                                             Non-Null Count  Dtype         \n---  ------                                             --------------  -----         \n 0   Datum von                                          8760 non-null   datetime64[ns]\n 1   Datum bis                                          8760 non-null   datetime64[ns]\n 2   Deutschland/Luxemburg [€/MWh] Originalauflösungen  8760 non-null   float64       \n 3   ∅ Anrainer DE/LU [€/MWh] Originalauflösungen       8760 non-null   float64       \n 4   Belgien [€/MWh] Originalauflösungen                8760 non-null   float64       \n 5   Dänemark 1 [€/MWh] Originalauflösungen             8760 non-null   float64       \n 6   Dänemark 2 [€/MWh] Originalauflösungen             8760 non-null   float64       \n 7   Frankreich [€/MWh] Originalauflösungen             8760 non-null   float64       \n 8   Niederlande [€/MWh] Originalauflösungen            8760 non-null   float64       \n 9   Norwegen 2 [€/MWh] Originalauflösungen             8760 non-null   float64       \n 10  Österreich [€/MWh] Originalauflösungen             8760 non-null   float64       \n 11  Polen [€/MWh] Originalauflösungen                  8760 non-null   float64       \n 12  Schweden 4 [€/MWh] Originalauflösungen             8760 non-null   float64       \n 13  Schweiz [€/MWh] Originalauflösungen                8760 non-null   float64       \n 14  Tschechien [€/MWh] Originalauflösungen             8760 non-null   float64       \n 15  Italien (Nord) [€/MWh] Originalauflösungen         8760 non-null   float64       \n 16  Slowenien [€/MWh] Originalauflösungen              8760 non-null   float64       \n 17  Ungarn [€/MWh] Originalauflösungen                 8760 non-null   float64       \ndtypes: datetime64[ns](2), float64(16)\nmemory usage: 1.2 MB\nNone\n```\n:::\n:::\n\n\n:::\n\n## Zugriff auf Zeitreihen\nPandas bietet zahlreiche Attribute und Methoden, um Informationen aus `datetime64`-Objekten auszulesen. NumPy unterstützt vergleichbare Funktionen derzeit nicht nativ. Eine Übersicht aller verfügbaren Attribute und Methoden liefert  `dir(pd.to_datetime(0))`.\n\n::: {.cell execution_count=112}\n``` {.python .cell-code}\n# Attribute\nprint(\"Jahr:\", pd.to_datetime(0).year)\nprint(\"Monat:\", pd.to_datetime(0).month)\nprint(\"Tag:\", pd.to_datetime(0).day)\nprint(\"Stunde:\", pd.to_datetime(0).hour)\nprint(\"Minute:\", pd.to_datetime(0).minute)\nprint(\"Sekunde:\", pd.to_datetime(0).second)\nprint(\"Tag des Jahres:\", pd.to_datetime(0).dayofyear)\nprint(\"Wochentag:\", pd.to_datetime(0).dayofweek)\nprint(\"Tage im Monat:\", pd.to_datetime(0).days_in_month)\nprint(\"Schaltjahr:\", pd.to_datetime(0).is_leap_year)\n\n# Methoden\nprint(\"\\nDatum:\", pd.to_datetime(0).date())\nprint(\"Zeit:\", pd.to_datetime(0).time())\nprint(\"Wochentag (0-6):\", pd.to_datetime(0).weekday())\nprint(\"Monatsname:\",  pd.to_datetime(0).month_name())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nJahr: 1970\nMonat: 1\nTag: 1\nStunde: 0\nMinute: 0\nSekunde: 0\nTag des Jahres: 1\nWochentag: 3\nTage im Monat: 31\nSchaltjahr: False\n\nDatum: 1970-01-01\nZeit: 00:00:00\nWochentag (0-6): 3\nMonatsname: January\n```\n:::\n:::\n\n\nFür `pd.Series` erfolgt der Zugriff über den .dt-Operator (siehe [.dt accessor](https://pandas.pydata.org/docs/user_guide/basics.html#basics-dt-accessors)). Der Zugriff auf verschiedene Informationen über ein Attribut (ohne Klammern) oder über eine Methode (mit Klammern) unterscheidet sich jedoch teilweise (siehe folgendes Beispiel).\n\n:::: {.callout-note collapse=\"true\"}\n## Der dt-Operator\n\n::: {.cell execution_count=113}\n``` {.python .cell-code}\n# Attribute\nprint(\"Datum:\", pd.Series(pd.to_datetime(0)).dt.date) # Unterschied\nprint(\"Zeit:\", pd.Series(pd.to_datetime(0)).dt.time) # Unterschied\nprint(\"Jahr\", pd.Series(pd.to_datetime(0)).dt.year)\nprint(\"Monat\", pd.Series(pd.to_datetime(0)).dt.month)\nprint(\"Tag\", pd.Series(pd.to_datetime(0)).dt.day)\nprint(\"Stunde\", pd.Series(pd.to_datetime(0)).dt.hour)\nprint(\"Minute\", pd.Series(pd.to_datetime(0)).dt.minute)\nprint(\"Sekunde\", pd.Series(pd.to_datetime(0)).dt.second)\n\nprint(\"\\nTag des Jahres\", pd.Series(pd.to_datetime(0)).dt.dayofyear)\nprint(\"Wochentag:\", pd.Series(pd.to_datetime(0)).dt.dayofweek)\nprint(\"Wochentag:\", pd.Series(pd.to_datetime(0)).dt.weekday) # Unterschied\nprint(\"Tage im Monat:\", pd.Series(pd.to_datetime(0)).dt.days_in_month)\nprint(\"Schaltjahr:\", pd.Series(pd.to_datetime(0)).dt.is_leap_year)\n\n# Methoden\nprint(\"\\nName des Monats:\", pd.Series(pd.to_datetime(0)).dt.month_name())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDatum: 0    1970-01-01\ndtype: object\nZeit: 0    00:00:00\ndtype: object\nJahr 0    1970\ndtype: int32\nMonat 0    1\ndtype: int32\nTag 0    1\ndtype: int32\nStunde 0    0\ndtype: int32\nMinute 0    0\ndtype: int32\nSekunde 0    0\ndtype: int32\n\nTag des Jahres 0    1\ndtype: int32\nWochentag: 0    3\ndtype: int32\nWochentag: 0    3\ndtype: int32\nTage im Monat: 0    31\ndtype: int32\nSchaltjahr: 0    False\ndtype: bool\n\nName des Monats: 0    January\ndtype: object\n```\n:::\n:::\n\n\n::::\n\nDie im vorherigen Abschnitt eingelesenen Großhandelspreise für Strom 2023 sollen auf die Unterschiede an Werktagen und am Wochenende untersucht werden. **Vergleichen Sie den durchschnittlichen Strompreis im Gebiet Deutschland/Luxemburg an Werktagen (Montag - Freitag) mit dem durchschnittlichen Strompreis am Wochenende.**\n\n::: {#tip-Durchschnittspreis .callout-tip collapse=\"true\"}\n## Musterlösung Strompreisvergleich\n\n::: {.cell execution_count=114}\n``` {.python .cell-code}\n## Zugriff mit .dt für pd.Series\n# Werktage und Wochenende unterscheiden\nwerktags = preise['Datum von'].dt.weekday.isin(list(range(0, 5)))\nwochenende = preise['Datum von'].dt.weekday.isin(list(range(5, 7)))\n\nprint(werktags.head())\nprint(wochenende.head())\n\n# Preise vergleichen\npreis_werktags = preise.loc[werktags, 'Deutschland/Luxemburg [€/MWh] Originalauflösungen'].mean()\npreis_wochenende = preise.loc[wochenende, 'Deutschland/Luxemburg [€/MWh] Originalauflösungen'].mean()\n\nprint(f\"\\nDurchschnittspreis werktags: {preis_werktags:.2f} [€/MWh]\\nDurchschnittspreis am Wochenende: {preis_wochenende:.2f} [€/MWh]\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0    False\n1    False\n2    False\n3    False\n4    False\nName: Datum von, dtype: bool\n0    True\n1    True\n2    True\n3    True\n4    True\nName: Datum von, dtype: bool\n\nDurchschnittspreis werktags: 103.18 [€/MWh]\nDurchschnittspreis am Wochenende: 75.34 [€/MWh]\n```\n:::\n:::\n\n\n:::\n\n## Fehlende Werte in Zeitreihen\nNumPy und Pandas unterstützen `NaT` für `np.datetime64`, `np.timedelta64`\n\n  - NumPy: <https://numpy.org/doc/stable/reference/arrays.datetime.html>  \n  `NAT`, in any combination of lowercase/uppercase letters, for a “Not A Time” \n  \n  - Pandas: <https://pandas.pydata.org/docs/user_guide/missing_data.html>\n\n::: {#wrn-logicNaT .callout-warning appearance=\"simple\" collapse=\"false\"}\n## Achtung Logik!\n\nDie logische Abfrage fehlender Werte unterscheidet sich für `None`, `np.nan` und `pd.NA` und `pd.NaT`. \n\n::: {.cell execution_count=115}\n``` {.python .cell-code}\nbool_values = [None, float('nan'), pd.NA, pd.NaT]\n\nfor element in bool_values:\n  try:\n    bool_value = bool(element)\n  except TypeError as error:\n      print(error)\n  else:\n    print(\"Wahrheitswert von\", element, \"ist\", bool_value)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWahrheitswert von None ist False\nWahrheitswert von nan ist True\nboolean value of NA is ambiguous\nWahrheitswert von NaT ist True\n```\n:::\n:::\n\n\nDies gilt auch für die Wertgleichheit.\n\n::: {.cell execution_count=116}\n``` {.python .cell-code}\nfor element in bool_values:\n  try:\n    result = element == element\n  except TypeError as error:\n      print(error)\n  else:\n    print(\"Wertgleichheit von\", element, \"ist\", result)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWertgleichheit von None ist True\nWertgleichheit von nan ist False\nWertgleichheit von <NA> ist <NA>\nWertgleichheit von NaT ist False\n```\n:::\n:::\n\n\n:::\n\n## Übungen: Zeitreihen einlesen {#sec-übungenzeitreihen}\n\n::: {.border layout=\"[[5, 90, 5], [1], [1]]\"}\n\n&nbsp;\n\n\"everybody I know has war stories about cleaning up lousy datasets\"  \nNicholas J. Cox\n\n&nbsp;\n\n&nbsp;\n\nCox, Nicholas J. 2004: Exploratory Data Mining and Data Cleaning. Book Review 9. In: Journal of Statistical Software 2004, Volume 11. <https://www.jstatsoft.org/article/view/v011b09/30>\n\n:::\n\nDie folgenden Übungen trainieren die Anwendung der in diesem Kapitel vorgestellten Werkzeuge und können mit NumPy oder mit Pandas gelöst werden.\n\n#### Leicht: Englisches Datumsformat einlesen\n\n**Aufgabe: Lesen Sie die Datei Dateipfad: '01-daten/Day-ahead_prices_202301010000_202401010000_Hour.csv' so ein, dass die Datentypen korrekt erkannt werden. (Hinweise zur Datei siehe @wrn-SMARD, siehe [Dokumentation strftime zur string-Formatierung](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior))**\n\n\n::: {#tip-strompreise .callout-tip collapse=\"true\"}\n## Musterlösung Strompreise\n\n:::: {.border}\n\n::: {.cell execution_count=117}\n``` {.python .cell-code}\nimport pandas as pd\n\ndateipfad = \"01-daten/Day-ahead_prices_202301010000_202401010000_Hour.csv\"\n\ndata = pd.read_csv(dateipfad, sep=\";\")   # Semikolon als Trennzeichen muss angegeben werden\ndata.info() # -> man sieht, dass Spalten 0 \"Start date\" und 1 \"End date\" als Dtype \"object\" erkannt werden\nprint(\"\\n\")\n\nprint(data.iloc[0:10, 0:2], \"\\n\") # anzeigen von ein paar Zeilen, um zu schauen wie die ersten beiden \"Object\" Spalten aufgebaut sind\n# Ausgabe lautet:\n# Start date: Jan 1, 2023 12:00 AM\n# End date Jan 1, 2023 1:00 AM\n# also bieten sich zwei Varianten an:\n\n# 1. Variante: beim Einlesen schon Datumsformat angeben:\ndata = pd.read_csv(dateipfad, sep=\";\" , parse_dates=[0,1], date_format=\"%b %d, %Y %I:%M %p\")   # Parameter heißt hier date_format, bei 2. Variante heißt er nur \"format\"\n\n# 2. Variante: Die beiden Spalten zu Anfang, die dtype object haben, seperat ändern nachdem die Datei schon eingelesen wurde:\ndata[\"Start date\"] = pd.to_datetime(data[\"Start date\"], format=\"%b %d, %Y %I:%M %p\")  #%b für Monatsangabe als Kürzel, %I für Stunde im AM/PM Format, %p für AM/PM\ndata[\"End date\"] = pd.to_datetime(data[\"End date\"], format=\"%b %d, %Y %I:%M %p\")\nprint(data.iloc[0:10, 0:2], \"\\n\")\n\ndata.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 8760 entries, 0 to 8759\nData columns (total 19 columns):\n #   Column                                           Non-Null Count  Dtype  \n---  ------                                           --------------  -----  \n 0   Start date                                       8760 non-null   object \n 1   End date                                         8760 non-null   object \n 2   Germany/Luxembourg [€/MWh] Original resolutions  8760 non-null   float64\n 3   ∅ DE/LU neighbours [€/MWh] Original resolutions  8760 non-null   float64\n 4   Belgium [€/MWh] Original resolutions             8760 non-null   float64\n 5   Denmark 1 [€/MWh] Original resolutions           8760 non-null   float64\n 6   Denmark 2 [€/MWh] Original resolutions           8760 non-null   float64\n 7   France [€/MWh] Original resolutions              8760 non-null   float64\n 8   Netherlands [€/MWh] Original resolutions         8760 non-null   float64\n 9   Norway 2 [€/MWh] Original resolutions            8760 non-null   float64\n 10  Austria [€/MWh] Original resolutions             8760 non-null   float64\n 11  Poland [€/MWh] Original resolutions              8760 non-null   float64\n 12  Sweden 4 [€/MWh] Original resolutions            8760 non-null   float64\n 13  Switzerland [€/MWh] Original resolutions         8760 non-null   float64\n 14  Czech Republic [€/MWh] Original resolutions      8760 non-null   float64\n 15  DE/AT/LU [€/MWh] Original resolutions            8760 non-null   object \n 16  Northern Italy [€/MWh] Original resolutions      8760 non-null   float64\n 17  Slovenia [€/MWh] Original resolutions            8760 non-null   float64\n 18  Hungary [€/MWh] Original resolutions             8760 non-null   float64\ndtypes: float64(16), object(3)\nmemory usage: 1.3+ MB\n\n\n             Start date              End date\n0  Jan 1, 2023 12:00 AM   Jan 1, 2023 1:00 AM\n1   Jan 1, 2023 1:00 AM   Jan 1, 2023 2:00 AM\n2   Jan 1, 2023 2:00 AM   Jan 1, 2023 3:00 AM\n3   Jan 1, 2023 3:00 AM   Jan 1, 2023 4:00 AM\n4   Jan 1, 2023 4:00 AM   Jan 1, 2023 5:00 AM\n5   Jan 1, 2023 5:00 AM   Jan 1, 2023 6:00 AM\n6   Jan 1, 2023 6:00 AM   Jan 1, 2023 7:00 AM\n7   Jan 1, 2023 7:00 AM   Jan 1, 2023 8:00 AM\n8   Jan 1, 2023 8:00 AM   Jan 1, 2023 9:00 AM\n9   Jan 1, 2023 9:00 AM  Jan 1, 2023 10:00 AM \n\n           Start date            End date\n0 2023-01-01 00:00:00 2023-01-01 01:00:00\n1 2023-01-01 01:00:00 2023-01-01 02:00:00\n2 2023-01-01 02:00:00 2023-01-01 03:00:00\n3 2023-01-01 03:00:00 2023-01-01 04:00:00\n4 2023-01-01 04:00:00 2023-01-01 05:00:00\n5 2023-01-01 05:00:00 2023-01-01 06:00:00\n6 2023-01-01 06:00:00 2023-01-01 07:00:00\n7 2023-01-01 07:00:00 2023-01-01 08:00:00\n8 2023-01-01 08:00:00 2023-01-01 09:00:00\n9 2023-01-01 09:00:00 2023-01-01 10:00:00 \n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 8760 entries, 0 to 8759\nData columns (total 19 columns):\n #   Column                                           Non-Null Count  Dtype         \n---  ------                                           --------------  -----         \n 0   Start date                                       8760 non-null   datetime64[ns]\n 1   End date                                         8760 non-null   datetime64[ns]\n 2   Germany/Luxembourg [€/MWh] Original resolutions  8760 non-null   float64       \n 3   ∅ DE/LU neighbours [€/MWh] Original resolutions  8760 non-null   float64       \n 4   Belgium [€/MWh] Original resolutions             8760 non-null   float64       \n 5   Denmark 1 [€/MWh] Original resolutions           8760 non-null   float64       \n 6   Denmark 2 [€/MWh] Original resolutions           8760 non-null   float64       \n 7   France [€/MWh] Original resolutions              8760 non-null   float64       \n 8   Netherlands [€/MWh] Original resolutions         8760 non-null   float64       \n 9   Norway 2 [€/MWh] Original resolutions            8760 non-null   float64       \n 10  Austria [€/MWh] Original resolutions             8760 non-null   float64       \n 11  Poland [€/MWh] Original resolutions              8760 non-null   float64       \n 12  Sweden 4 [€/MWh] Original resolutions            8760 non-null   float64       \n 13  Switzerland [€/MWh] Original resolutions         8760 non-null   float64       \n 14  Czech Republic [€/MWh] Original resolutions      8760 non-null   float64       \n 15  DE/AT/LU [€/MWh] Original resolutions            8760 non-null   object        \n 16  Northern Italy [€/MWh] Original resolutions      8760 non-null   float64       \n 17  Slovenia [€/MWh] Original resolutions            8760 non-null   float64       \n 18  Hungary [€/MWh] Original resolutions             8760 non-null   float64       \ndtypes: datetime64[ns](2), float64(16), object(1)\nmemory usage: 1.3+ MB\n```\n:::\n:::\n\n\nMusterlösung von Marc Sönnecken. Zur Verbesserung der Lesbarkeit wurde die Ausgabe mit `print(data.head(10))` ersetzt durch `print(data.iloc[0:10, 0:2], \"\\n\") `. Um sich einen Überblick über einen Datensatz zu verschaffen, ist die Methode `.head()` jedoch besser geeignet\n::::\n:::\n\n#### Mittel: Strommarktdaten Österreich\nDie Austrian Power Grid AG (APG) veröffentlicht Strommarktdaten unter [https://markttransparenz.apg.at/](https://markttransparenz.apg.at/de/markt/Markttransparenz/erzeugung/Erzeugung-pro-Typ). Unter dem Link können Erzeugungsdaten für das Jahr 2023 heruntergeladen werden.\n\n::: {#wrn-Strommarkt-Austria .callout-warning appearance=\"simple\" collapse=\"true\"}\n# Markttranzparenzdaten Österreich herunterladen\n\nNach der Auswahl des Zeitraums auf Exportieren klicken, dann erscheint die Schaltfläche Download. \n\n![&nbsp;](00-bilder/APG-erzeugungsdaten-2023-de.png){fig-alt=\"Ansicht der Eingabefelder, um die österreichischen Strommarktdaten herunterzuladen.\"}\n\n![&nbsp;](00-bilder/english/APG-generation-data-2023-en.png){fig-alt=\"Ansicht der Eingabefelder, um die österreichischen Strommarktdaten in englischer Sprache herunterzuladen.\"}\n\nDas Datumsformat der Dateien ist abhängig von der auf der Internetseite eingestellten Sprache (Deutsch/English).\n\n:::\n\n&nbsp;\n\nDiesem Skript ist folgende Datei angefügt. \n\n::: {style=\"font-size: 90%;\"}\n| Daten | Dateiname |\n|---|---|\n| Realisierte Stromerzeugung 2023 | AGPT_2022-12-31T23_00_00Z_2023-12-31T23_00_00Z_15M_de_2024-06-10T09_32_38Z.csv |\n:::\n\nIn dem Datensatz wird durch die Umstellung von Sommer- auf Winterzeit am letzten Sonntag im Oktober die Stunde 2 Uhr morgens doppelt eingetragen (dafür fehlt eine Stunde bei der Umstellung von Winter- auf Sommerzeit am letzten Sonntag im März). Die doppelte Stunde wird im Datensatz mit 2A bzw. 2B gekennzeichnet. (Mitteilung Austrian Power Grid AG vom 13.08.2024)\n\n![Zeitumstellung im österreichischen Datensatz](00-bilder/erzeugung-aut-zeitumstellung.png){fig-alt=\"Dargestellt ist ein Ausschnitt aus dem Datensatz der österreichischen Erzeugungsdaten. In den Spalten Zeit von [CET/CEST] und Zeit bis [CET/CEST] ist der Zeitraum vom 29.10.2023 02:00:00 bis 29.10.2023 03:00:00 gelb markiert, um die im vorausgehenden Text beschriebene Auffälligkeit zu illustrieren.\"}\n\n**Lesen Sie die Datei so ein, dass die Spalten mit Datums- und Zeitinformationen als datetime erkannt werden. Lösen Sie die Zeitumstellung so auf, dass jeder Tag 24 Stunden hat.**\n\n::: {#tip-Austria title=\"Musterlösung Strommarktdaten Österreich\" .callout-tip collapse=\"true\"}\n\n:::: {.callout-tip collapse=\"true\"}\n## einfache Variante\n\nDie einfachste Lösung ist es, Zeitreihen zu generieren und die Spalten 'Zeit von [CET/CEST]' und 'Zeit bis [CET/CEST]' damit zu ersetzen.\n```\nvon = pd.date_range(start = \"2023-01-01T00:00\", end = \"2023-12-31T23:45\", freq = '15min')\nbis = pd.date_range(start = \"2023-01-01T00:15\", end = \"2024-01-01T00:00\", freq = '15min')\n\nprint(von[8070:8078], \"\\n\")\nprint(bis[8070:8078], \"\\n\")\n```\n::::\n\n**Datei einlesen und String-Manipulation**\n\nZunächst wird die Datei eingelesen. Die Zellen, die sich nicht in datetime umwandeln lassen, können mit Python ausgegeben werden.\n\n::: {.cell execution_count=118}\n``` {.python .cell-code}\n# Datei einlesen\nerzeugung_austria = pd.read_csv(filepath_or_buffer = \"01-daten/AGPT_2022-12-31T23_00_00Z_2023-12-31T23_00_00Z_15M_de_2024-06-10T09_32_38Z.csv\",\n                                sep = \";\", decimal = \",\", thousands = \".\")\n\n# Zellen mit fehlerhaften datetime strings identifizieren\nprint(\"Spalte 'Zeit von [CET/CEST]'\")\ni = 0\nposition_element = []\nfor element in erzeugung_austria['Zeit von [CET/CEST]']:\n  try:\n    pd.to_datetime(element, format = \"%d.%m.%Y %H:%M:%S\")\n  except:\n    print(element)\n    position_element.append(i)\n  i += 1\nprint(\"\\nDie Zellen haben den Zeilenindex: \", position_element, \"\\n\")\n\nprint(\"Spalte 'Zeit bis [CET/CEST]'\")\ni = 0\nposition_element = []\nfor element in erzeugung_austria['Zeit bis [CET/CEST]']:\n  try:\n    pd.to_datetime(element, format = \"%d.%m.%Y %H:%M:%S\")\n  except:\n    print(element)\n    position_element.append(i)\n  i += 1\nprint(\"\\nDie Zellen haben den Zeilenindex: \", position_element, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSpalte 'Zeit von [CET/CEST]'\n29.10.2023 2A:00:00\n29.10.2023 2A:15:00\n29.10.2023 2A:30:00\n29.10.2023 2A:45:00\n29.10.2023 2B:00:00\n29.10.2023 2B:15:00\n29.10.2023 2B:30:00\n29.10.2023 2B:45:00\n\nDie Zellen haben den Zeilenindex:  [28900, 28901, 28902, 28903, 28904, 28905, 28906, 28907] \n\nSpalte 'Zeit bis [CET/CEST]'\n29.10.2023 2A:00:00\n29.10.2023 2A:15:00\n29.10.2023 2A:30:00\n29.10.2023 2A:45:00\n29.10.2023 2B:00:00\n29.10.2023 2B:15:00\n29.10.2023 2B:30:00\n29.10.2023 2B:45:00\n\nDie Zellen haben den Zeilenindex:  [28899, 28900, 28901, 28902, 28903, 28904, 28905, 28906] \n\n```\n:::\n:::\n\n\nDamit die Datumsspalten korrekt eingelesen werden können, werden die Zeichenfolgen \"2A\" und \"2B\" mit der Methode `str.replace()` durch \"02\" ersetzt. Dadurch wird eine Dublette im Datensatz erzeugt.\n\n::: {.cell execution_count=119}\n``` {.python .cell-code}\n# string replace & als Datum einlesen\n## Spalte Zeit von [CET/CEST]\nerzeugung_austria['Zeit von [CET/CEST]'] = erzeugung_austria['Zeit von [CET/CEST]'].str.replace(pat = '2A', repl = '02')\nerzeugung_austria['Zeit von [CET/CEST]'] = erzeugung_austria['Zeit von [CET/CEST]'].str.replace(pat = '2B', repl = '02')\n\nerzeugung_austria['Zeit von [CET/CEST]'] = pd.to_datetime(erzeugung_austria['Zeit von [CET/CEST]'], format = \"%d.%m.%Y %H:%M:%S\")\n\n## Spalte Zeit bis [CET/CEST]\nerzeugung_austria['Zeit bis [CET/CEST]'] = erzeugung_austria['Zeit bis [CET/CEST]'].str.replace(pat = '2A', repl = '02')\nerzeugung_austria['Zeit bis [CET/CEST]'] = erzeugung_austria['Zeit bis [CET/CEST]'].str.replace(pat = '2B', repl = '02')\n\nerzeugung_austria['Zeit bis [CET/CEST]'] = pd.to_datetime(erzeugung_austria['Zeit bis [CET/CEST]'], format = \"%d.%m.%Y %H:%M:%S\")\n\nprint(erzeugung_austria.info())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 35040 entries, 0 to 35039\nData columns (total 15 columns):\n #   Column                        Non-Null Count  Dtype         \n---  ------                        --------------  -----         \n 0   Zeit von [CET/CEST]           35040 non-null  datetime64[ns]\n 1   Zeit bis [CET/CEST]           35040 non-null  datetime64[ns]\n 2   Wind [MW]                     35040 non-null  float64       \n 3   Solar [MW]                    35040 non-null  float64       \n 4   Biomasse [MW]                 35040 non-null  float64       \n 5   Gas [MW]                      35040 non-null  float64       \n 6   Kohle [MW]                    35040 non-null  float64       \n 7   Öl [MW]                       35040 non-null  float64       \n 8   Geothermie [MW]               35040 non-null  float64       \n 9   Pumpspeicher [MW]             35040 non-null  float64       \n 10  Lauf- und Schwellwasser [MW]  35040 non-null  float64       \n 11  Speicher [MW]                 35040 non-null  float64       \n 12  Sonstige Erneuerbare [MW]     35040 non-null  float64       \n 13  Müll [MW]                     35040 non-null  float64       \n 14  Andere [MW]                   35040 non-null  float64       \ndtypes: datetime64[ns](2), float64(13)\nmemory usage: 4.0 MB\nNone\n```\n:::\n:::\n\n\n**Indexpositionen der doppelten und der fehlenden Stunde bestimmen**\n\nIm nächsten Schritt werden die Indexposition der doppelten und der fehlenden Stunde bestimmt. Dazu wird ein neues Objekt angelegt, das auf den Speicherbereich der Datumsspalten zugreift (was nicht zwingend erforderlich ist). Die Position der doppelten Stunde wird mit der Methode `pd.Series.duplicated()` bestimmt, die einen logischen Vektor zurückgibt. Dieser wird zum Slicing und der Ausgabe der Indexposition verwendet. Durch die Subtraktion von 4 wird der Index der ersten Stunde ausgegeben (der Datensatz ist auf Viertelstundenbasis).\n\n*doppelte Stunde*\n\n::: {.cell execution_count=120}\n``` {.python .cell-code}\n# neues Objekt anlegen\naustria_dates = erzeugung_austria[['Zeit von [CET/CEST]', 'Zeit bis [CET/CEST]']].copy()\n\n# Indexposition der doppelten Stunde bestimmen\n## Zeit von\nposition_doppelte_stunde_von = austria_dates['Zeit von [CET/CEST]'][austria_dates['Zeit von [CET/CEST]'].duplicated()].index - 4\n\nprint(f\"Die doppelte Stunde (Zeit von):\\n{austria_dates.loc[position_doppelte_stunde_von, 'Zeit von [CET/CEST]']}\\nsteht an Indexposition\\n {position_doppelte_stunde_von}\\n\"\nf\"\\nDie nächste Stunde lautet:\\n{austria_dates.loc[position_doppelte_stunde_von + 4, 'Zeit von [CET/CEST]']}\"\nf\"\\n\\nBeide Stunden sind identisch.\")\n\n### Ende der Verschiebung in Spalte Zeit von\nende_verschiebung_von = position_doppelte_stunde_von[-1]\nprint(f\"\\nDie Zeitverschiebung in der Spalte Zeit von endet bei Indexposition: {ende_verschiebung_von}\")\n\n## Zeit bis\nposition_doppelte_stunde_bis = austria_dates['Zeit bis [CET/CEST]'][austria_dates['Zeit bis [CET/CEST]'].duplicated()].index - 4\n\nprint(f\"\\n\\nDie doppelte Stunde (Zeit bis):\\n{austria_dates.loc[position_doppelte_stunde_bis, 'Zeit bis [CET/CEST]']}\\nsteht an Indexposition\\n {position_doppelte_stunde_bis}\\n\"\nf\"\\nDie nächste Stunde lautet:\\n{austria_dates.loc[position_doppelte_stunde_bis + 4, 'Zeit bis [CET/CEST]']}\"\nf\"\\n\\nBeide Stunden sind identisch.\")\n\n### Ende der Verschiebung in Spalte Zeit bis\nende_verschiebung_bis = position_doppelte_stunde_bis[-1]\nprint(f\"\\nDie Zeitverschiebung in der Spalte Zeit bis endet bei Indexposition: {ende_verschiebung_bis}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDie doppelte Stunde (Zeit von):\n28900   2023-10-29 02:00:00\n28901   2023-10-29 02:15:00\n28902   2023-10-29 02:30:00\n28903   2023-10-29 02:45:00\nName: Zeit von [CET/CEST], dtype: datetime64[ns]\nsteht an Indexposition\n Index([28900, 28901, 28902, 28903], dtype='int64')\n\nDie nächste Stunde lautet:\n28904   2023-10-29 02:00:00\n28905   2023-10-29 02:15:00\n28906   2023-10-29 02:30:00\n28907   2023-10-29 02:45:00\nName: Zeit von [CET/CEST], dtype: datetime64[ns]\n\nBeide Stunden sind identisch.\n\nDie Zeitverschiebung in der Spalte Zeit von endet bei Indexposition: 28903\n\n\nDie doppelte Stunde (Zeit bis):\n28899   2023-10-29 02:00:00\n28900   2023-10-29 02:15:00\n28901   2023-10-29 02:30:00\n28902   2023-10-29 02:45:00\nName: Zeit bis [CET/CEST], dtype: datetime64[ns]\nsteht an Indexposition\n Index([28899, 28900, 28901, 28902], dtype='int64')\n\nDie nächste Stunde lautet:\n28903   2023-10-29 02:00:00\n28904   2023-10-29 02:15:00\n28905   2023-10-29 02:30:00\n28906   2023-10-29 02:45:00\nName: Zeit bis [CET/CEST], dtype: datetime64[ns]\n\nBeide Stunden sind identisch.\n\nDie Zeitverschiebung in der Spalte Zeit bis endet bei Indexposition: 28902\n```\n:::\n:::\n\n\n*fehlende Stunde* \n\nDie Sommerzeit beginnt am letzen Sonntag im März. Die Stunde liegt nicht in range(0, 24). Diese Bedingung kann in vier Schritten kontrolliert werden:\n\n  - Monat März: `march = pd.Series[pd.Series.dt.month == 3]`\n\n  - Sonntage im März: `sundays = march[march.dt.dayofweek == 6]`\n\n  - letzter Sonntag im März: Die letzten 23*4 Einträge sind der letzte Sonntag des Monats (23 weil eine Stunde fehlt).  \n  `last_sunday = sundays[-23*4:]`\n  \n  - fehlende Stunde: `np.argwhere(np.invert(pd.Series(range(0,24)).isin(last_sunday.dt.hour)))`\n\n::: {.cell execution_count=121}\n``` {.python .cell-code}\n# Indexposition der fehlenden Stunde bestimmen\n## Zeit von\n### Monat März\nmaske_märz_von = austria_dates['Zeit von [CET/CEST]'].dt.month == 3\naustria_dates_march_von = austria_dates.loc[maske_märz_von, 'Zeit von [CET/CEST]']\nprint(f\"Der Monat März (Zeit von):\\n{austria_dates_march_von.head}\\n\");\n\n### letzter Sonntag\nmaske_sonntag_von = (austria_dates_march_von.dt.dayofweek == 6)\nletzter_sonntag_von = (austria_dates_march_von[maske_sonntag_von]) [-23*4 :]\nprint(f\"Der letzte Sonntag im März:\\n{letzter_sonntag_von}\\n\")\n\n### fehlende Stunde\nprint(letzter_sonntag_von.dt.hour)\nfehlende_stunde_von = np.argwhere(np.invert(pd.Series(range(0,24)).isin(letzter_sonntag_von.dt.hour))).item() \nprint(f\"\\nEs fehlt die Stunde:\\n{fehlende_stunde_von}\\n\")\nprint(letzter_sonntag_von[letzter_sonntag_von.dt.hour == (fehlende_stunde_von - 1)], letzter_sonntag_von[letzter_sonntag_von.dt.hour == (fehlende_stunde_von + 1)], sep = \"\\n\")\n\n### Beginn der Verschiebung in Spalte Zeit von\nbeginn_verschiebung_von = letzter_sonntag_von[letzter_sonntag_von.dt.hour == (fehlende_stunde_von + 1)].index[0]\n\nprint(f\"\\nDie Zeitverschiebung in der Spalte Zeit von beginnt bei Indexposition: {beginn_verschiebung_von}\\n\\n\")\n\n## Zeit bis\n### Monat März\nmaske_märz_bis = austria_dates['Zeit bis [CET/CEST]'].dt.month == 3\naustria_dates_march_bis = austria_dates.loc[maske_märz_bis, 'Zeit bis [CET/CEST]']\nprint(f\"Der Monat März (Zeit bis):\\n{austria_dates_march_bis.head}\\n\");\n\n### letzter Sonntag\nmaske_sonntag_bis = (austria_dates_march_bis.dt.dayofweek == 6)\nletzter_sonntag_bis = (austria_dates_march_bis[maske_sonntag_bis]) [-23*4 :]\nprint(f\"Der letzte Sonntag im März:\\n{letzter_sonntag_bis}\\n\")\n\n### fehlende Stunde\nprint(letzter_sonntag_bis.dt.hour)\nfehlende_stunde_bis = np.argwhere(np.invert(pd.Series(range(0,24)).isin(letzter_sonntag_bis.dt.hour))).item() \nprint(f\"\\nEs fehlt die Stunde:\\n{fehlende_stunde_bis}\\n\")\nprint(letzter_sonntag_bis[letzter_sonntag_bis.dt.hour == (fehlende_stunde_bis - 1)], letzter_sonntag_bis[letzter_sonntag_bis.dt.hour == (fehlende_stunde_bis + 1)], sep = \"\\n\")\n\n### Beginn der Verschiebung in Spalte Zeit bis\nbeginn_verschiebung_bis = letzter_sonntag_bis[letzter_sonntag_bis.dt.hour == (fehlende_stunde_bis + 1)].index[0]\n\nprint(f\"\\nDie Zeitverschiebung in der Spalte Zeit bis beginnt bei Indexposition: {beginn_verschiebung_bis}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDer Monat März (Zeit von):\n<bound method NDFrame.head of 5664   2023-03-01 00:00:00\n5665   2023-03-01 00:15:00\n5666   2023-03-01 00:30:00\n5667   2023-03-01 00:45:00\n5668   2023-03-01 01:00:00\n               ...        \n8631   2023-03-31 22:45:00\n8632   2023-03-31 23:00:00\n8633   2023-03-31 23:15:00\n8634   2023-03-31 23:30:00\n8635   2023-03-31 23:45:00\nName: Zeit von [CET/CEST], Length: 2972, dtype: datetime64[ns]>\n\nDer letzte Sonntag im März:\n8064   2023-03-26 00:00:00\n8065   2023-03-26 00:15:00\n8066   2023-03-26 00:30:00\n8067   2023-03-26 00:45:00\n8068   2023-03-26 01:00:00\n               ...        \n8151   2023-03-26 22:45:00\n8152   2023-03-26 23:00:00\n8153   2023-03-26 23:15:00\n8154   2023-03-26 23:30:00\n8155   2023-03-26 23:45:00\nName: Zeit von [CET/CEST], Length: 92, dtype: datetime64[ns]\n\n8064     0\n8065     0\n8066     0\n8067     0\n8068     1\n        ..\n8151    22\n8152    23\n8153    23\n8154    23\n8155    23\nName: Zeit von [CET/CEST], Length: 92, dtype: int32\n\nEs fehlt die Stunde:\n2\n\n8068   2023-03-26 01:00:00\n8069   2023-03-26 01:15:00\n8070   2023-03-26 01:30:00\n8071   2023-03-26 01:45:00\nName: Zeit von [CET/CEST], dtype: datetime64[ns]\n8072   2023-03-26 03:00:00\n8073   2023-03-26 03:15:00\n8074   2023-03-26 03:30:00\n8075   2023-03-26 03:45:00\nName: Zeit von [CET/CEST], dtype: datetime64[ns]\n\nDie Zeitverschiebung in der Spalte Zeit von beginnt bei Indexposition: 8072\n\n\nDer Monat März (Zeit bis):\n<bound method NDFrame.head of 5663   2023-03-01 00:00:00\n5664   2023-03-01 00:15:00\n5665   2023-03-01 00:30:00\n5666   2023-03-01 00:45:00\n5667   2023-03-01 01:00:00\n               ...        \n8630   2023-03-31 22:45:00\n8631   2023-03-31 23:00:00\n8632   2023-03-31 23:15:00\n8633   2023-03-31 23:30:00\n8634   2023-03-31 23:45:00\nName: Zeit bis [CET/CEST], Length: 2972, dtype: datetime64[ns]>\n\nDer letzte Sonntag im März:\n8063   2023-03-26 00:00:00\n8064   2023-03-26 00:15:00\n8065   2023-03-26 00:30:00\n8066   2023-03-26 00:45:00\n8067   2023-03-26 01:00:00\n               ...        \n8150   2023-03-26 22:45:00\n8151   2023-03-26 23:00:00\n8152   2023-03-26 23:15:00\n8153   2023-03-26 23:30:00\n8154   2023-03-26 23:45:00\nName: Zeit bis [CET/CEST], Length: 92, dtype: datetime64[ns]\n\n8063     0\n8064     0\n8065     0\n8066     0\n8067     1\n        ..\n8150    22\n8151    23\n8152    23\n8153    23\n8154    23\nName: Zeit bis [CET/CEST], Length: 92, dtype: int32\n\nEs fehlt die Stunde:\n2\n\n8067   2023-03-26 01:00:00\n8068   2023-03-26 01:15:00\n8069   2023-03-26 01:30:00\n8070   2023-03-26 01:45:00\nName: Zeit bis [CET/CEST], dtype: datetime64[ns]\n8071   2023-03-26 03:00:00\n8072   2023-03-26 03:15:00\n8073   2023-03-26 03:30:00\n8074   2023-03-26 03:45:00\nName: Zeit bis [CET/CEST], dtype: datetime64[ns]\n\nDie Zeitverschiebung in der Spalte Zeit bis beginnt bei Indexposition: 8071\n```\n:::\n:::\n\n\nMit den gespeicherten Indexpositionen können die betreffenden Zeitstempel verschoben werden:\n\n  - Spalte Zeit von: 8072 (Objekt beginn_verschiebung_von) bis 28903 (Objekt ende_verschiebung_von)\n\n  - Spalte Zeit bis: 8071 (Objekt beginn_verschiebung_bis) bis 28902 (Objekt ende_verschiebung_bis)\n\nFür das Slicing wird die Methode `pd.Series.iloc[]` verwendet, die exklusiv indexiert, d. h. die Endpositionen müssen um 1 erhöht werden. Durch Subtraktion von pd.Timedelta(1, unit = 'h') wird die Zeitverschiebung aus dem Datensatz entfernt. \n\n::: {.cell execution_count=122}\n``` {.python .cell-code}\n# Zeitverschiebung korrigieren\n## Zeit von\naustria_dates['Zeit von [CET/CEST]'].iloc[beginn_verschiebung_von : ende_verschiebung_von + 1] = austria_dates['Zeit von [CET/CEST]'].iloc[beginn_verschiebung_von : ende_verschiebung_von + 1].subtract(pd.Timedelta(1, unit = 'h'))\n\nerzeugung_austria['Zeit von [CET/CEST]'] = austria_dates['Zeit von [CET/CEST]']\n\n## Zeit bis\naustria_dates['Zeit bis [CET/CEST]'].iloc[beginn_verschiebung_bis : ende_verschiebung_bis + 1] = austria_dates['Zeit bis [CET/CEST]'].iloc[beginn_verschiebung_bis : ende_verschiebung_bis + 1].subtract(pd.Timedelta(1, unit = 'h'))\n\nerzeugung_austria['Zeit bis [CET/CEST]'] = austria_dates['Zeit bis [CET/CEST]']\n\n# Kontrolle\nprint(\"Kontrolle im Datensatz +/- eine Viertelstunde\\n\")\nprint(\"Die Spalte Zeit von\")\nprint(erzeugung_austria['Zeit von [CET/CEST]'].iloc[beginn_verschiebung_von -1 : ende_verschiebung_von + 2], \"\\n\")\n\nprint(\"Die Spalte Zeit bis\")\nprint(erzeugung_austria['Zeit bis [CET/CEST]'].iloc[beginn_verschiebung_bis -1 : ende_verschiebung_bis + 2], \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKontrolle im Datensatz +/- eine Viertelstunde\n\nDie Spalte Zeit von\n8071    2023-03-26 01:45:00\n8072    2023-03-26 02:00:00\n8073    2023-03-26 02:15:00\n8074    2023-03-26 02:30:00\n8075    2023-03-26 02:45:00\n                ...        \n28900   2023-10-29 01:00:00\n28901   2023-10-29 01:15:00\n28902   2023-10-29 01:30:00\n28903   2023-10-29 01:45:00\n28904   2023-10-29 02:00:00\nName: Zeit von [CET/CEST], Length: 20834, dtype: datetime64[ns] \n\nDie Spalte Zeit bis\n8070    2023-03-26 01:45:00\n8071    2023-03-26 02:00:00\n8072    2023-03-26 02:15:00\n8073    2023-03-26 02:30:00\n8074    2023-03-26 02:45:00\n                ...        \n28899   2023-10-29 01:00:00\n28900   2023-10-29 01:15:00\n28901   2023-10-29 01:30:00\n28902   2023-10-29 01:45:00\n28903   2023-10-29 02:00:00\nName: Zeit bis [CET/CEST], Length: 20834, dtype: datetime64[ns] \n\n```\n:::\n:::\n\n\n:::\n\n#### Schwer: CAPE Ratio - ein Datensatz voller Tücken\nDer Nobelpreisgewinner für Wirtschaftswissenschaften von 2013 Robert Shiller pflegt einen Datensatz mit monatlichen Kursdaten des amerikanischen Aktienindexes S&P500 und weiteren Wirtschaftsindikatoren zur Berechnung des inflationsbereinigten Kurs-Gewinn-Verhältnisses (CAPE Ratio). Der Datensatz ist auf der [Webseite](https://shillerdata.com/) von Robert Shiller verfügbar ([Direktlink zur XLS-Datei](https://img1.wsimg.com/blobby/go/e5e77e0b-59d1-44d9-ab25-4763ac982e53/downloads/ie_data.xls?ver=1712069253887)). **Lesen Sie den Datensatz ein.**\n\n::: {style=\"font-size: 90%;\"}\n| Daten | Dateiname |\n|---|------|\n| monatliche Kursdaten S&P500 |shiller_data.xls |\n\n::: \n\n::: {#tip-shiller-data1 .callout-tip collapse=\"true\"}\n## Hinweise und Musterlösung Shiller data\n\nSchauen Sie sich den Datensatz zunächst mit einem Tabellenkalkulationsprogramm an. Bemerkenswerte Auffälligkeiten sind:\n\n  - Metadaten in den Zeilen 2 und 3, in denen teilweise auch Spaltenbeschriftungen eingetragen sind, sowie am Ende des Datensatzes,\n\n  - mehrzeilige Spaltenbeschriftungen,\n  \n  - Leerspalten P und N,\n\n  - Kennzeichnung fehlender Werte durch 'NA' und leere Zellen '' sowie\n  \n  - abweichende Formatierung des Monats Oktober in der Spalte Date 'YYYY-M'.\n\n:::: {#tip-shiller-data2 .callout-tip collapse=\"true\"}\n## Lösungshilfe\n\nAufgrund der zahlreichen Auffälligkeiten ist es hilfreich, die Daten und die Kopfzeilen getrennt einzulesen.  Den korrekten Zeilenindex können Sie entweder der ersten Betrachtung mit einem Tabellenkalkulationsprogramm entnehmen oder indem Sie einfach die ersten 10 oder 20 Zeilen des Datensatzes in Python einlesen. Dadurch können die Daten einfacher überblickt und mit Methoden der String-Bearbeitung manipuliert werden. In der Praxis ist es einfacher, die Spaltenbeschriftungen manuell mit dem Argument `names = Sequence of column labels to apply` einzutragen bzw. dies mit Hilfe eines Tabellenkalkulationsprogramms zu erledigen.\n\nAußerdem empfiehlt es sich, schrittweise vorzugehen und für jedes Problem eine separate Lösung, mit Ausschnitten des Datensatzes bzw. mit dafür generierten Testdaten, zu entwickeln.\n\n::::: {#tip-shiller-data3 .callout-tip collapse=\"true\"}\n## Vollständige Musterlösung\n\nZum Einlesen wird die Funktion `pd.read_excel()` verwendet. Mit dem Argument `sheet_name` kann das Tabellenblatt Data ausgewählt werden. Über die Methode pd.head(n = 10) kann der Zeilenindex bestimmt werden, an dem der Tabellenkopf endet und die Daten beginnen. Es handelt sich um die achte Zeile, die in Python den Zeilenindex 7 hat.\n\n**Kopf einlesen**  \n\n::: {.cell execution_count=123}\n``` {.python .cell-code}\ndateipfad = '01-daten/shiller_data.xls'\nshiller = pd.read_excel(io = dateipfad, sheet_name = 'Data')\n\n# manuell Ende des Kopfs und Beginn der Daten identifizieren (auskommentiert)\n# print(shiller.head(n = 10), \"\\n\")\n\n# Kopf einlesen\nshiller_head = pd.read_excel(io = dateipfad, sheet_name = 'Data', skiprows = 1, nrows = 7, header = None)\nprint(shiller_head, \"\\n\")\nprint(shiller_head.info()) # die leeren Spalten werden als numerisch erkannt\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                  0      1         2   \\\n0  Stock Market Data Used in \"Irrational Exuberan...    NaN       NaN   \n1                                 Robert J. Shiller     NaN       NaN   \n2                                                NaN    NaN       NaN   \n3                                                NaN    NaN       NaN   \n4                                                NaN    S&P       NaN   \n5                                                NaN  Comp.  Dividend   \n6                                               Date      P         D   \n\n         3           4         5          6      7         8       9   ...  \\\n0       NaN         NaN       NaN        NaN    NaN       NaN     NaN  ...   \n1       NaN         NaN       NaN        NaN    NaN       NaN     NaN  ...   \n2       NaN         NaN       NaN        NaN    NaN       NaN     NaN  ...   \n3       NaN    Consumer       NaN        NaN    NaN       NaN    Real  ...   \n4       NaN       Price       NaN       Long    NaN       NaN   Total  ...   \n5  Earnings       Index    Date     Interest   Real      Real  Return  ...   \n6         E         CPI  Fraction  Rate GS10  Price  Dividend   Price  ...   \n\n           12  13                  14  15      16       17       18  \\\n0  Cyclically NaN         Cyclically  NaN     NaN      NaN      NaN   \n1    Adjusted NaN            Adjusted NaN     NaN      NaN      NaN   \n2       Price NaN  Total Return Price NaN     NaN      NaN      NaN   \n3    Earnings NaN            Earnings NaN     NaN  Monthly     Real   \n4       Ratio NaN               Ratio NaN  Excess    Total    Total   \n5    P/E10 or NaN         TR P/E10 or NaN    CAPE     Bond     Bond   \n6        CAPE NaN             TR CAPE NaN   Yield  Returns  Returns   \n\n                 19                 20                  21  \n0               NaN                NaN                 NaN  \n1               NaN                NaN                 NaN  \n2               NaN                NaN                 NaN  \n3               NaN                NaN                 NaN  \n4           10 Year            10 Year        Real 10 Year  \n5  Annualized Stock  Annualized Bonds   Excess Annualized   \n6       Real Return        Real Return             Returns  \n\n[7 rows x 22 columns] \n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 7 entries, 0 to 6\nData columns (total 22 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   0       3 non-null      object \n 1   1       3 non-null      object \n 2   2       2 non-null      object \n 3   3       2 non-null      object \n 4   4       4 non-null      object \n 5   5       2 non-null      object \n 6   6       3 non-null      object \n 7   7       2 non-null      object \n 8   8       2 non-null      object \n 9   9       4 non-null      object \n 10  10      2 non-null      object \n 11  11      4 non-null      object \n 12  12      7 non-null      object \n 13  13      0 non-null      float64\n 14  14      7 non-null      object \n 15  15      0 non-null      float64\n 16  16      3 non-null      object \n 17  17      4 non-null      object \n 18  18      4 non-null      object \n 19  19      3 non-null      object \n 20  20      3 non-null      object \n 21  21      3 non-null      object \ndtypes: float64(2), object(20)\nmemory usage: 1.3+ KB\nNone\n```\n:::\n:::\n\n\n**Spaltenbeschriftungen isolieren**  \nAnschließend kann der Kopf weiter bearbeitet werden, um die Spaltenbeschriftungen zu isolieren. Dafür gibt es verschiedene Möglichkeiten. Weitere Alternativen zur folgenden Variante finden Sie im nachfolgenden Beispiel. Spaltenweise erfolgt die Verkettung der Zeichenketten mit der Methode `pd.Series.str.cat()`, die nur für `pd.Series` verfügbar ist (weshalb mit einer Schleife die Spalten einzeln durchlaufen werden) und nur mit dem Datentyp 'string' verfügbar ist, was durch die Methode `astype('string')` sichergestellt wird.\n\nAnschließend werden nicht zur Spaltenbeschriftung gehörende Zeichenketten mit der Methode `str.replace()` entfernt. Dabei erweist sich das Argument `regex = True` als nützlich.\n\n::: {.cell execution_count=124}\n``` {.python .cell-code}\n# Spaltenbeschriftung mit Schleife erzeugen\nshiller_column_labels = pd.Series()\n\nfor column in shiller_head:\n  shiller_column_labels = pd.concat([shiller_column_labels, pd.Series(shiller_head[column].astype('string').str.cat())])\n\n# Zeichenketten säubern\n## erste Zelle entfernen Stock Market Data Used in \"Irrational Exuberan...\nshiller_column_labels = shiller_column_labels.astype('str').replace(shiller_head.loc[0, 0], '', regex = True)\n\n## 'Robert J. Shiller' entfernen \nshiller_column_labels = shiller_column_labels.astype('str').replace(shiller_head.loc[1, 0], '', regex = True)\n\n## Leerzeichen entfernen\n## regex = True um Leerzeichen innerhalb von Strings zu entfernen\nshiller_column_labels = shiller_column_labels.astype('str').replace(' ', '', regex = True)\n\n## sehr lange strings ersetzen\nshiller_column_labels = shiller_column_labels.astype('str').replace('CyclicallyAdjustedPriceEarningsRatioP/E10or', '', regex = True)\nshiller_column_labels = shiller_column_labels.astype('str').replace('CyclicallyAdjustedTotalReturnPriceEarningsRatioTRP/E10or', '', regex = True)\n\n## Index zurücksetzen\nshiller_column_labels.reset_index(inplace = True, drop = True)\n\nprint(shiller_column_labels)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0                                  Date\n1                             S&PComp.P\n2                             DividendD\n3                             EarningsE\n4                 ConsumerPriceIndexCPI\n5                          DateFraction\n6                  LongInterestRateGS10\n7                             RealPrice\n8                          RealDividend\n9                  RealTotalReturnPrice\n10                         RealEarnings\n11                 RealTRScaledEarnings\n12                                 CAPE\n13                                     \n14                               TRCAPE\n15                                     \n16                      ExcessCAPEYield\n17              MonthlyTotalBondReturns\n18                 RealTotalBondReturns\n19      10YearAnnualizedStockRealReturn\n20      10YearAnnualizedBondsRealReturn\n21    Real10YearExcessAnnualizedReturns\ndtype: object\n```\n:::\n:::\n\n\n:::::: {.callout-note collapse=\"true\"}\n## Alternative Vorgehensweisen zur String-Manipulation des Dateikopfs\n\n#### Verwendung des NumPy-Datentyps 'str'\n\nDie Angabe von dtype = 'str' führt zur Verwendung des NumPy-Datentyps string (dtype = 'str'), der veränderlich (mutable) ist. Nur damit funktioniert die Verkettung von Strings mit der Methode `PD.df.sum()`.\n\n::: {.cell execution_count=125}\n``` {.python .cell-code}\n# Der NumPy-Datentyp string ist veränderlich\nmy_array = np.array([['1', '2'], ['3', '4']])\nmy_array[0] = ['a', 'b']\nmy_array\n```\n\n::: {.cell-output .cell-output-display execution_count=125}\n```\narray([['a', 'b'],\n       ['3', '4']], dtype='<U1')\n```\n:::\n:::\n\n\nMit den Pandas-Datentypen 'string' und 'object' funktioniert das gezeigte Vorgehen nicht. Denn Pandas nutzt den Python-Datentyp 'string', der unveränderlich ist. Das bedeutet, es gibt keine Methoden, die eine angelegte Zeichenkette verändern können. Stattdessen geben Methoden wie str.replace() neue strings zurück.\n\n``` {.raw}\n## mit NumPy-Datentyp string\nshiller_head = pd.read_excel(io = dateipfad, sheet_name = 'Data', skiprows = 1, nrows = 7, header = None)\n\n# Kopf mit NumPy-Datentyp string mit pd.sum() verketten\n# print(shiller_head.astype('str').sum(skipna = True, axis = 0))\n\n## Bereinigung des Datensatzes\n### nan entfernen\nshiller_head = shiller_head.astype('str').replace('nan', '')\n\n### erste Zelle entfernen Stock Market Data Used in \"Irrational Exuberan...\nshiller_head = shiller_head.astype('str').replace(shiller_head.loc[0, 0], '')\n\n### 'Robert J. Shiller' entfernen \nshiller_head = shiller_head.astype('str').replace(shiller_head.loc[1, 0], '')\n\n### Leerzeichen entfernen\n### regex = True um Leerzeichen innerhalb von Strings zu entfernen\nshiller_head = shiller_head.astype('str').replace(' ', '', regex = True)\n\n### sehr lange strings ersetzen\nshiller_head = shiller_head.astype('str').replace('CyclicallyAdjustedPriceEarningsRatioP/E10or', '', regex = True)\nshiller_head = shiller_head.astype('str').replace('CyclicallyAdjustedTotalReturnPriceEarningsRatioTRP/E10or', '', regex = True)\n\n### spaltenweise Zeilen verketten\nshiller_head = shiller_head.astype('str').sum(skipna = True, axis = 0)\n\nprint(\"\\nzusammengeführte Spaltennamen\\n\", shiller_head)\n```\n\n#### Verwendung von DF.agg() oder DF.apply()\n\nDie Pandas-Methode `DF.agg()` aggregiert einen DataFrame zeilen- oder spaltenweise durch eine spezifizierbare Funktion. Die Pandas-Methode `DF.apply()` wendet eine Funktion zeilen- oder spaltenweise auf einen DataFrame an. Die Methoden machen also das selbe. Details zur Verwendung des [Lambda-Ausdrucks](https://docs.python.org/3/reference/expressions.html#lambda) und der Methode `join` aus der [Pythonbasis](https://www.w3schools.com/python/ref_string_join.asp) finden Sie in den angegebenen Links.\n\n``` {.raw}\nshiller_head = pd.read_excel(io = dateipfad, sheet_name = 'Data', skiprows = 1, nrows = 7, header = None)\n# DF.agg()\nprint(shiller_head.agg(lambda x: ''.join(x.astype(str))))\n\n# DF.apply\nprint(shiller_head.apply(lambda x: ''.join(x.astype(str))))\n```\n\n::::::\n\n**Daten einlesen**\n\nBeim Einlesen der Daten wird der Kopf kontrolliert sowie mit der Methode `.tail()` das Ende der Datenreihe bestimmt, an dem weitere Metadaten vermerkt sind. Diese Metadaten werden anschließend mit dem Argument `skipfooter = 1` übersprungen.  \n\n::: {.cell execution_count=126}\n``` {.python .cell-code}\ndateipfad = '01-daten/shiller_data.xls'\n\n# Daten einlesen\nshiller_data = pd.read_excel(io = dateipfad, sheet_name = 'Data', skiprows = 8, header = None)\nprint(shiller_data.head(n = 2), \"\\n\")\nshiller_data.tail(n = 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        0     1     2    3      4        5     6       7     8       9   ...  \\\n0  1871.01  4.44  0.26  0.4  12.46  1871.04  5.32  111.06  6.50  111.06  ...   \n1  1871.02   4.5  0.26  0.4  12.84  1871.12  5.32  109.22  6.31  109.75  ...   \n\n   12  13  14  15  16   17    18    19    20    21  \n0 NaN NaN NaN NaN NaN  1.0  1.00  0.13  0.09  0.04  \n1 NaN NaN NaN NaN NaN  1.0  0.97  0.13  0.09  0.04  \n\n[2 rows x 22 columns] \n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=126}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1836</th>\n      <td>2024.01</td>\n      <td>4815.61</td>\n      <td>70.48</td>\n      <td>NaN</td>\n      <td>308.42</td>\n      <td>2024.04</td>\n      <td>4.06</td>\n      <td>4867.78</td>\n      <td>71.24</td>\n      <td>3.22e+06</td>\n      <td>...</td>\n      <td>32.05</td>\n      <td>NaN</td>\n      <td>34.62</td>\n      <td>NaN</td>\n      <td>0.02</td>\n      <td>0.99</td>\n      <td>39.84</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1837</th>\n      <td>2024.02</td>\n      <td>5011.96</td>\n      <td>70.65</td>\n      <td>NaN</td>\n      <td>310.33</td>\n      <td>2024.12</td>\n      <td>4.21</td>\n      <td>5035.09</td>\n      <td>70.98</td>\n      <td>3.34e+06</td>\n      <td>...</td>\n      <td>33.11</td>\n      <td>NaN</td>\n      <td>35.79</td>\n      <td>NaN</td>\n      <td>0.02</td>\n      <td>1.00</td>\n      <td>39.25</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1838</th>\n      <td>2024.03</td>\n      <td>5170.57</td>\n      <td>70.82</td>\n      <td>NaN</td>\n      <td>311.28</td>\n      <td>2024.21</td>\n      <td>4.21</td>\n      <td>5178.50</td>\n      <td>70.93</td>\n      <td>3.43e+06</td>\n      <td>...</td>\n      <td>34.02</td>\n      <td>NaN</td>\n      <td>36.79</td>\n      <td>NaN</td>\n      <td>0.02</td>\n      <td>1.00</td>\n      <td>39.27</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1839</th>\n      <td>2024.04</td>\n      <td>5243.77</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>311.76</td>\n      <td>2024.29</td>\n      <td>4.2</td>\n      <td>5243.77</td>\n      <td>NaN</td>\n      <td>3.48e+06</td>\n      <td>...</td>\n      <td>34.41</td>\n      <td>NaN</td>\n      <td>37.18</td>\n      <td>NaN</td>\n      <td>0.01</td>\n      <td>NaN</td>\n      <td>39.37</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1840</th>\n      <td>NaN</td>\n      <td>Apr price is Apr 1st close</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Mar/Apr CPI estimated</td>\n      <td>NaN</td>\n      <td>Apr GS10 is Mar 29th value</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 22 columns</p>\n</div>\n```\n:::\n:::\n\n\n&nbsp;\n\nNachdem die Metadaten entfernt wurden, werden die erkannten Datentypen mit der Methode `.info()` kontrolliert.\n\n::: {.cell execution_count=127}\n``` {.python .cell-code}\nshiller_data = pd.read_excel(io = dateipfad, sheet_name = 'Data', skiprows = 8, header = None, skipfooter = 1)\nprint(shiller_data.head(n = 2), \"\\n\")\nprint(shiller_data.tail(n = 2))\n\n# Datentypen bestimmen\nshiller_data.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        0     1     2    3      4        5     6       7     8       9   ...  \\\n0  1871.01  4.44  0.26  0.4  12.46  1871.04  5.32  111.06  6.50  111.06  ...   \n1  1871.02  4.50  0.26  0.4  12.84  1871.12  5.32  109.22  6.31  109.75  ...   \n\n   12  13  14  15  16   17    18    19    20    21  \n0 NaN NaN NaN NaN NaN  1.0  1.00  0.13  0.09  0.04  \n1 NaN NaN NaN NaN NaN  1.0  0.97  0.13  0.09  0.04  \n\n[2 rows x 22 columns] \n\n           0        1      2   3       4        5     6        7      8   \\\n1838  2024.03  5170.57  70.82 NaN  311.28  2024.21  4.21  5178.50  70.93   \n1839  2024.04  5243.77    NaN NaN  311.76  2024.29  4.20  5243.77    NaN   \n\n            9   ...     12  13     14  15    16   17     18  19  20  21  \n1838  3.43e+06  ...  34.02 NaN  36.79 NaN  0.02  1.0  39.27 NaN NaN NaN  \n1839  3.48e+06  ...  34.41 NaN  37.18 NaN  0.01  NaN  39.37 NaN NaN NaN  \n\n[2 rows x 22 columns]\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1840 entries, 0 to 1839\nData columns (total 22 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   0       1840 non-null   float64\n 1   1       1840 non-null   float64\n 2   2       1839 non-null   float64\n 3   3       1836 non-null   float64\n 4   4       1840 non-null   float64\n 5   5       1840 non-null   float64\n 6   6       1840 non-null   float64\n 7   7       1840 non-null   float64\n 8   8       1839 non-null   float64\n 9   9       1840 non-null   float64\n 10  10      1836 non-null   float64\n 11  11      1836 non-null   float64\n 12  12      1720 non-null   float64\n 13  13      0 non-null      float64\n 14  14      1720 non-null   float64\n 15  15      0 non-null      float64\n 16  16      1720 non-null   float64\n 17  17      1839 non-null   float64\n 18  18      1840 non-null   float64\n 19  19      1720 non-null   float64\n 20  20      1720 non-null   float64\n 21  21      1720 non-null   float64\ndtypes: float64(22)\nmemory usage: 316.4 KB\n```\n:::\n:::\n\n\n**Kopf und Daten zusammenführen**\n\nDie Spalten mit dem Index 13 (Spalte N) und 15 (Spalte P) sind leer, diese werden aus dem Kopf und den Daten entfernt. Alle Spalten werden als Fließkommazahl erkannt. Das bedeutet, die Prozentzeichen in den Spalten mit den Indizes 16 und 19 bis 21 (Q, T:V) wurden durch eine Division durch 100 verarbeitet. Mit Ausnahme der Spalte Date wurde somit alle Datentypen korrekt erkannt.\n\n::: {.cell execution_count=128}\n``` {.python .cell-code}\n# # leere Spalten entfernen\nshiller_column_labels = shiller_column_labels.drop(labels = [13, 15])\nshiller_data = shiller_data.drop(labels = [13, 15], axis = 'columns')\n\n# Spaltennamen eintragen\nshiller_data.columns = shiller_column_labels\nshiller_data.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1840 entries, 0 to 1839\nData columns (total 20 columns):\n #   Column                             Non-Null Count  Dtype  \n---  ------                             --------------  -----  \n 0   Date                               1840 non-null   float64\n 1   S&PComp.P                          1840 non-null   float64\n 2   DividendD                          1839 non-null   float64\n 3   EarningsE                          1836 non-null   float64\n 4   ConsumerPriceIndexCPI              1840 non-null   float64\n 5   DateFraction                       1840 non-null   float64\n 6   LongInterestRateGS10               1840 non-null   float64\n 7   RealPrice                          1840 non-null   float64\n 8   RealDividend                       1839 non-null   float64\n 9   RealTotalReturnPrice               1840 non-null   float64\n 10  RealEarnings                       1836 non-null   float64\n 11  RealTRScaledEarnings               1836 non-null   float64\n 12  CAPE                               1720 non-null   float64\n 13  TRCAPE                             1720 non-null   float64\n 14  ExcessCAPEYield                    1720 non-null   float64\n 15  MonthlyTotalBondReturns            1839 non-null   float64\n 16  RealTotalBondReturns               1840 non-null   float64\n 17  10YearAnnualizedStockRealReturn    1720 non-null   float64\n 18  10YearAnnualizedBondsRealReturn    1720 non-null   float64\n 19  Real10YearExcessAnnualizedReturns  1720 non-null   float64\ndtypes: float64(20)\nmemory usage: 287.6 KB\n```\n:::\n:::\n\n\n**Datumsformat korrigieren**\n\nIm nächsten Schritt wird das Datumsformat korrigiert. Die Spalte Date enthält Zeichenketten im Format 'YYYY,MM'. Eine Ausnahme ist der Monat Oktober, der im Format 'YYYY.M' kodiert ist. In der Ausgabe mit `print()` ist dies nicht zu sehen, da die Darstellung von 2 Dezimalstellen mit dem Befehl `pd.set_option(\"display.precision\", 2)` eingestellt wurde. Die unterschiedliche Länge der Zeichketten kann mit dem Befehl `shiller_data.loc[0:12, 'Date'].astype('str').str.len()` verdeutlicht werden.\n\n::: {.cell execution_count=129}\n``` {.python .cell-code}\nprint(shiller_data.loc[0:12, 'Date'], \"\\n\")\n\ntry:\n  pd.to_datetime(shiller_data.loc[0:12, 'Date'], format = \"%Y.%m\")\nexcept ValueError as error:\n  print(error, \"\\n\")\nelse:\n  pd.to_datetime(shiller_data.loc[0:12, 'Date'], format = \"%Y.%m\")\n\nprint(shiller_data.loc[0:12, 'Date'].astype('str').str.len(), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0     1871.01\n1     1871.02\n2     1871.03\n3     1871.04\n4     1871.05\n5     1871.06\n6     1871.07\n7     1871.08\n8     1871.09\n9     1871.10\n10    1871.11\n11    1871.12\n12    1872.01\nName: Date, dtype: float64 \n\ntime data \"1871\" doesn't match format \"%Y.%m\", at position 0. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this. \n\n0     7\n1     7\n2     7\n3     7\n4     7\n5     7\n6     7\n7     7\n8     7\n9     6\n10    7\n11    7\n12    7\nName: Date, dtype: int64 \n\n```\n:::\n:::\n\n\nDer Datentyp der Spalte 'Date' wird als String deklariert. Mit einer Maske werden die Zeichenketten mit Länge 6 bestimmt. An die Zeichenketten mit der Länge 6 wird eine 0 angehängt, um das Datumsformat anzugleichen. Anschließend wird die Spalte mit der Funktion `pd.to_datetime(format = \"%Y.%m\")` als datetime eingelesen.\n\n::: {.cell execution_count=130}\n``` {.python .cell-code}\n# Datentyp Spalte Date als String setzen \n# führt zu einer Fehlermeldung:\nshiller_data['Date'] = shiller_data['Date'].astype('str');\n\nmaske = shiller_data['Date'].str.len() == 6\nshiller_data.loc[maske, 'Date'] = shiller_data.loc[maske, 'Date'] + '0'\n\n# Ausgabe Länge Zeichenkette\nprint(shiller_data.loc[0:12, 'Date'].str.len())\nprint(shiller_data.loc[0:12, 'Date'])\n\n# Datentyp Spalte Date als datetime setzen\nshiller_data['Date'] = pd.to_datetime(shiller_data['Date'], format = \"%Y.%m\");\nshiller_data.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0     7\n1     7\n2     7\n3     7\n4     7\n5     7\n6     7\n7     7\n8     7\n9     7\n10    7\n11    7\n12    7\nName: Date, dtype: int64\n0     1871.01\n1     1871.02\n2     1871.03\n3     1871.04\n4     1871.05\n5     1871.06\n6     1871.07\n7     1871.08\n8     1871.09\n9     1871.10\n10    1871.11\n11    1871.12\n12    1872.01\nName: Date, dtype: object\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1840 entries, 0 to 1839\nData columns (total 20 columns):\n #   Column                             Non-Null Count  Dtype         \n---  ------                             --------------  -----         \n 0   Date                               1840 non-null   datetime64[ns]\n 1   S&PComp.P                          1840 non-null   float64       \n 2   DividendD                          1839 non-null   float64       \n 3   EarningsE                          1836 non-null   float64       \n 4   ConsumerPriceIndexCPI              1840 non-null   float64       \n 5   DateFraction                       1840 non-null   float64       \n 6   LongInterestRateGS10               1840 non-null   float64       \n 7   RealPrice                          1840 non-null   float64       \n 8   RealDividend                       1839 non-null   float64       \n 9   RealTotalReturnPrice               1840 non-null   float64       \n 10  RealEarnings                       1836 non-null   float64       \n 11  RealTRScaledEarnings               1836 non-null   float64       \n 12  CAPE                               1720 non-null   float64       \n 13  TRCAPE                             1720 non-null   float64       \n 14  ExcessCAPEYield                    1720 non-null   float64       \n 15  MonthlyTotalBondReturns            1839 non-null   float64       \n 16  RealTotalBondReturns               1840 non-null   float64       \n 17  10YearAnnualizedStockRealReturn    1720 non-null   float64       \n 18  10YearAnnualizedBondsRealReturn    1720 non-null   float64       \n 19  Real10YearExcessAnnualizedReturns  1720 non-null   float64       \ndtypes: datetime64[ns](1), float64(19)\nmemory usage: 287.6 KB\n```\n:::\n:::\n\n\n:::::\n::::\n:::\n\n# Zugriff auf mehrere lokale Dateien: Modul glob\n\n::: {.border}\nDas Modul [glob](https://docs.python.org/3/library/glob.html#module-glob) erlaubt es, mit der Funktion `glob.glob(pathname, *, root_dir = None, recursive = False)` aus einem Ordner alle Dateipfade, die dem im Argument `pathname` spezifizierten Muster entsprechen zurückzugeben. Das Argument `pathname` kann als Schlüsselwort oder positional übergeben werden, die übrigen Argumente müssen als Schlüsselwort übergeben werden (dies signalisiert das Zeichen `*`). Die Speicheradresse des Ordners wird mit dem Argument `root_dir` übergeben, dessen Standardwert das aktuelle Arbeitsverzeichnis ist. \nIm Argument `pathname` können Platzhalter, sogenannte Wildcards, verwendet werden, um beliebige Zeichen und Zeichenfolgen zu spezifizieren.\n\n| Wildcard | Funktion |\n| :--: | :------------: |\n| `*` | beliebige Zeichenfolge außer Dateipfadelemente wie `/` oder `.` |\n| `?` | genau ein beliebiges Zeichen |\n|`[]` | alle in den Klammern eingeschlossenen Zeichen inklusive der Wildcards `*` `?`|\n|`[0-9]` | alle Ziffern 0 bis 9 |\n\n&nbsp;\n\nUnter dem Pfad '01-daten/glob' liegen, teils in einem Unterordner, verschiedene Dateien.\n\n::: {.cell execution_count=131}\n``` {.python .cell-code}\nimport glob\nordnerpfad = '01-daten/glob' \n\npfadliste = glob.glob(pathname = '*', root_dir = ordnerpfad, recursive = False)\nprint(pfadliste)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['ToothGrowth.csv', 'Unfallorte2020_LinRef.csv', 'Unfallorte2022_LinRef.csv', 'Unterordner glob', 'hintergrund.png']\n```\n:::\n:::\n\n\nDas Argument `recursive` steuert, ob auch Unterordner durchsucht werden. Um auch Unterordner zu durchsuchen, muss es auf True gesetzt und im pathname `**` spezifiziert werden.\n\n::: {.cell execution_count=132}\n``` {.python .cell-code}\npfadliste = glob.glob(pathname = '**', root_dir = ordnerpfad, recursive = True)\nprint(pfadliste)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['ToothGrowth.csv', 'Unfallorte2020_LinRef.csv', 'Unfallorte2022_LinRef.csv', 'Unterordner glob', 'Unterordner glob/Unfallorte2021_LinRef.csv', 'Unterordner glob/Unfallorte2023_LinRef.csv', 'hintergrund.png']\n```\n:::\n:::\n\n\nIm angegebenen Pfad '01-daten/glob' liegen die Dateien 'hintergrund.png', 'ToothGrowth.csv', 'Unfallorte2020_LinRef.csv', 'Unfallorte2022_LinRef.csv' sowie der Unterordner 'Unterordner glob'. In diesem Unterordner liegen die Dateien 'Unfallorte2021_LinRef.csv' und 'Unfallorte2023_LinRef.csv'.\n\nUm die Suchergebnisse auf die Dateien 'Unfallorte2020_LinRef.csv', 'Unfallorte2021_LinRef.csv', 'Unfallorte2022_LinRef.csv' und 'Unfallorte2023_LinRef.csv' zu beschränken, muss der im Argument `pathname` übergebene Dateipfad angepasst werden.  **Schauen Sie in die [Dokumentation des Moduls glob](https://docs.python.org/3/library/glob.html#module-glob) und übergeben einen geeigneten Dateipfad.**\n\n::: {#tip-glob .callout-tip collapse=\"true\"}\n## Tipp und Musterlösung\n\nTipp: Die gesuchten Dateien beginnen mit dem Buchstaben 'U' und enden mit der Dateiendung '.csv'. Wie Sie Dateien aus einem Ordner und aus einem Unterordner auslesen, sehen Sie in der verlinkten Dokumentation unter Examples. \n\n:::: {#tip-glob .callout-tip collapse=\"true\"}\n## Musterlösung glob\n\n::: {.cell execution_count=133}\n``` {.python .cell-code}\npfadliste = glob.glob(pathname = '**/U*.csv', root_dir = ordnerpfad, recursive = True)\nprint(pfadliste)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['Unfallorte2020_LinRef.csv', 'Unfallorte2022_LinRef.csv', 'Unterordner glob/Unfallorte2021_LinRef.csv', 'Unterordner glob/Unfallorte2023_LinRef.csv']\n```\n:::\n:::\n\n\n::::\n:::\n\nMit den Dateipfaden können die Dateien mit Hilfe einer Schleife in eine Liste eingelesen werden (aus Gründen der Lesbarkeit jeweils 3 Zeilen und 3 Spalten).\n\n::: {.cell execution_count=134}\n``` {.python .cell-code}\nlist_of_files = []\nfor pfad in pfadliste:\n  zwischenspeicher = pd.read_csv(filepath_or_buffer = ordnerpfad + '/' + pfad, \n  delimiter = ';', nrows = 3, usecols = [0, 1, 2])\n  list_of_files.append(zwischenspeicher)\n  print(pfad, \"\\n\", zwischenspeicher, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnfallorte2020_LinRef.csv \n    OBJECTID           UIDENTSTLAE  ULAND\n0         1  12200116471201851100     12\n1         2  12200106642131830700     12\n2         3  12200109522101836720     12 \n\nUnfallorte2022_LinRef.csv \n    OBJECTID          UIDENTSTLAE  ULAND\n0         1  1220204125013262022      1\n1         2  1220529134013152022      1\n2         3  1220508125013982022      1 \n\nUnterordner glob/Unfallorte2021_LinRef.csv \n    OBJECTID          UIDENTSTLAE  ULAND\n0         1  1210308125013512021      1\n1         2  1210608134013112021      1\n2         3  1210610181013902021      1 \n\nUnterordner glob/Unfallorte2023_LinRef.csv \n    OID_          UIDENTSTLAE  ULAND\n0     1  1230519134013042023      1\n1     2  1230519134013022023      1\n2     3  1230519125013522023      1 \n\n```\n:::\n:::\n\n\nAus der Liste können die Datensätze dann eigenen Objekten zugewiesen werden.\n\n::: {.cell execution_count=135}\n``` {.python .cell-code}\nunfalldaten2020 = list_of_files[0] \nunfalldaten2021 = list_of_files[2] \nunfalldaten2022 = list_of_files[1] \nunfalldaten2023 = list_of_files[3] \n\nprint(unfalldaten2020, \"\\n\")\nprint(unfalldaten2020.dtypes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   OBJECTID           UIDENTSTLAE  ULAND\n0         1  12200116471201851100     12\n1         2  12200106642131830700     12\n2         3  12200109522101836720     12 \n\nOBJECTID        int64\nUIDENTSTLAE    uint64\nULAND           int64\ndtype: object\n```\n:::\n:::\n\n\nDie Objekte könnten auch in einem DataFrame zusammengeführt werden.\n\n::: {.cell execution_count=136}\n``` {.python .cell-code}\n# Spalte OID_ / OBJECTID vereinheitlichen\nunfalldaten = pd.concat([unfalldaten2020.iloc[: , 1:3], unfalldaten2022.iloc[: , 1:3], unfalldaten2023.iloc[: , 1:3] ], ignore_index = True)\nunfalldaten.insert(loc = 0, column = 'OBJECTID', value = pd.Series(range(1, unfalldaten.shape[0] + 1)))\nunfalldaten = unfalldaten.astype('uint64')\n\nunfalldaten\n```\n\n::: {.cell-output .cell-output-display execution_count=136}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>OBJECTID</th>\n      <th>UIDENTSTLAE</th>\n      <th>ULAND</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>12200116471201851392</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>12200106642131830784</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>12200109522101835776</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1220204125013262080</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1220529134013152000</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>1220508125013981952</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>1230519134013041920</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>1230519134013021952</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>1230519125013521920</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n[@Arnold-2023-funktionen-module-dateien]\n\n:::\n\n## Übungen Modul glob\n\n#### Leicht: US State Facts and Figures\nIm Ordner \"01-daten/glob leicht\" liegen verschiedene .CSV-Dateien mit Daten zu den US-Bundesstaaten. **Lesen Sie die Dateien in einen neuen Datensatz  ein.**\n\n  - Wie heißen die Dateien?\n\n  - Welche Regionen werden in den Datensätzen unterschieden?\n  \n  - Wie viele Staaten gehören zu jeder Region?\n  \n  - Welche Region ist flächenmäßig die größte?\n\n::: {.border}\n\nUS State Facts and Figures von Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988) The New S Language. Wadsworth & Brooks/Cole. [rdocumentation.org](https://www.rdocumentation.org/packages/datasets/versions/3.6.2/topics/state). Datensätze:\n\n  - state.abb\n\n  - state.area\n\n  - state.name\n\n  - state.region\n\nDie Datensätze können in R durch Eingabe des Datensatznamens in der Konsole aufgerufen werden.\n:::\n\n::: {#tip-musterloesung-glob .callout-tip collapse=\"true\"}\n## Musterlösung US State Facts and Figures\nMit dem Modul glob werden die Pfade der im Ordner liegenden Dateien in einer Liste gespeichert. In einer Schleife wird mit der Funktion `open()` jede Datei als Dateiobjekt geöffnet, aus dem der Dateiname und der Dateiinhalte ausgelesen werden, um diese zu betrachten.\n\n::: {.cell execution_count=137}\n``` {.python .cell-code}\nimport os\nordnerpfad = \"01-daten/glob leicht\" \npfadliste = glob.glob(pathname = '*', root_dir = ordnerpfad, recursive = False)\n\nlist_of_files = []\nnames_of_files = []\nfor pfad in pfadliste:\n\n  # Dateiobjekt öffnen\n  zwischenspeicher = open(ordnerpfad + '/' + pfad, 'r')\n  \n  # Dateinamen extrahieren\n  name = os.path.basename(zwischenspeicher.name)\n  names_of_files.append(name)\n\n  # Datei auslesen\n  datei = zwischenspeicher.read()\n  list_of_files.append(datei)\n\n  # Ausgabe\n  print(name, \"Encoding:\", zwischenspeicher.encoding)\n  print(datei[0:40], \"\\n\")\n\n  # Dateiobjekt schließen\n  zwischenspeicher.close()\n\nprint(f\"Die Dateien heißen:\\n{names_of_files}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nstate region.csv Encoding: UTF-8\n\"x\"\n\"1\" \"South\"\n\"2\" \"West\"\n\"3\" \"West\"\n\"4 \n\nstate name.csv Encoding: UTF-8\n\"x\"\n\"1\" \"Alabama\"\n\"2\" \"Alaska\"\n\"3\" \"Ariz \n\nstate area.csv Encoding: UTF-8\n\"x\"\n\"1\" 51609\n\"2\" 589757\n\"3\" 113909\n\"4\"  \n\nstate abb.csv Encoding: UTF-8\n\"x\"\n\"1\" \"AL\"\n\"2\" \"AK\"\n\"3\" \"AZ\"\n\"4\" \"AR\"\n \n\nDie Dateien heißen:\n['state region.csv', 'state name.csv', 'state area.csv', 'state abb.csv']\n```\n:::\n:::\n\n\nAls nächstes werden die Daten in einem Datensatz 'states' gespeichert. Als Spaltennamen bieten sich die Dateinamen ohne Dateiendung an. \n\n::: {.cell execution_count=138}\n``` {.python .cell-code}\n# Spaltennamen vorbereiten\ni = 0\nfor name in names_of_files:\n  names_of_files[i] = name[0:-4]\n  i+= 1\n\n# DataFrame erstellen\nstates = pd.DataFrame()\ni = 0\nlist_of_files = []\nfor pfad in pfadliste:\n  zwischenspeicher = open(ordnerpfad + '/' + pfad, 'r')\n  states[i] = pd.read_csv(filepath_or_buffer = zwischenspeicher, sep = ' ', usecols = [1]) # skiprows = 1 überspringt den ersten Staat\n  zwischenspeicher.close()\n  i+= 1\n\n# Spaltennamen eintragen\nstates.columns = names_of_files\n\nprint(f\"Es gibt die Regionen:\\n{states['state region'].unique()}\\n\")\n\nstates.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEs gibt die Regionen:\n['South' 'West' 'Northeast' 'North Central']\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=138}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>state region</th>\n      <th>state name</th>\n      <th>state area</th>\n      <th>state abb</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>South</td>\n      <td>Alabama</td>\n      <td>51609</td>\n      <td>AL</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>West</td>\n      <td>Alaska</td>\n      <td>589757</td>\n      <td>AK</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>West</td>\n      <td>Arizona</td>\n      <td>113909</td>\n      <td>AZ</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>South</td>\n      <td>Arkansas</td>\n      <td>53104</td>\n      <td>AR</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>West</td>\n      <td>California</td>\n      <td>158693</td>\n      <td>CA</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nIm nächsten Schritt können die Anzahl der Staaten und die Fläche je Region bestimmt werden.\n\n::: {.cell execution_count=139}\n``` {.python .cell-code}\nprint(f\"Die Anzahl der Staaten je Region beträgt:\\n{states.groupby('state region')['state name'].apply(len)}\\n\")\nprint(f\"Die Fläche der Regionen beträgt:\\n{states.groupby('state region')['state area'].apply(sum)}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDie Anzahl der Staaten je Region beträgt:\nstate region\nNorth Central    12\nNortheast         9\nSouth            16\nWest             13\nName: state name, dtype: int64\n\nDie Fläche der Regionen beträgt:\nstate region\nNorth Central     765530\nNortheast         169353\nSouth             899556\nWest             1783960\nName: state area, dtype: int64\n\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/p_/ks3trxjx0jd839_g4g0vm4nc0000gn/T/ipykernel_80936/3346728540.py:2: FutureWarning: The provided callable <built-in function sum> is currently using np.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string np.sum instead.\n  print(f\"Die Fläche der Regionen beträgt:\\n{states.groupby('state region')['state area'].apply(sum)}\\n\")\n```\n:::\n:::\n\n\n:::\n\n#### Schwer: DSB Unfallatlas\nIm Skript wurden bereits Daten aus dem DSB Unfallatlas eingelesen. Aus Gründen der Lesbarkeit wurden jedoch nur die ersten drei Zeilen und Spalten eingelesen. Für diese Aufgabe sollen nun die Dateien im Dateipfad '01-daten/glob schwer' vollständig eingelesen und zu einem Datensatz zusammengeführt werden. Ein Problem dabei ist, dass die Spaltenbeschriftungen für den Straßenzustand und die ID-Variable nicht einheitlich sind.\n\n::: {.border}\n\nUnfallatlas Deutschland. von Statistische Ämter des Bundes und der Länder steht unter der Lizenz [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/deed.de) und ist verfügbar auf dem [Statistikportal des Bundes und der Länder](https://unfallatlas.statistikportal.de/ ). 2024\n\n:::\n\n:::  {#tip-musterloesung-unfallatlas .callout-tip collapse=\"true\"}\n## Musterlösung Unfallatlas\n\nDas Erhebungsjahr ist eine Variable im Datensatz. Dennoch wird gezeigt, wie dieses aus dem Dateinamen ausgelesen werden kann.\n\nZuerst werden der Inhalt des Ordners und anschließend die Dateipfade ausgelesen.\n\n::: {.cell execution_count=140}\n``` {.python .cell-code}\n# Inhalt Ordner auslesen\nordnerpfad = '01-daten/glob schwer'\npfadliste = glob.glob(pathname = '**', root_dir = ordnerpfad, recursive = True)\nprint(f\"Im Suchpfad liegen {len(pfadliste)} Elemente. Die Bezeichnungen lauten:\\n{pfadliste}\\n\")\n\n# Dateipfade auslesen\npfadliste = glob.glob(pathname = '**/U*.csv', root_dir = ordnerpfad, recursive = True)\nprint(f\"Im Suchpfad liegen {len(pfadliste)} CSV-Dateien. Die Dateinamen lauten:\\n{pfadliste}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIm Suchpfad liegen 6 Elemente. Die Bezeichnungen lauten:\n['Unfallorte2020_LinRef.csv', 'Unfallorte2022_LinRef.csv', 'DSB_Unfallatlas Metadaten.pdf', 'Unterordner glob', 'Unterordner glob/Unfallorte2021_LinRef.csv', 'Unterordner glob/Unfallorte2023_LinRef.csv']\n\nIm Suchpfad liegen 4 CSV-Dateien. Die Dateinamen lauten:\n['Unfallorte2020_LinRef.csv', 'Unfallorte2022_LinRef.csv', 'Unterordner glob/Unfallorte2021_LinRef.csv', 'Unterordner glob/Unfallorte2023_LinRef.csv']\n\n```\n:::\n:::\n\n\nAls nächstes werden die Jahreszahlen aus den Pfadnamen extrahiert sowie die Spaltennamen ausgelesen.\n\nUm die Jahreszahlen aus den Dateinamen zu extrahieren, können die Zeichenketten manuell beschnitten werden (Die Jahreszahl steht an den Indexpositionen 10 bis exklusiv 14). Alternativ kann ein regulärer Ausdruck verwendet werden. Reguläre Ausdrücke werden mit dem Modul re verarbeitet. Die Funktion `re.search(string = x, pattern = '[0-9]+')` sucht im Objekt x nach dem im Argument `pattern` spezifizierten Ausdruck, hier nach einer Zahl von 0 bis 9 `[0-9]` und jeder darauf folgenden Zahl `+`. Die Funktion `re.search()` erzeugt ein Match-Objekt, dessen Inhalt mit der Methode `Match.group()` ausgegeben werden kann, die einen string zurückgibt. (siehe [Dokumentation Modul re](https://docs.python.org/3/library/re.html#match-objects))\n\nUm die Spaltennamen auszulesen, wird die Funktion `pd.read_csv()` mit den Argumenten `nrows = 1, header = 0` angewiesen, die erste Zeile auszulesen. Anschließend können die Spaltennamen visuell abgeglichen werden - dies ist im Reiter DataFrame der Spaltennamen umgesetzt. Für größere Datensätze ist dies aber nicht mehr praktikabel. Eine algorithmische Lösung ist im Reiter Spaltennamen finden umgesetzt. Dazu wird jede Spalte des DataFrames gegen alle anderen Spalten mit der Methode `pd.Series.isin(df)` abgeglichen.\n\n:::: {.panel-tabset}\n\n## Jahreszahlen und Spaltennamen auslesen\n\n::: {.cell execution_count=141}\n``` {.python .cell-code}\n# Die Jahreszahlen auslesen\nimport re\n\njahreszahlen = []\nfor pfad in pfadliste:\n  zwischenspeicher = re.search(string = pfad, pattern = '[0-9]+').group()\n  jahreszahlen.append(int(zwischenspeicher))\n\nprint(f\"Aus den Dateinamen extrahierte Jahreszahlen:\\t{jahreszahlen}\\n\")\n\n# Spaltennamen durch Einlesen der ersten Zeile extrahieren, header = None\ni = 0\nlist_of_columns = []\nfor pfad in pfadliste:\n\n  zwischenspeicher = pd.read_csv(filepath_or_buffer = ordnerpfad + '/' + pfad, \n  delimiter = ';', nrows = 1, header = None).transpose()\n\n  list_of_columns.append(zwischenspeicher)\n  i += 1\n\ndf_of_columns = pd.concat(list_of_columns, axis = 1, ignore_index = True)\ndf_of_columns.columns = jahreszahlen\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAus den Dateinamen extrahierte Jahreszahlen:\t[2020, 2022, 2021, 2023]\n\n```\n:::\n:::\n\n\nDie Spaltennamen werden im nächsten Reiter ausgegeben.\n\n## DataFrame der Spaltennamen\nMit der Ausgabe des DataFrames können die ungleichen Spaltennamen visuell identifiziert werden.\n\n::: {.cell execution_count=142}\n``` {.python .cell-code}\nprint(f\"Ein DataFrame der Spaltennamen:\\n{df_of_columns}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEin DataFrame der Spaltennamen:\n           2020                2022         2021                2023\n0      OBJECTID            OBJECTID     OBJECTID                OID_\n1   UIDENTSTLAE         UIDENTSTLAE  UIDENTSTLAE         UIDENTSTLAE\n2         ULAND               ULAND        ULAND               ULAND\n3       UREGBEZ             UREGBEZ      UREGBEZ             UREGBEZ\n4        UKREIS              UKREIS       UKREIS              UKREIS\n5     UGEMEINDE           UGEMEINDE    UGEMEINDE           UGEMEINDE\n6         UJAHR               UJAHR        UJAHR               UJAHR\n7        UMONAT              UMONAT       UMONAT              UMONAT\n8       USTUNDE             USTUNDE      USTUNDE             USTUNDE\n9    UWOCHENTAG          UWOCHENTAG   UWOCHENTAG          UWOCHENTAG\n10   UKATEGORIE          UKATEGORIE   UKATEGORIE          UKATEGORIE\n11         UART                UART         UART                UART\n12        UTYP1               UTYP1        UTYP1               UTYP1\n13   ULICHTVERH          ULICHTVERH   ULICHTVERH          ULICHTVERH\n14       IstRad  IstStrassenzustand  USTRZUSTAND  IstStrassenzustand\n15       IstPKW              IstRad       IstRad              IstRad\n16      IstFuss              IstPKW       IstPKW              IstPKW\n17      IstKrad             IstFuss      IstFuss             IstFuss\n18      IstGkfz             IstKrad      IstKrad             IstKrad\n19  IstSonstige             IstGkfz      IstGkfz             IstGkfz\n20      LINREFX         IstSonstige  IstSonstige         IstSonstige\n21      LINREFY             LINREFX      LINREFX             LINREFX\n22    XGCSWGS84             LINREFY      LINREFY             LINREFY\n23    YGCSWGS84           XGCSWGS84    XGCSWGS84           XGCSWGS84\n24   STRZUSTAND           YGCSWGS84    YGCSWGS84           YGCSWGS84\n25          NaN                 NaN          NaN                PLST\n```\n:::\n:::\n\n\n## Spaltennamen finden\nDer Abgleich der ungleichen Spaltennamen kann auch algorithmisch gelöst werden.\n\n::: {.cell execution_count=143}\n``` {.python .cell-code}\n# Spaltennamen algorithmisch abgleichen\nungleiche_spaltennamen = pd.Series()\n\n## vergleiche jede Spalte gegen alle anderen Spalten\nfor i in range(0, df_of_columns.shape[1]):\n\n  aktuelle_spalte = df_of_columns.iloc[:, i]\n\n  vergleichs_df = df_of_columns.drop(df_of_columns.columns[i], axis = 1)\n\n  j = 0\n  for vergleichsspalte in vergleichs_df: # vergleichsspalte ist der Spaltenname (als int) 2020, 2021 usw.\n    \n    # print(vergleichsspalte)\n    maske_fehlender_spaltennamen = aktuelle_spalte.isin(vergleichs_df.loc[:, vergleichsspalte])\n    maske_fehlender_spaltennamen = np.invert(maske_fehlender_spaltennamen) # pandas hat keine Methode .invert()\n    \n    # WENN Spaltennamen fehlen DANN\n    if maske_fehlender_spaltennamen.sum() > 0:\n\n      print(f\"In den Unfalldaten {df_of_columns.columns[i]} treten die Spaltennamen {aktuelle_spalte[maske_fehlender_spaltennamen].values} auf, die nicht in den Unfalldaten {vergleichs_df.columns[j]} enthalten sind.\\n\")\n\n      ungleiche_spaltennamen = pd.concat([ungleiche_spaltennamen, pd.Series(aktuelle_spalte[maske_fehlender_spaltennamen].values)])\n\n    j +=1\n\nungleiche_spaltennamen = ungleiche_spaltennamen.dropna().unique()\n\nprint(f\"Folgende Spaltennamen müssen vereinheitlicht werden:\\n{ungleiche_spaltennamen}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIn den Unfalldaten 2020 treten die Spaltennamen ['STRZUSTAND'] auf, die nicht in den Unfalldaten 2022 enthalten sind.\n\nIn den Unfalldaten 2020 treten die Spaltennamen ['STRZUSTAND'] auf, die nicht in den Unfalldaten 2021 enthalten sind.\n\nIn den Unfalldaten 2020 treten die Spaltennamen ['OBJECTID' 'STRZUSTAND' nan] auf, die nicht in den Unfalldaten 2023 enthalten sind.\n\nIn den Unfalldaten 2022 treten die Spaltennamen ['IstStrassenzustand'] auf, die nicht in den Unfalldaten 2020 enthalten sind.\n\nIn den Unfalldaten 2022 treten die Spaltennamen ['IstStrassenzustand'] auf, die nicht in den Unfalldaten 2021 enthalten sind.\n\nIn den Unfalldaten 2022 treten die Spaltennamen ['OBJECTID' nan] auf, die nicht in den Unfalldaten 2023 enthalten sind.\n\nIn den Unfalldaten 2021 treten die Spaltennamen ['USTRZUSTAND'] auf, die nicht in den Unfalldaten 2020 enthalten sind.\n\nIn den Unfalldaten 2021 treten die Spaltennamen ['USTRZUSTAND'] auf, die nicht in den Unfalldaten 2022 enthalten sind.\n\nIn den Unfalldaten 2021 treten die Spaltennamen ['OBJECTID' 'USTRZUSTAND' nan] auf, die nicht in den Unfalldaten 2023 enthalten sind.\n\nIn den Unfalldaten 2023 treten die Spaltennamen ['OID_' 'IstStrassenzustand' 'PLST'] auf, die nicht in den Unfalldaten 2020 enthalten sind.\n\nIn den Unfalldaten 2023 treten die Spaltennamen ['OID_' 'PLST'] auf, die nicht in den Unfalldaten 2022 enthalten sind.\n\nIn den Unfalldaten 2023 treten die Spaltennamen ['OID_' 'IstStrassenzustand' 'PLST'] auf, die nicht in den Unfalldaten 2021 enthalten sind.\n\nFolgende Spaltennamen müssen vereinheitlicht werden:\n['STRZUSTAND' 'OBJECTID' 'IstStrassenzustand' 'USTRZUSTAND' 'OID_' 'PLST']\n```\n:::\n:::\n\n\n::::\n\n&nbsp; \n\nDie Spaltennamen 'STRZUSTAND', 'IstStrassenzustand' und 'USTRZUSTAND' sowie 'OBJECTID', und 'OID_' müssen beim Einlesen der Daten vereinheitlicht werden. Die Spalte 'PLST' wurde nur im Jahr 2023 erhoben und kann entfallen (siehe Unfallatlas Metadaten). Dafür wird der Methode `pd.DataFrame.drop()` das Argument `errors = 'ignore'` übergeben, da in einem Datensatz nicht vorhandene Spalten andernfalls zu einer Fehlermeldung führen.\n\nDie Spalte 'UIDENTSTLAE' enthält sehr große Ganzzahlen, weshalb diese als String eingelesen wird.\n\n::: {.cell execution_count=144}\n``` {.python .cell-code}\n## Dateien einlesen\nunfallatlas = pd.DataFrame()\n\nfor pfad in pfadliste:\n\n  ### Datei einlesen\n  zwischenspeicher = pd.read_csv(filepath_or_buffer = ordnerpfad + '/' + pfad, \n  delimiter = ';', dtype = {'UIDENTSTLAE': 'string'})\n\n  ### Spaltennamen vereinheitlichen\n  zwischenspeicher.rename(columns = {\"IstStrassenzustand\": \"STRZUSTAND\", \"USTRZUSTAND\": \"STRZUSTAND\", \"OID_\": \"OBJECTID\"}, inplace = True)\n  zwischenspeicher.drop(columns = \"PLST\", inplace = True, errors = 'ignore')\n  \n  unfallatlas = pd.concat([unfallatlas, zwischenspeicher])\n\n# Ausgabe\nprint(unfallatlas.info(), \"\\n\")\nunfallatlas.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nIndex: 996742 entries, 0 to 269047\nData columns (total 25 columns):\n #   Column       Non-Null Count   Dtype \n---  ------       --------------   ----- \n 0   OBJECTID     996742 non-null  int64 \n 1   UIDENTSTLAE  996742 non-null  string\n 2   ULAND        996742 non-null  int64 \n 3   UREGBEZ      996742 non-null  int64 \n 4   UKREIS       996742 non-null  int64 \n 5   UGEMEINDE    996742 non-null  int64 \n 6   UJAHR        996742 non-null  int64 \n 7   UMONAT       996742 non-null  int64 \n 8   USTUNDE      996742 non-null  int64 \n 9   UWOCHENTAG   996742 non-null  int64 \n 10  UKATEGORIE   996742 non-null  int64 \n 11  UART         996742 non-null  int64 \n 12  UTYP1        996742 non-null  int64 \n 13  ULICHTVERH   996742 non-null  int64 \n 14  IstRad       996742 non-null  int64 \n 15  IstPKW       996742 non-null  int64 \n 16  IstFuss      996742 non-null  int64 \n 17  IstKrad      996742 non-null  int64 \n 18  IstGkfz      996742 non-null  int64 \n 19  IstSonstige  996742 non-null  int64 \n 20  LINREFX      996742 non-null  object\n 21  LINREFY      996742 non-null  object\n 22  XGCSWGS84    996742 non-null  object\n 23  YGCSWGS84    996742 non-null  object\n 24  STRZUSTAND   996742 non-null  int64 \ndtypes: int64(20), object(4), string(1)\nmemory usage: 197.7+ MB\nNone \n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=144}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>OBJECTID</th>\n      <th>UIDENTSTLAE</th>\n      <th>ULAND</th>\n      <th>UREGBEZ</th>\n      <th>UKREIS</th>\n      <th>UGEMEINDE</th>\n      <th>UJAHR</th>\n      <th>UMONAT</th>\n      <th>USTUNDE</th>\n      <th>UWOCHENTAG</th>\n      <th>...</th>\n      <th>IstPKW</th>\n      <th>IstFuss</th>\n      <th>IstKrad</th>\n      <th>IstGkfz</th>\n      <th>IstSonstige</th>\n      <th>LINREFX</th>\n      <th>LINREFY</th>\n      <th>XGCSWGS84</th>\n      <th>YGCSWGS84</th>\n      <th>STRZUSTAND</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>12200116471201851100</td>\n      <td>12</td>\n      <td>0</td>\n      <td>68</td>\n      <td>468</td>\n      <td>2020</td>\n      <td>1</td>\n      <td>11</td>\n      <td>5</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>735840,436700000430000</td>\n      <td>5887204,801599999900000</td>\n      <td>12,521519179000052</td>\n      <td>53,082132832000070</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>12200106642131830700</td>\n      <td>12</td>\n      <td>0</td>\n      <td>61</td>\n      <td>112</td>\n      <td>2020</td>\n      <td>1</td>\n      <td>17</td>\n      <td>2</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>814106,991899999790000</td>\n      <td>5811960,550300000200000</td>\n      <td>13,614608653000062</td>\n      <td>52,367677682000078</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>12200109522101836720</td>\n      <td>12</td>\n      <td>0</td>\n      <td>67</td>\n      <td>144</td>\n      <td>2020</td>\n      <td>1</td>\n      <td>16</td>\n      <td>5</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>845207,375699999740000</td>\n      <td>5811964,177100000900000</td>\n      <td>14,069661201000031</td>\n      <td>52,349021128000061</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>12200131722601877310</td>\n      <td>12</td>\n      <td>0</td>\n      <td>69</td>\n      <td>76</td>\n      <td>2020</td>\n      <td>1</td>\n      <td>11</td>\n      <td>6</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>756844,833999999800000</td>\n      <td>5787696,062999999200000</td>\n      <td>12,757074839000040</td>\n      <td>52,179829995000034</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>12200124632322878920</td>\n      <td>12</td>\n      <td>0</td>\n      <td>62</td>\n      <td>224</td>\n      <td>2020</td>\n      <td>1</td>\n      <td>16</td>\n      <td>6</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>793616,489599999970000</td>\n      <td>5735748,046199999700000</td>\n      <td>13,249132448000069</td>\n      <td>51,695730215000026</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 25 columns</p>\n</div>\n```\n:::\n:::\n\n\n&nbsp;\n\nAbschließend kann der Datensatz aufgeräumt werden:\n\n  - der Datensatz wird nach dem Jahr sortiert (wichtig: als erstes ausführen),\n  \n  - der Index wird zurückgesetzt (wichtig: als zweites),\n\n\n\n  - die Spalte 'OBJECTID' wird neu erstellt, um die beim Einlesen erzeugten Duplikate zu entfernen und\n\n::: {.cell execution_count=145}\n``` {.python .cell-code}\n# Datensatz nach Jahr sortieren\nunfallatlas.sort_values(by = 'UJAHR', inplace = True)\n\n# Index zurücksetzen\nunfallatlas.reset_index(drop = True, inplace = True)\n\n# Spalte 'OBJECTID' neu generieren\nunfallatlas['OBJECTID'] = pd.Series(range(1, unfallatlas.shape[0] + 1))\n\nprint(unfallatlas.info(), \"\\n\")\nunfallatlas.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 996742 entries, 0 to 996741\nData columns (total 25 columns):\n #   Column       Non-Null Count   Dtype \n---  ------       --------------   ----- \n 0   OBJECTID     996742 non-null  int64 \n 1   UIDENTSTLAE  996742 non-null  string\n 2   ULAND        996742 non-null  int64 \n 3   UREGBEZ      996742 non-null  int64 \n 4   UKREIS       996742 non-null  int64 \n 5   UGEMEINDE    996742 non-null  int64 \n 6   UJAHR        996742 non-null  int64 \n 7   UMONAT       996742 non-null  int64 \n 8   USTUNDE      996742 non-null  int64 \n 9   UWOCHENTAG   996742 non-null  int64 \n 10  UKATEGORIE   996742 non-null  int64 \n 11  UART         996742 non-null  int64 \n 12  UTYP1        996742 non-null  int64 \n 13  ULICHTVERH   996742 non-null  int64 \n 14  IstRad       996742 non-null  int64 \n 15  IstPKW       996742 non-null  int64 \n 16  IstFuss      996742 non-null  int64 \n 17  IstKrad      996742 non-null  int64 \n 18  IstGkfz      996742 non-null  int64 \n 19  IstSonstige  996742 non-null  int64 \n 20  LINREFX      996742 non-null  object\n 21  LINREFY      996742 non-null  object\n 22  XGCSWGS84    996742 non-null  object\n 23  YGCSWGS84    996742 non-null  object\n 24  STRZUSTAND   996742 non-null  int64 \ndtypes: int64(20), object(4), string(1)\nmemory usage: 190.1+ MB\nNone \n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=145}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>OBJECTID</th>\n      <th>UIDENTSTLAE</th>\n      <th>ULAND</th>\n      <th>UREGBEZ</th>\n      <th>UKREIS</th>\n      <th>UGEMEINDE</th>\n      <th>UJAHR</th>\n      <th>UMONAT</th>\n      <th>USTUNDE</th>\n      <th>UWOCHENTAG</th>\n      <th>...</th>\n      <th>IstPKW</th>\n      <th>IstFuss</th>\n      <th>IstKrad</th>\n      <th>IstGkfz</th>\n      <th>IstSonstige</th>\n      <th>LINREFX</th>\n      <th>LINREFY</th>\n      <th>XGCSWGS84</th>\n      <th>YGCSWGS84</th>\n      <th>STRZUSTAND</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>12200116471201851100</td>\n      <td>12</td>\n      <td>0</td>\n      <td>68</td>\n      <td>468</td>\n      <td>2020</td>\n      <td>1</td>\n      <td>11</td>\n      <td>5</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>735840,436700000430000</td>\n      <td>5887204,801599999900000</td>\n      <td>12,521519179000052</td>\n      <td>53,082132832000070</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>09200116005123027960</td>\n      <td>9</td>\n      <td>5</td>\n      <td>77</td>\n      <td>177</td>\n      <td>2020</td>\n      <td>1</td>\n      <td>16</td>\n      <td>5</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>643651,871799999850000</td>\n      <td>5431630,146999999900000</td>\n      <td>10,964871918000028</td>\n      <td>49,020820015000027</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>09200119006203025660</td>\n      <td>9</td>\n      <td>6</td>\n      <td>72</td>\n      <td>114</td>\n      <td>2020</td>\n      <td>1</td>\n      <td>15</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>577166,822800000200000</td>\n      <td>5561461,695399999600000</td>\n      <td>10,081250780000062</td>\n      <td>50,200308086000064</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>09200116004301008520</td>\n      <td>9</td>\n      <td>4</td>\n      <td>73</td>\n      <td>120</td>\n      <td>2020</td>\n      <td>1</td>\n      <td>16</td>\n      <td>5</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>641867,597000000070000</td>\n      <td>5572120,551000000900000</td>\n      <td>10,991370639000024</td>\n      <td>50,284142850000080</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>09200119001515007880</td>\n      <td>9</td>\n      <td>1</td>\n      <td>83</td>\n      <td>128</td>\n      <td>2020</td>\n      <td>1</td>\n      <td>10</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>760977,136500000020000</td>\n      <td>5349657,038400000000000</td>\n      <td>12,515579712000033</td>\n      <td>48,246339794000050</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 25 columns</p>\n</div>\n```\n:::\n:::\n\n\n:::\n\n# Maskierte Arrays {#sec-ma}\nMaskierte Arrays sind eine Erweiterung für NumPy, um Probleme mit fehlenden Werten insbesondere bei Ganzzahlen zu umgehen.\n\n::: {.border layout=\"[5, 90, 5]\"}\n\n&nbsp;\n\n\"**nan**, short for 'not a number' [...] was specifically designed to address the problem of missing values, but the reality is that different platforms behave differently, making life more difficult. On some platforms, the presence of **nan** slows calculations 10-100 times. For integer data, no **nan** value exists. [...] Those wishing to avoid potential headaches will be interested in an alternative solution which has a long history in NumPy’s predecessors – **masked arrays**. [...] Despite their additional memory requirement, masked arrays are faster than nans on many floating point units.\" (SciPy.org Frequently Askend Questions, Zugriff via [waybackmachine](https://web.archive.org/web/20191007223944/https://scipy.org/scipylib/faq.html#does-numpy-support-nan))\n\n&nbsp;\n\n:::\n\n&nbsp;\n\n::: {.cell execution_count=146}\n``` {.python .cell-code}\n# Fehlende Werte für Ganzzahlen sind in Pandas kein Problem\nprint(\"Eine pd.Series vom dtype Int64 mit einem fehlenden Wert:\")\nprint(pd.Series([0, 1, np.nan], dtype = 'Int64'), \"\\n\")\n\n# In NumPy ist das anders\ntry:\n  np.array([0, 1, np.nan], dtype = 'int')\nexcept ValueError as error:\n  print(\"Da np.nan vom Datentyp float ist, haben Integer-Arrays keinen fehlenden Wert.\\n Die Eingabe np.array([0, 1, np.nan], dtype = 'int') führt zu der Fehlermeldung:\\n\", error)\nelse:\n  np.array([0, 1, np.nan], dtype = 'int')\n\nprint(f\"Die Ursache ist der Datentyp von nan: type(np.nan)\\n{type(np.nan)}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEine pd.Series vom dtype Int64 mit einem fehlenden Wert:\n0       0\n1       1\n2    <NA>\ndtype: Int64 \n\nDa np.nan vom Datentyp float ist, haben Integer-Arrays keinen fehlenden Wert.\n Die Eingabe np.array([0, 1, np.nan], dtype = 'int') führt zu der Fehlermeldung:\n cannot convert float NaN to integer\nDie Ursache ist der Datentyp von nan: type(np.nan)\n<class 'float'>\n\n```\n:::\n:::\n\n\nMaskierte Arrays (masked arrays) werden durch das Modul `numpy.ma` bereitgestellt und erlauben es, Werte als ungültig oder fehlend zu markieren, ohne diese zu ersetzen oder zu löschen. Ein maskiertes Array besteht aus drei Elementen:\n\n  1. einem normalen NumPy-Array, das die Daten enthält.\n  \n  2. einer Maske, die entweder den Wert `numpy.ma.nomask` hat, wenn kein Wert ungültig ist oder fehlt, oder aus einem NumPy-Array mit Wahrheitswerten für jedes Element des datentragenden NumPy-Arrays besteht: `True` kennzeichnet einen ungültigen/fehlenden Wert, `False` einen gülten Wert.\n  \n  3. dem Füllwert, mit dem in der Maske mit `True` markierte Werte ersetzt werden. Standardmäßig ist dies die Zeichenkette `'--'`.\n  \nÜber die Attribute `.data` und `.mask` kann auf das zugrundeliegende NumPy-Array und die Maske zugegriffen werden. Ebenfalls können die Funktionen `ma.getmask(maskiertes_array)` und `ma.getdata(maskiertes_array)` genutzt werden. Der Füllwert wird mit der Methode `maskiertes_array.filled()` eingesetzt. Dies gibt eine Kopie des datentragenden Arrays zurück, die Daten selbst bleiben unverändert.\n\n::: {.cell execution_count=147}\n``` {.python .cell-code}\nimport numpy.ma as ma\n\nmaskiertes_array = ma.masked_array(data = np.array([1, 2, 3, 4]), mask = [0, 0, 1, 1])\nprint(f\"maskiertes Array:\\t{maskiertes_array}\")\n\nprint(f\"Daten:\\t\\t\\t\\t{maskiertes_array.data}\")\n\nprint(f\"Maske:\\t\\t\\t\\t{maskiertes_array.mask}\")\n\nprint(f\"Zugriff über Funktionen ma.getdata() und ma.getmask():\\n{ma.getdata(maskiertes_array), ma.getmask(maskiertes_array)}\\n\")\n\nprint(f\"gefülltes Array:\\t\\t\\t\\t{maskiertes_array.filled()}\")\n\nprint(f\"Das maskierte Array bleibt unverändert: {maskiertes_array}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nmaskiertes Array:\t[1 2 -- --]\nDaten:\t\t\t\t[1 2 3 4]\nMaske:\t\t\t\t[False False  True  True]\nZugriff über Funktionen ma.getdata() und ma.getmask():\n(array([1, 2, 3, 4]), array([False, False,  True,  True]))\n\ngefülltes Array:\t\t\t\t[     1      2 999999 999999]\nDas maskierte Array bleibt unverändert: [1 2 -- --]\n```\n:::\n:::\n\n\nDer Standardfüllwert ist abhängig vom Datentyp des NumPy-Arrays.\n\n| Datentyp | Standardfüllwert |\n|:---:|:---:|\n| bool | True |\n| int | 999999 |\n| float | 1.e20 |\n| complex | 1.e20+0j |\n| object | ‘?’ |\n| string | ‘N/A’ |\n\nDer Standarfüllwert kann mit der Funktion `np.ma.default_fill_value()` ausgegeben werden.\n\n::: {.cell execution_count=148}\n``` {.python .cell-code}\nprint(np.ma.default_fill_value(1))\nprint(np.ma.default_fill_value('1'))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n999999\nN/A\n```\n:::\n:::\n\n\n([NumPy Dokumentation](https://numpy.org/doc/2.1/reference/generated/numpy.ma.default_fill_value.html))\n\nDer Füllwert kann über das Attribut `maskiertes_array.fill_value` geändert werden. Die Übergabe von `maskiertes_array.fill_value = None` setzt den Füllwert auf den Standardwert zurück.\n\n::: {.cell execution_count=149}\n``` {.python .cell-code}\n# Anlegen eines maskierten Arrays vom Datentyp Integer mit einem ganzzahligen Füllwert\nmaskiertes_array = ma.masked_array(data = np.arange(0, 4), mask = [0, 0, 1, 1], fill_value = 42)\n\nprint(f\"Gefülltes Array vom Datentyp integer mit ganzzahligem Füllwert:\\n{maskiertes_array.filled()}\\n\")\n\n# Ändern des Füllwerts mit Datentyp float\nmaskiertes_array.fill_value = 1.5\nprint(f\"Wird einem maskierten Array vom Datentyp integer ein Füllwert vom Datentyp float übergeben,\\n wird nur der ganzzahlige Teil eingesetzt:\\n{maskiertes_array.filled()}\\n\")\n\nprint(f\"Wird einem maskierten Array vom Datentyp integer ein Füllwert vom Datentyp string übergeben,\\n folgt eine Fehlermeldung:\\n\")\n\ntry:\n  maskiertes_array.fill_value = '1.5'\n  print(maskiertes_array.filled())\nexcept TypeError as error:\n  print(error)\nelse:\n  maskiertes_array.fill_value = '1.5'\n  print(maskiertes_array.filled())\n\n# Füllwert None\nmaskiertes_array.fill_value = None\nprint(f\"\\nDer Füllwert None bewirkt den Rückgriff auf einen datentypabhängigen Standardwert:\\n{maskiertes_array.filled()}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGefülltes Array vom Datentyp integer mit ganzzahligem Füllwert:\n[ 0  1 42 42]\n\nWird einem maskierten Array vom Datentyp integer ein Füllwert vom Datentyp float übergeben,\n wird nur der ganzzahlige Teil eingesetzt:\n[0 1 1 1]\n\nWird einem maskierten Array vom Datentyp integer ein Füllwert vom Datentyp string übergeben,\n folgt eine Fehlermeldung:\n\nCannot set fill value of string with array of dtype int64\n\nDer Füllwert None bewirkt den Rückgriff auf einen datentypabhängigen Standardwert:\n[     0      1 999999 999999]\n```\n:::\n:::\n\n\n## Maskierte Arrays erzeugen\nMaskierte Arrays können auf zahlreichen Wegen erzeugt werden. Eine vollständige Übersicht erhalten Sie in der [Dokumentation](https://numpy.org/doc/stable/reference/maskedarray.generic.html#constructing-masked-arrays).\n\n::: {.cell execution_count=150}\n``` {.python .cell-code}\na = np.array([1, 10, 100, 1000])\n\nprint(f\"Die Methode ma.asanyarray(array, dtype = ) legt den Datentyp des maskierten Arrays fest.\\n\"\n      f\"ma.asanyarray(a, dtype = 'float64'):\\n\"\n      f\"{ma.asanyarray(a, dtype = 'float64')}\\n\")\n\nprint(f\"Die Methode ma.masked_equal(x = array, value) maskiert alle Werte value.\\n\"\n      f\"ma.masked_equal(a, 100):\\n\"\n      f\"{ma.masked_equal(a, 100)}\\n\")\n\nprint(f\"Die Methode ma.masked_greater(x = array, value) maskiert alle Werte größer als value.\\n\"\n      f\"ma.masked_greater(a, 100):\\n\"\n      f\"{ma.masked_greater(a, 100)}\\n\")\n\nprint(f\"Die Methode ma.masked_inside(x = array, v1, v2) maskiert alle Werte im Intervall v1 bis v2.\\n\"\n      f\"ma.masked_inside(a, 5, 100):\\n\"\n      f\"{ma.masked_inside(a, 5, 100)}\\n\")\n\nprint(f\"Die Methode ma.masked_where(condition, a = array) maskiert alle Werte, für die die Bedingung condition gilt.\\n\"\n      f\"ma.masked_where(a % 2 != 0, a):\\n\"\n      f\"{ma.masked_where(a % 2 != 0, a)}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDie Methode ma.asanyarray(array, dtype = ) legt den Datentyp des maskierten Arrays fest.\nma.asanyarray(a, dtype = 'float64'):\n[   1.   10.  100. 1000.]\n\nDie Methode ma.masked_equal(x = array, value) maskiert alle Werte value.\nma.masked_equal(a, 100):\n[1 10 -- 1000]\n\nDie Methode ma.masked_greater(x = array, value) maskiert alle Werte größer als value.\nma.masked_greater(a, 100):\n[1 10 100 --]\n\nDie Methode ma.masked_inside(x = array, v1, v2) maskiert alle Werte im Intervall v1 bis v2.\nma.masked_inside(a, 5, 100):\n[1 -- -- 1000]\n\nDie Methode ma.masked_where(condition, a = array) maskiert alle Werte, für die die Bedingung condition gilt.\nma.masked_where(a % 2 != 0, a):\n[-- 10 100 1000]\n\n```\n:::\n:::\n\n\nDie NumPy-Funktion `genfromtxt(usemask = True)` erzeugt mit dem entsprechenden Argument ein maskiertes Array.\n\n::: {.cell execution_count=151}\n``` {.python .cell-code}\ndateipfad = '01-daten/TC01_double_hyphen.csv'\ndaten_double_hypen = np.genfromtxt(dateipfad, missing_values = '--', usemask = True)\ndaten_double_hypen\n```\n\n::: {.cell-output .cell-output-display execution_count=151}\n```\nmasked_array(data=[20.1, --, 20.1, ..., 24.3, 24.2, 24.2],\n             mask=[False,  True, False, ..., False, False, False],\n       fill_value=1e+20)\n```\n:::\n:::\n\n\nEine weitere Möglichkeit, Arraybereiche zu maskieren, ist es, einzelnen Werten oder einem Wertebereich eines maskierten Arrays den Wert `ma.masked` zuzuweisen.\n\n::: {.cell execution_count=152}\n``` {.python .cell-code}\nmaskiertes_array = ma.masked_array([1, 2, 3, 4])\n\nmaskiertes_array[1] = ma.masked\nprint(maskiertes_array)\n\nmaskiertes_array[1:3] = ma.masked\nprint(maskiertes_array)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1 -- 3 4]\n[1 -- -- 4]\n```\n:::\n:::\n\n\n## Übung maskierte Arrays erzeugen\n\nGegegben sei das NumPy-Array `np.linspace(1, 1000, 18, dtype = 'int')`. **Erzeugen Sie mit dem NumPy-Array ein maskiertes Array.**\n\n  a. Maskieren Sie jeden Wert im Intervall 250 bis 750. Wie viele Werte sind maskiert?\n  \n  b. Maskieren Sie jeden zweiten Wert. Wie lautet die Summe des maskierten Arrays?\n\n  c. Maskieren Sie jeden geraden Wert.\n\n  d. Maskieren Sie jeden Wert, der mindestens 3 Stellen hat.\n\n::: {#tip-ma .callout-tip collapse=\"true\"}\n## Musterlösung maskierte Arrays erzeugen\n\n::: {.cell execution_count=153}\n``` {.python .cell-code}\n# Array erzeugen\na = np.linspace(1, 1000, 18, dtype = 'int')\n\n# Maske im Intervall 250-750\nmy_ma = ma.masked_inside(x = a, v1 = 250, v2 = 750)\nprint(\"Maske im Intervall 250-750:\")\nprint(my_ma, \"Anzahl maskierter Werte:\", my_ma.mask.sum(), \"\\n\")\n\n# jeder 2. Wert maskiert\nmy_ma = ma.masked_array(a)\nmy_ma[::2] = ma.masked\nprint(\"Jeder 2. Wert maskiert:\")\nprint(my_ma, \"Summe nicht maskierter Werte:\", my_ma.sum(), \"\\n\")\n\n# jeder gerade Wert maskiert\nmy_ma = ma.masked_where(a % 2 == 0, a)\nprint(\"Jeder gerade Wert maskiert:\")\nprint(my_ma, \"\\n\")\n\n# jeder mindestens dreistellige Wert maskiert\nmy_ma = ma.masked_where(a / 100 >= 1, a)\nprint(\"Jeder mindestens dreistellige Wert maskiert:\")\nprint(my_ma)\n\n## eine Alternative\nmy_mask = [len(str(i)) >= 3 for i in a]\nprint(f\"ma.masked_array(data = a, mask = my_mask\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMaske im Intervall 250-750:\n[1 59 118 177 236 -- -- -- -- -- -- -- -- 764 823 882 941 1000] Anzahl maskierter Werte: 8 \n\nJeder 2. Wert maskiert:\n[-- 59 -- 177 -- 294 -- 412 -- 529 -- 647 -- 764 -- 882 -- 1000] Summe nicht maskierter Werte: 4764 \n\nJeder gerade Wert maskiert:\n[1 59 -- 177 -- -- 353 -- 471 529 -- 647 -- -- 823 -- 941 --] \n\nJeder mindestens dreistellige Wert maskiert:\n[1 59 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --]\nma.masked_array(data = a, mask = my_mask\n```\n:::\n:::\n\n\n::: \n\n## unmasking, soft und hard masks\nEin maskiertes Array kann auf verschiedene Weise demaskiert werden.\n\n::: {.cell execution_count=154}\n``` {.python .cell-code}\nunmask_me = my_ma.copy()\nunmask_me.mask = ma.nomask\nprint(f\"Die Maske kann auf ma.nomask oder False gesetzt werden.\\n\"\n      f\"unmask_me.mask = ma.nomask: {unmask_me}\\n\")\n\nunmask_me = my_ma.copy()\nunmask_me.mask = False\nprint(f\"unmask_me.mask = False: {unmask_me}\\n\")\n\nunmask_me = my_ma.copy()\nunmask_me = np.linspace(1, 1000, 18, dtype = 'int')\nprint(f\"Die Maskierung wird auch durch eine Wertzuweisung aufgehoben.\\nDas Objekt wird neu angelegt.\")\nprint(f\"unmask_me = np.linspace(1, 1000, 18, dtype = 'int'):\\n\"\n      f\"{unmask_me}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDie Maske kann auf ma.nomask oder False gesetzt werden.\nunmask_me.mask = ma.nomask: [1 59 118 177 236 294 353 412 471 529 588 647 706 764 823 882 941 1000]\n\nunmask_me.mask = False: [1 59 118 177 236 294 353 412 471 529 588 647 706 764 823 882 941 1000]\n\nDie Maskierung wird auch durch eine Wertzuweisung aufgehoben.\nDas Objekt wird neu angelegt.\nunmask_me = np.linspace(1, 1000, 18, dtype = 'int'):\n[   1   59  118  177  236  294  353  412  471  529  588  647  706  764\n  823  882  941 1000]\n\n```\n:::\n:::\n\n\nDie Demaskierung ist auch für Indexbereiche möglich.\n\n::: {.cell execution_count=155}\n``` {.python .cell-code}\nunmask_me = my_ma.copy()\nunmask_me[6:10].mask = ma.nomask\nprint(f\"Die Maske kann auf ma.nomask oder False gesetzt werden.\\n\"\n      f\"unmask_me[6:10].mask = ma.nomask: {unmask_me}\\n\")\n\nunmask_me = my_ma.copy()\nunmask_me[6:10].mask = False\nprint(f\"unmask_me[6:10].mask = False: {unmask_me}\\n\")\n\nunmask_me = my_ma.copy()\nunmask_me[6:10] = np.linspace(1, 1000, 4, dtype = 'int')\nprint(f\"Die Maskierung wird auch durch eine Wertzuweisung aufgehoben.\")\nprint(f\"unmask_me[6:10] = np.linspace(1, 1000, 4, dtype = 'int'):\\n\"\n      f\"{unmask_me}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDie Maske kann auf ma.nomask oder False gesetzt werden.\nunmask_me[6:10].mask = ma.nomask: [1 59 -- -- -- -- 353 412 471 529 -- -- -- -- -- -- -- --]\n\nunmask_me[6:10].mask = False: [1 59 -- -- -- -- 353 412 471 529 -- -- -- -- -- -- -- --]\n\nDie Maskierung wird auch durch eine Wertzuweisung aufgehoben.\nunmask_me[6:10] = np.linspace(1, 1000, 4, dtype = 'int'):\n[1 59 -- -- -- -- 1 334 667 1000 -- -- -- -- -- -- -- --]\n\n```\n:::\n:::\n\n\nInsbesondere die Demaskierung durch eine Wertzuweisung in einem Indexbereich kann ein unerwünschtes Verhalten sein.\n\n#### soft und hard masks\nDie Maske eines maskierten Arrays ist eine veränderliche, sogenannte soft mask. Um die Maske vor Änderungen zu schützen, kann die Maske mit dem Argument ` ma.array(data = [data], mask = [mask], hard_mask = True)` oder mit der Methode `masked_array.harden_mask()` in eine hard mask verwandelt werden.\n\n::: {.cell execution_count=156}\n``` {.python .cell-code}\nunmask_me = my_ma.copy()\nunmask_me.harden_mask()\nunmask_me.mask = ma.nomask\nprint(f\"Die hard mask wird auf ma.nomask gesetzt.\\n\"\n      f\"unmask_me.mask = ma.nomask: {unmask_me}\\n\")\n\nunmask_me.mask = False\nprint(f\"unmask_me.mask = False: {unmask_me}\\n\")\n\nunmask_me = np.linspace(1, 1000, 18, dtype = 'int')\nprint(f\"Die hard mask wird dennoch durch eine Wertzuweisung aufgehoben.\\nDas Objekt wird neu angelegt.\")\nprint(f\"unmask_me = np.linspace(1, 1000, 18, dtype = 'int'):\\n\"\n      f\"{unmask_me}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDie hard mask wird auf ma.nomask gesetzt.\nunmask_me.mask = ma.nomask: [1 59 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --]\n\nunmask_me.mask = False: [1 59 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --]\n\nDie hard mask wird dennoch durch eine Wertzuweisung aufgehoben.\nDas Objekt wird neu angelegt.\nunmask_me = np.linspace(1, 1000, 18, dtype = 'int'):\n[   1   59  118  177  236  294  353  412  471  529  588  647  706  764\n  823  882  941 1000]\n\n```\n:::\n:::\n\n\nEine hard mask schützt einen Indexbereich vor der Demaskierung, auch durch eine Wertzuweisung.\n\n::: {.cell execution_count=157}\n``` {.python .cell-code}\nunmask_me = my_ma.copy()\nunmask_me.harden_mask()\nunmask_me[6:10].mask = ma.nomask\nprint(f\"Die hard mask wird auf ma.nomask gesetzt.\\n\"\n      f\"unmask_me.mask[6:10] = ma.nomask: {unmask_me}\\n\")\n\nunmask_me[6:10].mask = False\nprint(f\"unmask_me[6:10].mask = False: {unmask_me}\\n\")\n\nunmask_me[6:10] = np.linspace(1, 1000, 4, dtype = 'int')\nprint(f\"Die hard mask wird nicht (!) durch eine Wertzuweisung im Indexbereich aufgehoben.\")\nprint(f\"unmask_me[6:10] = np.linspace(1, 1000, 4, dtype = 'int'):\\n\"\n      f\"{unmask_me}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDie hard mask wird auf ma.nomask gesetzt.\nunmask_me.mask[6:10] = ma.nomask: [1 59 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --]\n\nunmask_me[6:10].mask = False: [1 59 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --]\n\nDie hard mask wird nicht (!) durch eine Wertzuweisung im Indexbereich aufgehoben.\nunmask_me[6:10] = np.linspace(1, 1000, 4, dtype = 'int'):\n[1 59 -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --]\n\n```\n:::\n:::\n\n\nEine hard mask kann jedoch auf weitere Indexbereiche erweitert werden.\n\n::: {.cell execution_count=158}\n``` {.python .cell-code}\nunmask_me[0:2] = ma.masked\nprint(f\"Die Maskierungen zusätzlicher Elemente ist mit einer hard mask möglich.\\n\"\n      f\"{unmask_me}\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDie Maskierungen zusätzlicher Elemente ist mit einer hard mask möglich.\n[-- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --]\n```\n:::\n:::\n\n\nEine hard mask kann mit der Methode `masked_array.soften_mask()` aufgehoben werden.\n\n::: {.cell execution_count=159}\n``` {.python .cell-code}\nunmask_me.soften_mask()\nunmask_me.mask = ma.nomask\nprint(f\"{unmask_me}\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1 59 118 177 236 294 353 412 471 529 588 647 706 764 823 882 941 1000]\n```\n:::\n:::\n\n\n([NumPy Dokumentation](https://numpy.org/doc/stable/reference/maskedarray.generic.html#unmasking-an-entry))\n\n## Operationen mit maskierten Arrays\nArithmetische und vergleichende Operationen mit maskierten Arrays werden nicht auf die maskierten Elemente angewendetet.\n\n<https://numpy.org/doc/stable/reference/maskedarray.generic.html#operations-on-masked-arrays>\n\n<https://numpy.org/doc/2.1/reference/routines.ma.html>\n\n::: {.cell execution_count=160}\n``` {.python .cell-code}\n# Array erzeugen\na = np.linspace(1, 1000, 18, dtype = 'int')\nmaskiertes_array = ma.masked_array(a)\nmaskiertes_array[::2] = ma.masked\n\n# Ausgewählte Operationen\nprint(f\"Summe maskiertes_array: {maskiertes_array.sum()}\\tSumme maskiertes_array.data: {maskiertes_array.data.sum()}\")\nprint(f\"maskiertes_array > 222:\\n{maskiertes_array > 222}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSumme maskiertes_array: 4764\tSumme maskiertes_array.data: 9001\nmaskiertes_array > 222:\n[-- False -- False -- True -- True -- True -- True -- True -- True -- True]\n```\n:::\n:::\n\n\nArithmetische und vergleichende Operationen mit maskierten Elementen führen immer zu maskierten Ergebnissen.\n\n::: {.cell execution_count=161}\n``` {.python .cell-code}\nprint(\"NumPy ist bei Operationen mit np.nan nicht konsistent.\")\nprint(f\"np.nan ** 0:\\t{np.nan ** 0}\")\nprint(f\"1 ** np.nan:\\t{1 ** np.nan}\")\nprint(f\"np.nan == np.nan:\\t{np.nan == np.nan}\\n\")\n\nprint(\"Maskierte Arrays verhalten sich dagegen konsistent.\")\nprint(f\"maskiertes_array[0] ** 0:\\t\\t\\t\\t\\t{maskiertes_array[0] ** 0}\")\nprint(f\"1 ** maskiertes_array[0]:\\t\\t\\t\\t\\t{1 ** maskiertes_array[0]}\")\nprint(f\"maskiertes_array[0] == maskiertes_array[2]:\\t{maskiertes_array[0] == maskiertes_array[2]}\")\nprint(f\"maskiertes_array[0] == np.nan:\\t\\t\\t\\t{maskiertes_array[0] == np.nan}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumPy ist bei Operationen mit np.nan nicht konsistent.\nnp.nan ** 0:\t1.0\n1 ** np.nan:\t1.0\nnp.nan == np.nan:\tFalse\n\nMaskierte Arrays verhalten sich dagegen konsistent.\nmaskiertes_array[0] ** 0:\t\t\t\t\t--\n1 ** maskiertes_array[0]:\t\t\t\t\t--\nmaskiertes_array[0] == maskiertes_array[2]:\t--\nmaskiertes_array[0] == np.nan:\t\t\t\t--\n```\n:::\n:::\n\n\nDas Modul `numpy.ma` implementiert die meisten NumPy-Funktionen (siehe [NumPy Dokumentation](https://numpy.org/doc/2.1/reference/routines.ma.html)). Funktionen, die nur einen bestimmten Wertebereich als Eingabe akzeptieren, geben den Wert `masked` zurück, wenn Werte außerhalb des gültigen Bereichs übergeben werden. Ein Beispiel ist die Funktion `ma.log()`.\n\n::: {.cell execution_count=162}\n``` {.python .cell-code}\nma.log([-1, 0, 1, 2])\n```\n\n::: {.cell-output .cell-output-display execution_count=162}\n```\nmasked_array(data=[--, --, 0.0, 0.6931471805599453],\n             mask=[ True,  True, False, False],\n       fill_value=1e+20)\n```\n:::\n:::\n\n\nAuch Operation mit mehreren Arrays sind möglich. Im Ergebnis ist jedes in der Eingabe maskierte Element ebenfalls maskiert.\n\n::: {.cell execution_count=163}\n``` {.python .cell-code}\nprint(a + maskiertes_array)\nprint(np.logical_or(a, maskiertes_array))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[-- 118 -- 354 -- 588 -- 824 -- 1058 -- 1294 -- 1528 -- 1764 -- 2000]\n[-- True -- True -- True -- True -- True -- True -- True -- True -- True]\n```\n:::\n:::\n\n\nZusammenfassende Funktionen wie `len()` oder `ma.unique(masked_array)` werden auch auf die maskierten Elemente angewendet.\n\n::: {.cell execution_count=164}\n``` {.python .cell-code}\n# Ausgewählte Operationen\nprint(len(maskiertes_array))\nprint(ma.unique(maskiertes_array))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n18\n[59 177 294 412 529 647 764 882 1000 --]\n```\n:::\n:::\n\n\n::: {#wrn-masked-arrays appearance=\"simple\" .callout-warning}\n## Warnhinweis maskierte Arrays\n\nDie Dokumentation warnt, dass maskierte Werte nicht zuverlässig von Operationen ausgenommen sind. Welche Operationen dies betrifft, ist nicht angegeben.\n\n:::: {.border layout=\"[[5, 90, 5], [5, 90, 5], [1]]\"}\n\n&nbsp;\n\n\"Arithmetic and comparison operations are supported by masked arrays. As much as possible, invalid entries of a masked array are not processed, meaning that the corresponding `data` entries *should* be the same before and after the operation.\n\n&nbsp;\n\n&nbsp;\n\n:::: {.callout-warning}\nWe need to stress that this behavior may not be systematic, that masked data may be affected by the operation in some cases and therefore users should not rely on this data remaining unchanged.\"\n::::\n\n&nbsp;\n\n<https://numpy.org/doc/stable/reference/maskedarray.generic.html#operations-on-masked-arrays>\n\n::::\n:::\n\n## Übungen Operationen mit maskierten Arrays\n\n1. Erstellen Sie ein zweidimensionales Array (10x10) der Zahlen 1 bis 100. Maskieren Sie alle ungeraden Werte, die ganzzahlig durch 3 teilbar sind.\n\n2. Berechnen Sie anschließend die Summe der maskierten und die Summe der unmaskierten Werte.\n\n3. Setzen sie die Maske zurück, sodass keine Werte mehr maskiert sind.\n\n4. Primzahlensieb: Maskieren Sie alle Elemente, die den Wert 1 haben oder Vielfache von Primzahlen sind, bis das gefüllte Array nur noch aus Primzahlen besteht (siehe [Sieb des Eratosthenes](https://de.wikipedia.org/wiki/Sieb_des_Eratosthenes)).\n\n5. Wie viele Primzahlen sind in dem Array? Welches ist die größte, welches die kleinste Primzahl? Sortieren Sie die Primzahlen absteigend.\n\n::: {#tip-masked-arrays .callout-tip collapse=\"true\"}\n## Musterlösung\n\n::: {.cell execution_count=165}\n``` {.python .cell-code}\n# 1. NumPy-Array erstellen und maskieren\nmaskiertes_array = np.arange(1, 100 + 1).reshape(10, 10)\nmaskiertes_array = ma.masked_where(condition = (maskiertes_array % 2 != 0) & (maskiertes_array % 3 == 0), a = maskiertes_array)\nprint(maskiertes_array, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1 2 -- 4 5 6 7 8 -- 10]\n [11 12 13 14 -- 16 17 18 19 20]\n [-- 22 23 24 25 26 -- 28 29 30]\n [31 32 -- 34 35 36 37 38 -- 40]\n [41 42 43 44 -- 46 47 48 49 50]\n [-- 52 53 54 55 56 -- 58 59 60]\n [61 62 -- 64 65 66 67 68 -- 70]\n [71 72 73 74 -- 76 77 78 79 80]\n [-- 82 83 84 85 86 -- 88 89 90]\n [91 92 -- 94 95 96 97 98 -- 100]] \n\n```\n:::\n:::\n\n\n::: {.cell execution_count=166}\n``` {.python .cell-code}\n# 2. Summen bilden\nprint(f\"Summe der maskierten Werte: {maskiertes_array.data[maskiertes_array.mask].sum()}\\tSumme der unmaskierten Werte: {maskiertes_array.sum()}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSumme der maskierten Werte: 867\tSumme der unmaskierten Werte: 4183\n\n```\n:::\n:::\n\n\n::: {.cell execution_count=167}\n``` {.python .cell-code}\n# 3. Maske zurücksetzen\nmaskiertes_array.mask = False\nprint(maskiertes_array, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1 2 3 4 5 6 7 8 9 10]\n [11 12 13 14 15 16 17 18 19 20]\n [21 22 23 24 25 26 27 28 29 30]\n [31 32 33 34 35 36 37 38 39 40]\n [41 42 43 44 45 46 47 48 49 50]\n [51 52 53 54 55 56 57 58 59 60]\n [61 62 63 64 65 66 67 68 69 70]\n [71 72 73 74 75 76 77 78 79 80]\n [81 82 83 84 85 86 87 88 89 90]\n [91 92 93 94 95 96 97 98 99 100]] \n\n```\n:::\n:::\n\n\n::: {.cell execution_count=168}\n``` {.python .cell-code}\n# 4. Primzahlsieb\n## 1\ni = 1\nbedingung1 = maskiertes_array == i\nsieb = ma.masked_where(bedingung1, maskiertes_array)\nprint(f\"i = {i}\\n{sieb}\\n\")\n\n## 2\ni = 2\nbedingung2 = (maskiertes_array > i) & (maskiertes_array % i == 0)\nsieb = ma.masked_where(bedingung1 | bedingung2, maskiertes_array)\nprint(f\"zusätzlich Vielfache von {i}\\n{sieb}\\n\")\n\n## 3\ni = 3\nbedingung3 = (maskiertes_array > i) & (maskiertes_array % i == 0)\nsieb = ma.masked_where(bedingung1 | bedingung2 | bedingung3, maskiertes_array)\nprint(f\"zusätzlich Vielfache von {i}\\n{sieb}\\n\")\n\n## 5\ni = 5\nbedingung5 = (maskiertes_array > i) & (maskiertes_array % i == 0)\nsieb = ma.masked_where(bedingung1 | bedingung2 | bedingung3 | bedingung5, maskiertes_array)\nprint(f\"zusätzlich Vielfache von {i}\\n{sieb}\\n\")\n\n## 7\ni = 7\nbedingung7 = (maskiertes_array > i) & (maskiertes_array % i == 0)\nsieb = ma.masked_where(bedingung1 | bedingung2 | bedingung3 | bedingung5 | bedingung7, maskiertes_array)\nprint(f\"zusätzlich Vielfache von {i}\\n{sieb}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ni = 1\n[[-- 2 3 4 5 6 7 8 9 10]\n [11 12 13 14 15 16 17 18 19 20]\n [21 22 23 24 25 26 27 28 29 30]\n [31 32 33 34 35 36 37 38 39 40]\n [41 42 43 44 45 46 47 48 49 50]\n [51 52 53 54 55 56 57 58 59 60]\n [61 62 63 64 65 66 67 68 69 70]\n [71 72 73 74 75 76 77 78 79 80]\n [81 82 83 84 85 86 87 88 89 90]\n [91 92 93 94 95 96 97 98 99 100]]\n\nzusätzlich Vielfache von 2\n[[-- 2 3 -- 5 -- 7 -- 9 --]\n [11 -- 13 -- 15 -- 17 -- 19 --]\n [21 -- 23 -- 25 -- 27 -- 29 --]\n [31 -- 33 -- 35 -- 37 -- 39 --]\n [41 -- 43 -- 45 -- 47 -- 49 --]\n [51 -- 53 -- 55 -- 57 -- 59 --]\n [61 -- 63 -- 65 -- 67 -- 69 --]\n [71 -- 73 -- 75 -- 77 -- 79 --]\n [81 -- 83 -- 85 -- 87 -- 89 --]\n [91 -- 93 -- 95 -- 97 -- 99 --]]\n\nzusätzlich Vielfache von 3\n[[-- 2 3 -- 5 -- 7 -- -- --]\n [11 -- 13 -- -- -- 17 -- 19 --]\n [-- -- 23 -- 25 -- -- -- 29 --]\n [31 -- -- -- 35 -- 37 -- -- --]\n [41 -- 43 -- -- -- 47 -- 49 --]\n [-- -- 53 -- 55 -- -- -- 59 --]\n [61 -- -- -- 65 -- 67 -- -- --]\n [71 -- 73 -- -- -- 77 -- 79 --]\n [-- -- 83 -- 85 -- -- -- 89 --]\n [91 -- -- -- 95 -- 97 -- -- --]]\n\nzusätzlich Vielfache von 5\n[[-- 2 3 -- 5 -- 7 -- -- --]\n [11 -- 13 -- -- -- 17 -- 19 --]\n [-- -- 23 -- -- -- -- -- 29 --]\n [31 -- -- -- -- -- 37 -- -- --]\n [41 -- 43 -- -- -- 47 -- 49 --]\n [-- -- 53 -- -- -- -- -- 59 --]\n [61 -- -- -- -- -- 67 -- -- --]\n [71 -- 73 -- -- -- 77 -- 79 --]\n [-- -- 83 -- -- -- -- -- 89 --]\n [91 -- -- -- -- -- 97 -- -- --]]\n\nzusätzlich Vielfache von 7\n[[-- 2 3 -- 5 -- 7 -- -- --]\n [11 -- 13 -- -- -- 17 -- 19 --]\n [-- -- 23 -- -- -- -- -- 29 --]\n [31 -- -- -- -- -- 37 -- -- --]\n [41 -- 43 -- -- -- 47 -- -- --]\n [-- -- 53 -- -- -- -- -- 59 --]\n [61 -- -- -- -- -- 67 -- -- --]\n [71 -- 73 -- -- -- -- -- 79 --]\n [-- -- 83 -- -- -- -- -- 89 --]\n [-- -- -- -- -- -- 97 -- -- --]]\n\n```\n:::\n:::\n\n\n::: {.cell execution_count=169}\n``` {.python .cell-code}\n# 5. verschiedene Aufgaben\n## Wie viele Primzahlen sind in dem Array?\nprint(\"Anzahl Primzahlen:\", sieb.size - sieb.mask.sum())\n\n## Welches ist die größte, welches die kleinste Primzahl?\nprint(sieb.max(), sieb.min())\n\n## Primzahlen absteigend sortiert\n### ma.sort() sortiert nur aufsteigend, deshalb:\n### Übergabe des eindimensionalen arrays sieb.flatten() in umgekehrter Reihenfolge [::-1]\n### anschließend umformen in 2D-Array\n### endwith = False missing values are treated as smallest values\nprint(ma.sort(sieb.flatten(), endwith = False)[::-1].reshape(sieb.shape))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnzahl Primzahlen: 25\n97 2\n[[97 89 83 79 73 71 67 61 59 53]\n [47 43 41 37 31 29 23 19 17 13]\n [11 7 5 3 2 -- -- -- -- --]\n [-- -- -- -- -- -- -- -- -- --]\n [-- -- -- -- -- -- -- -- -- --]\n [-- -- -- -- -- -- -- -- -- --]\n [-- -- -- -- -- -- -- -- -- --]\n [-- -- -- -- -- -- -- -- -- --]\n [-- -- -- -- -- -- -- -- -- --]\n [-- -- -- -- -- -- -- -- -- --]]\n```\n:::\n:::\n\n\n:::\n\n# Wissenschaftliche Dateiformate {#sec-spezialformate}\nWissenschaftliche Datensätze können extrem groß sein. Beispielsweise hat das Kernforschungszentrum CERN im November 2018 15,8 Petabyte Daten aufgezeichnet ([Key Facts and Figures - CERN Data Centre](https://information-technology.web.cern.ch/sites/default/files/CERNDataCentre_KeyInformation_July2022_V3.pdf)). Für den Austausch großer Datenmengen wurden Datenformate wie das Hierarchical Data Format (HDF) und Network Common Data Format (netCDF) entwickelt. Diese erlauben es unter anderem, Datensätze direkt vom Massenspeicher des Computers auszulesen, sodass eine mögliche Begrenzung durch den Arbeitsspeicher vermieden wird.\n\nIm Folgenden werden knapp die Grundfunktionen für das Arbeiten mit HDF5- und netCDF4-Dateien vorgestellt. Detailliertere Informationen finden Sie in der jeweiligen Dokumentation.\n\n## HDF5\nHDF5 bedeutet Hierarchical Data Format Version 5. HDF organisiert Daten in einer Ordnerstruktur, die mit der Ordnerstruktur eines Computers vergleichbar ist, und enthält beschreibende Metadaten, weshalb HDF als selbstbeschreibend gilt. Die HDF-Verzeichnisstruktur wird mit bestimmten Begriffen beschrieben.\n\n  - Gruppen oder `groups` bezeichnen die Verzeichnisse (Unterordner) innerhalb der HDF-Datei. `groups` können weitere Gruppen oder `datasets` enthalten.\n\n  - Dateien oder `datasets` entsprechen einzelnen Dateien.\n\n::: {.border}\n![HDF5-Verzeichnisstruktur](00-bilder/hdf5_structure4-by-neonscience.org.jpg){width=\"80%\" fig-alt=\"Darstellung der Verzeichnisstruktur von HDF5. Dargestellt sind das Wurzelverzeichnis, dem Metadaten zugeordnet sind, und darin enthaltene Gruppen, die weitere Gruppen oder Datensätze enthalten. Den Objekten sind jeweils Metadaten zugeordnet.\"}\n\nAn illsturation of a HDF...and associated metadata von U.S National Science Foundation's National Ecological Observatory Network (NEON) ist abrufbar unter <https://www.neonscience.org/resources/learning-hub/tutorials/about-hdf5>.\n::: \n\n&nbsp;\n\nIn Python können die Pakete [PyTables](https://www.pytables.org/), das von Pandas verwendet wird, und h5py mit HDF5 umgehen. Das Einlesen von HDF5-Dateien funktioniert ähnlich wie das Einlesen von Dateien in der Pythonbasis: Der Zugriff auf die Datei erfolgt mit einem Dateiobjekt, über das verschiedene Methoden zum Lesen und Schreiben bereitgestellt werden. Abschließend wird das Dateiobjekt wieder geschlossen.\n\nDie Integration des Pakets PyTables in Pandas ist auf die Verwendung von Pandas-Objekten ausgelegt. Es wird eine spezifische Dateistruktur genutzt und erwartet. Anders aufgebaute HDF5-Dateien können nicht eingelesen werden. Deshalb wird das Paket hier nur kurz vorgestellt. ([Thema auf stackoverflow](https://stackoverflow.com/questions/71388502/read-hdf5-file-created-with-h5py-using-pandas), [Antwort von Kevin S unter CC-BY-SA 3.0](https://stackoverflow.com/a/33644128))\n\n::: {#wrn-hdfdata .callout-warning appearance=\"simple\"}\n## Datensatz\nIn diesem Abschnitt wird ein Teil des Datensatzes IceBridge ATM L1B Elevation and Return Strength, Version 2 genutzt ([kostenlose Registrierung bei NASA Earth erforderlich](https://nsidc.org/data/ilatm1b/versions/2#anchor-data-access-tools)). Das [Handbuch ist hier verfügbar](https://nsidc.org/sites/default/files/ilatm1b-v002-userguide_2.pdf). Der Datensatz enhält flugzeugbasierte Lasermessungen für die Höhe des Eisschildes in der Arktis und Antarktis mittels des NASA Airborne Topographic Mapper (ATM). ATM hat eine Abtastrate von 3 bis 5 kHz. Jede Datei enthält die Daten von einigen Minuten Flugdauer. Das geografische Referenzsystem ist das World Geodetic System 1984 (WGS 84)\n\nDie Dateien sind nach dem Schema ILATM1B_YYYYMMDD_HHMMSS.ATM4BT4.h5 benannt:\n\n  - Datensatz-ID_\n  \n  - Jahr, Monat, Tag der Messung_\n  \n  - Zeitpunkt am Beginn der Messung in Stunden, Minuten, Sekunden_\n\n  - Instrumenten-ID (fünfstellig) und Messwinkel (T2 = 15 Grad, T3 = 23 Grad, T4 = 30 Grad).\n\n  - xxx Dateityp (.h5 = HDF5, h5.xml = zusätzliche Metadatendatei) \n\n:::: {.border}\nStudinger, M. 2013, updated 2020. IceBridge ATM L1B Elevation and Return Strength, Version 2.\n[Indicate subset used]. Boulder, Colorado USA. NASA National Snow and Ice Data Center Distributed\nActive Archive Center. https://doi.org/10.5067/19SIM5TXKPGT. [05.12.2024]\n::::\n:::\n\n### PyTables\nDas Paket PyTables wird mit dem Befehl `pip install tables` installiert. Die Funktionen des Pakets sind in Pandas integriert. Der Zugriff auf HDF-Dateien wird in Pandas über ein [HDFStore-Objekt](https://pandas.pydata.org/docs/user_guide/io.html#hdf5-pytables) bereitgestellt. Hierrüber können HDF-Dateien gelesen und geschrieben werden. DAs HDFStore-Objekt entspricht dem Wurzelverzeichnis der strukturierten Datei. Die Funktion `pd.HDFStore(path, mode)` nimmt als Argumente den Dateipfad `path` und den Zugriffsmodus `mode` entgegen.\n\n| Zugriffsmodus | Beschreibung |\n|---|----------------------|\n| 'a' | Standardmodus append: öffnet die Datei im Lese- und Schreibmodus. Falls die Datei nicht vorhanden ist, wird diese erstellt. |\n| 'w' | Schreibzugriff, Erstellen einer neuer Datei, Überschreiben einer bestehenden, gleichnamigen Datei |\n| 'r' | Lesezugriff |\n| 'r+' | Lese- und Schreibzugriff wie 'a', Datei muss aber existieren. |\n\n&nbsp;\n\nÜber das HDFStore-Objekt kann der Inhalt der Datei ausgelesen werden, sofern das Format kompatibel ist.\n\n  - Die Methode `hdf.info()` gibt Informationen zum Dateiobjekt aus.\n  \n  - Die Methode `hdf.keys(include = 'pandas')` gibt standardmäßig die Schlüssel der Pandas-Objekte im Wurzelverzeichnis zurück. Mit dem Argument `hdf.keys(include = 'native')` sollten auch native HDF5-Objekte ausgegeben werden können, sofern die HDF5-Datei kompatibel ist.\n  \n  - Der Zugriff auf ausgewählte Schlüssel erfolgt mit der Methode `hdf.get('key')` oder durch `hdf['key']`.\n  \n  - Mit der Methode `hdf.close()` wird das HDFStore-Objekt wieder geschlossen.\n\n::: {#nte-HDFStore .callout-note collapse=\"true\"}\n## inkompatible HDF5-Datei\n\n::: {.cell execution_count=170}\n``` {.python .cell-code}\n# Datei öffnen\ndateipfad = \"01-daten/ILATM1B_20191120_041200.ATM6AT6.h5\"\nhdf = pd.HDFStore(dateipfad, mode = 'r')\n\nprint(hdf.info(), \"\\n\")\n\n# parameter include: str, default ‘pandas’.  When ‘pandas’ return pandas objects. When ‘native’ return native HDF5 Table objects.\nprint(hdf.keys(include = 'native'), \"\\n\") \nprint(hdf.keys(include = 'pandas'), \"\\n\") \n\nprint(hdf.groups(), \"\\n\")\n\n# Test hdf.get()\ntry:\n  elevation = hdf.get('elevation')\n  print(type(elevation))\nexcept TypeError as error:\n  print(\"Die Eingabe führt zu der Fehlermeldung:\\n\", error)\nelse:\n  elevation = hdf.get('elevation')\n  print(type(elevation))\n\n# Test pd.read_hdf()\ntry:\n  elevation = pd.read_hdf(path_or_buf = hdf, key = 'elevation')\n  print(type(elevation))\nexcept TypeError as error:\n  print(\"Die Eingabe führt zu der Fehlermeldung:\\n\", error)\nelse:\n  elevation = pd.read_hdf(path_or_buf = hdf, key = 'elevation')\n  print(type(elevation))\n\n# HDFStore-Objekt schließen\nhdf.close()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.io.pytables.HDFStore'>\nFile path: 01-daten/ILATM1B_20191120_041200.ATM6AT6.h5\nEmpty \n\n[] \n\n[] \n\n[] \n\nDie Eingabe führt zu der Fehlermeldung:\n cannot create a storer if the object is not existing nor a value are passed\nDie Eingabe führt zu der Fehlermeldung:\n cannot create a storer if the object is not existing nor a value are passed\n```\n:::\n:::\n\n\n:::\n\nUm die Funktionen zu demonstrieren, wird mit eine HDF5-Datei erstellt. Dazu wird ein HDFStore-Objekt im Modus `append` angelegt und mit der Methode `hdf.put(key, value, format)` ein DataFrame gespeichert.\n\n::: {.cell execution_count=171}\n``` {.python .cell-code}\n# HDF5-Datei anlegen\ndf = pd.DataFrame({'Spalte1': [1, 2, 3], 'Spalte2': [4, 5, 6]})\nhdf = pd.HDFStore(\"01-daten/hdf_file_demo.h5\", mode = 'a')\nhdf.put('daten', df, format = 'table')\n\nhdf.close()\n\n# HDF5-Datei einlesen\nhdf2 = pd.HDFStore(\"01-daten/hdf_file_demo.h5\", mode = 'r')\nprint(hdf2.info(), \"\\n\")\nprint(hdf2.keys(), \"\\n\")\n\ndf2 = hdf2.get(key = \"/daten\")\nprint(df2)\n\nhdf2.close()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.io.pytables.HDFStore'>\nFile path: 01-daten/hdf_file_demo.h5\n/daten            frame_table  (typ->appendable,nrows->3,ncols->2,indexers->[index],dc->[]) \n\n['/daten'] \n\n   Spalte1  Spalte2\n0        1        4\n1        2        5\n2        3        6\n```\n:::\n:::\n\n\nEine Liste verfügbarer Funktionen findet sich [in der Dokumentation](https://pandas.pydata.org/docs/reference/io.html#hdfstore-pytables-hdf5).  \n([Numpy Ninja](https://www.numpyninja.com/post/hdf5-file-format-with-pandas))\n\n### h5py\nDas Paket h5py wird mit dem Befehl `pip install h5py` installiert. Das Modul h5py funktioniert ählich wie PyTables. Der Zugriff auf eine HDF5-Datei erfolgt über ein Dateiobjekt, das mit der Funktion `h5py.File(Dateipfad, Zugriffsmodus)` erzeugt wird ([siehe h5py Dokumentation](https://docs.h5py.org/en/latest/high/file.html#file)).\n\n| Zugriffsmodus | Beschreibung |\n|---|----------------------|\n| r | Lesemodus, Datei muss existieren (default) |\n| r+ | Lese- und Schreibmodus, Datei muss existieren |\n| w | Datei erstellen, bestehende Datei kürzen |\n| w- or x | Datei erstellen, abbrechen wenn Datei bereits besteht |\n| a | Lesen / Schreiben, wenn die Datei besteht, andernfalls Datei erstellen |\n\n&nbsp;\n\nÜber das Dateiobjekt erfolgt der Zugriff auf Gruppen und Datensätze. Gruppen sind wie dictionaries aufgebaut, der Zugriff erfolgt also über Schlüssel-Wert-Paare (key: value). Die Schlüssel sind die Namen der Elemente in einer Gruppe, die Werte sind die Elemente selbst (Gruppen oder Datasets).\n\n::: {.border}\n\n\"The most fundamental thing to remember when using h5py is:\n\n  **Groups work like dictionaries, and datasets work like NumPy arrays**\" ([Dokumentation h5py](https://docs.h5py.org/en/latest/quick.html), Hervorhebung im Original)\n\n:::\n\n&nbsp;\n\nDie Gruppennamen können mit der Methode Dateiobjekt.keys() abgerufen werden.\n\n::: {.cell execution_count=172}\n``` {.python .cell-code}\nimport h5py\ndateipfad = \"01-daten/ILATM1B_20191120_041200.ATM6AT6.h5\"\nhdf = h5py.File(dateipfad, mode = 'r')\n\nprint(hdf, \"\\n\")\nprint(list(hdf.keys()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<HDF5 file \"ILATM1B_20191120_041200.ATM6AT6.h5\" (mode r)> \n\n['elevation', 'latitude', 'longitude', 'ancillary_data', 'instrument_parameters']\n```\n:::\n:::\n\n\nAnhand der Schlüssel kann mit der Funktion `isinstance(item, type)` geprüft werden, ob es sich um Gruppen `h5py.Group` oder Datensätze `h5py.Dataset` handelt. Datasets besitzen wie NumPy-Arrays die Attribute `dtype`, `shape`, `size`, `ndim` (und `nbytes`).\n\n::: {.cell execution_count=173}\n``` {.python .cell-code}\nfor key in list(hdf.keys()):\n  \n  object = hdf[key]\n  print(f\"Schlüssel:\\t{key} ist ein {type(object)}\")\n  \n  if isinstance(hdf[key], h5py.Dataset):\n    \n    print(f\"\\tDatentyp: {object.dtype}\\n\"\n          f\"\\tStruktur: {object.shape}\\t Dimensionen: {object.ndim}\\tAnzahl Elemente: {object.size}\\n\")\n  \n  elif isinstance(hdf[key], h5py.Group):\n    print(f\"\\tDie Gruppe {key} enthält die Objekte:\\n\\t {hdf[key].keys()}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSchlüssel:\televation ist ein <class 'h5py._hl.dataset.Dataset'>\n\tDatentyp: float32\n\tStruktur: (327285,)\t Dimensionen: 1\tAnzahl Elemente: 327285\n\nSchlüssel:\tlatitude ist ein <class 'h5py._hl.dataset.Dataset'>\n\tDatentyp: float64\n\tStruktur: (327285,)\t Dimensionen: 1\tAnzahl Elemente: 327285\n\nSchlüssel:\tlongitude ist ein <class 'h5py._hl.dataset.Dataset'>\n\tDatentyp: float64\n\tStruktur: (327285,)\t Dimensionen: 1\tAnzahl Elemente: 327285\n\nSchlüssel:\tancillary_data ist ein <class 'h5py._hl.group.Group'>\n\tDie Gruppe ancillary_data enthält die Objekte:\n\t <KeysViewHDF5 ['header_binary', 'header_text', 'max_latitude', 'max_longitude', 'min_latitude', 'min_longitude', 'reference_frame']>\n\nSchlüssel:\tinstrument_parameters ist ein <class 'h5py._hl.group.Group'>\n\tDie Gruppe instrument_parameters enthält die Objekte:\n\t <KeysViewHDF5 ['azimuth', 'gps_pdop', 'pitch', 'pulse_width', 'rcv_sigstr', 'rel_time', 'roll', 'time_hhmmss', 'xmt_sigstr']>\n\n```\n:::\n:::\n\n\nDie Objekte `elevation`, `latitude` und `longitude` sind Datensätze. Die Objekte `ancillary_data` und `instrument_parameters` sind Gruppen. Der Zugriff innerhalb von Gruppen erfolgt wie bei der Eingabe eines Dateipfads: `Dateiobjekt['Gruppe/key']`.\n\nUm mit Datasets zu arbeiten muss eine Kopie z. B. durch Slicing erstellt werden.\n\n::: {.cell execution_count=174}\n``` {.python .cell-code}\ntry:\n  print(hdf['elevation'] - 1000)\nexcept TypeError as error:\n  print(\"Der Direktzugriff führt zu der Fehlermeldung:\\n\", error)\n\n# Mit einer Kopie kann gearbeitet werden\nprint(\"\\nMit einer Kopie geht es:\")\nprint(hdf['elevation'][:] - 1000)\n\n# Mit Indexbereichen ebenso\nprint(\"\\nEbenso mit ausgewählten Indexbereichen:\")\nprint(hdf['elevation'][0:10])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDer Direktzugriff führt zu der Fehlermeldung:\n unsupported operand type(s) for -: 'Dataset' and 'int'\n\nMit einer Kopie geht es:\n[178.32202 178.34302 178.44495 ... 129.38794 128.62195 128.18799]\n\nEbenso mit ausgewählten Indexbereichen:\n[1178.322 1178.343 1178.445 1178.351 1178.363 1178.389 1178.396 1178.393\n 1178.274 1178.321]\n```\n:::\n:::\n\n\nAm Ende wird das Dateiobjekt geschlossen.\n\n::: {.cell execution_count=175}\n``` {.python .cell-code}\nhdf.close()\n```\n:::\n\n\n### Übung Zugriff auf H5P-Datasets\n**Berechnen Sie aus der relativen Zeit `hdf['instrument_parameters/rel_time']` im Format 'hhmmss' und dem im Dateinamen enthaltenen Beginn der Messung die tatsächliche Zeit der Messung.** Der Dateipfad lautet: '01-daten/ILATM1B_20191120_041200.ATM6AT6.h5'.  \n*Hinweis: Die relative Zeit liegt in Tausendstelsekunden aufgelöst vor. Aufgrund der hohen Abtastrate von ATM kommen Zeiten in der Regel mehrfach vor.*\n\n::: {#tip-Messzeit .callout-tip collapse=\"true\"}\n## Musterlösung absolute Zeit berechnen\n\n::: {.cell execution_count=176}\n``` {.python .cell-code}\n# HDF-Datei öffnen\ndateipfad = \"01-daten/ILATM1B_20191120_041200.ATM6AT6.h5\"\nhdf = h5py.File(dateipfad, mode = 'r')\n\nzeitstempel = int(str(hdf)[29:35])\nprint(f\"Beginn der Messungen: {zeitstempel}\\n\")\n\nrel_time = hdf['instrument_parameters/rel_time'][:]\nabs_time = rel_time + zeitstempel\n\nprint(f\"rel_time[-5:]:\\n{rel_time[-5:]}\\n\\nabs_time[-5:]:\\n{abs_time[-5:]}\")\n\n# HDF-Datei schließen\nhdf.close()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBeginn der Messungen: 41200\n\nrel_time[-5:]:\n[59.986 59.986 59.987 59.989 59.99 ]\n\nabs_time[-5:]:\n[41259.984 41259.984 41259.99  41259.99  41259.99 ]\n```\n:::\n:::\n\n\n:::\n\n#### HDF5 Dateien schreiben\nUm Objekte in eine HDF5-Datei zu schreiben, muss diese im Schreibmodus geöffnet werden.\n\n::: {.cell execution_count=177}\n``` {.python .cell-code}\nhdf_neu = h5py.File('01-daten/hdf_neu.h5', mode = 'w')\nhdf_neu['abs_time'] = abs_time\nhdf_neu.close()\n\n# Kontrolle\nhdf_neu = h5py.File('01-daten/hdf_neu.h5', mode = 'r')\nprint(hdf_neu.keys())\nhdf_neu.close()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<KeysViewHDF5 ['abs_time']>\n```\n:::\n:::\n\n\n([Dokumentation h5py](https://docs.h5py.org/en/latest/quick.html))\n\n\n## netCDF4\nDas Paket netCDF4 (Network Common Data Format Version 4) wird mit `pip install netCDF4` installiert. netCDF4 basiert auf HDF5, verwendet aber eine eigene Terminologie. Der Zugriff auf eine netCDF4 Datei erfolgt über das Dataset-Objekt, das der Wurzelgruppe entspricht. Eine netCDF-Datei bzw. das Dataset-Objekt besteht aus:\n\n  - Gruppen (groups): Gruppen sind Unterverzeichnisse des Dataset-Objekts und können Variablen, Dimensionen, Attribute und weitere Gruppen enthalten.\n  \n  - Variablen (variables): Variablen entsprechen Datensätzen bzw. NumPy-Arrays.\n\n  - Dimensionen (dimensions): Dimensionen beschreiben Eigenschaften der Variablen wie Name (name), Größe (size), Gruppenzugehörigkeit (group)\n\n  - Attribute (attributes): Attribute beschreiben entweder das gesamte Dataset (globale Attribute) oder einzelne Variablen. Attribute speichern die Metadaten.\n\n    - `Dataset.description = 'Beschreibung des Datensatzes'`\n\n    - `Dataset.history = 'Zeitpunkt der Erstellung'`\n\n    - `Dataset.source = 'Quelle'`\n\n    - `Dataset.units = 'Beschreibung der Messeinheit'`\n\n    - `Dataset.calender = 'gregorianischer Kalender'`\n    \nDiese Elemente sind als Dictionary aufgebaut und erlauben den Zugriff über Schlüssel-Wert-Paare.\n\n::: {#wrn-netcdfdata .callout-wrn appearance=\"simple\"}\n## Datensatz\nIn diesem Abschnitt wird ein Datensatz der NASA zur Blitzdichte (Anzahl Blitze pro Quadratkilometer) verwendet [kostenlose Registrierung bei NASA Earth erforderlich](https://www.earthdata.nasa.gov/data/catalog/ghrc-daac-lohrfc-2.3.2015). Die netCDF4-Datei enthält verschiedene Datensätze (Variablen), die mit satellitengestützten Messinstrumenten erstellt wurden. Blitze im Bereich von +/- 38 Grad um den Äquator wurden mit dem Lightning Imaging Sensor (LIS) des Satelliten der Tropical Rainfall Measuring Mission (TRMM) gemessen. In höheren und niedrigeren Breitengraden wurden Blitze mit dem Optical Transient Detector (OTD) auf Orbview-1 gemessen. Die Daten liegen in einer Auflösung von 0.5 Grad vor. ([Datensatzdokumentation](https://www.earthdata.nasa.gov/data/catalog/ghrc-daac-lolrac-2.3.2015), [Poetzsch 2021](https://refubium.fu-berlin.de/handle/fub188/43384): 182-183)\n\nDie Datensätze folgen dem Namensschema HRFC_COM_FR\n\n  - HRFC = High Resolution Full Climatology\n  \n  - COM = Kombination beider Messinstrumente [LIS, OTD]\n  \n  - FR = Flash Rates [RF = Raw Flash Rates, SF = Scaled Flash Rates]\n\n:::: {.border}\nLIS/OTD 0.5 Degree High Resolution Full Climatology (HRFC) V2.3.2015 von Global Hydrometeorology Resource Center DAAC (GHRC DAAC)\n\nDOI https://doi.org/10.5067/LIS/LIS-OTD/DATA302\n\n::::\n:::\n\n&nbsp;\n\nEin Dataset-Objekt wird mit der Funktion `nc.Dataset(filename, mode)` geöffnet. Mit dem Argument `filename` wird der Dateipfad übergeben. Im Argument `mode` wird der Zugriffsmodus festgelegt.\n\n| Zugriffsmodus | Beschreibung |\n|---|----------------------|\n| r | Lesemodus, Datei muss existieren (default) |\n| r+ | Update-Modus, Datei wird ggf. angelegt und kann gelesen und geschrieben werden |\n| w | Schreibmodus Datei erstellen, bestehende Datei überschreiben |\n| x | Datei erstellen, abbrechen wenn Datei bereits besteht |\n| a | Anhängen: Datei wird ggf. angelegt, neue Inhalte werden am Ende der Datei angehängt, bestehende Inhalte werden dabei nicht gelöscht. |\n\n&nbsp;\n\nnetCDF-Dateien können verschiedene Versionen haben (NETCDF3_CLASSIC, NETCDF3_64BIT_OFFSET, NETCDF3_64BIT_DATA, NETCDF4_CLASSIC, NETCDF4). Um die Version abzurufen, kann das Attribut `cdf.data_model` abgerufen werden.\n\n::: {.cell execution_count=178}\n``` {.python .cell-code}\nimport netCDF4 as nc\n\ndateipfad = \"01-daten/LISOTD_HRFC_V2.3.2015.nc\"\ncdf = nc.Dataset(filename = dateipfad, mode = 'r')\nprint(f\"Dateiversion: {cdf.data_model}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDateiversion: NETCDF4\n\n```\n:::\n:::\n\n\nDie wesentlichen Informationen können mit der print-Funktion ausgegeben werden `print(cdf)`. Die Ausgabe ist jedoch besonders für komplexere Dateien unübersichtlich.\n\nDie Gruppen der Datei können mit dem Attribut `print(cdf.groups)` ausgegeben werden.\n\n::: {.cell execution_count=179}\n``` {.python .cell-code}\nprint(cdf.groups)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{}\n```\n:::\n:::\n\n\nDie Ausgabe ist ein Dictionary, das in diesem Fall leer ist. Die Datei verfügt also über keine Unterverzeichnisse (Gruppen).\n\nDie Variablen (die Datensätze) können mit dem Attribut `print(cdf.variables)` ausgegeben werden, die Ausgabe ist ebenfalls ein Dictionary. Die vollständige Ausgabe findet sich im folgenden Beispiel.\n\n::: {.cell execution_count=180}\n``` {.python .cell-code}\nvariables = cdf.variables\nfor key, value in variables.items():\n    print(key)\n    print(value, \"\\n\")\n    break # Abbruch nach erstem Schlüssel-Wert-Paar\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDE_By_Threshold\n<class 'netCDF4.Variable'>\nfloat32 DE_By_Threshold(DE_By_Threshold)\n    long_name: By Threshold\n    comment: Threshold index for OTD detection efficiency\n    units: 1\nunlimited dimensions: \ncurrent shape = (3,)\nfilling on, default _FillValue of 9.969209968386869e+36 used \n\n```\n:::\n:::\n\n\n::: {#nte-cdf .callout-note collapse=\"true\"}\n## vollständige Ausgabe cdf.variables\n\n::: {.cell execution_count=181}\n``` {.python .cell-code}\nvariables = cdf.variables\nfor key, value in variables.items():\n    print(key)\n    print(value, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDE_By_Threshold\n<class 'netCDF4.Variable'>\nfloat32 DE_By_Threshold(DE_By_Threshold)\n    long_name: By Threshold\n    comment: Threshold index for OTD detection efficiency\n    units: 1\nunlimited dimensions: \ncurrent shape = (3,)\nfilling on, default _FillValue of 9.969209968386869e+36 used \n\nDE_Local_Hour\n<class 'netCDF4.Variable'>\nfloat32 DE_Local_Hour(DE_Local_Hour)\n    long_name: Local Hour\n    comment: Lightning detection efficiency is the function of local hour. A day is divided into 24 bins. The values indicate the mean time of each bin.\n    units: 1\nunlimited dimensions: \ncurrent shape = (24,)\nfilling on, default _FillValue of 9.969209968386869e+36 used \n\nDE_LowRes_Latitude\n<class 'netCDF4.Variable'>\nfloat32 DE_LowRes_Latitude(DE_LowRes_Latitude)\n    long_name: LowRes Latitude\n    standard_name: latitude\n    axis: Y\n    units: degrees_north\nunlimited dimensions: \ncurrent shape = (72,)\nfilling on, default _FillValue of 9.969209968386869e+36 used \n\nDE_LowRes_Longitude\n<class 'netCDF4.Variable'>\nfloat32 DE_LowRes_Longitude(DE_LowRes_Longitude)\n    long_name: LowRes Longitude\n    standard_name: longitude\n    axis: X\n    units: degrees_east\nunlimited dimensions: \ncurrent shape = (144,)\nfilling on, default _FillValue of 9.969209968386869e+36 used \n\nHRFC_AREA\n<class 'netCDF4.Variable'>\nfloat32 HRFC_AREA(Latitude, Longitude)\n    valid_range: [  13.48325 3090.078  ]\n    long_name: Grid Cell Area\n    units: km^2\n    _FillValue: 0.0\nunlimited dimensions: \ncurrent shape = (360, 720)\nfilling on \n\nHRFC_COM_FR\n<class 'netCDF4.Variable'>\nfloat32 HRFC_COM_FR(Latitude, Longitude)\n    valid_range: [  0.     201.7618]\n    long_name: Combined Flash Rate Climatology\n    _FillValue: 0.0\n    units: count/km^2/year\nunlimited dimensions: \ncurrent shape = (360, 720)\nfilling on \n\nHRFC_LIS_DE\n<class 'netCDF4.Variable'>\nfloat32 HRFC_LIS_DE(DE_Local_Hour, DE_LowRes_Latitude, DE_LowRes_Longitude)\n    valid_range: [ 0.      88.00034]\n    long_name: Applied LIS Detection Efficiency\n    units: Percent\n    _FillValue: 0.0\nunlimited dimensions: \ncurrent shape = (24, 72, 144)\nfilling on \n\nHRFC_LIS_FR\n<class 'netCDF4.Variable'>\nfloat32 HRFC_LIS_FR(Latitude, Longitude)\n    valid_range: [   0.    5723.275]\n    long_name: LIS Flash Rate Climatology\n    _FillValue: 0.0\n    units: count/km^2/year\nunlimited dimensions: \ncurrent shape = (360, 720)\nfilling on \n\nHRFC_LIS_RF\n<class 'netCDF4.Variable'>\nfloat32 HRFC_LIS_RF(Latitude, Longitude)\n    valid_range: [   0. 9770.]\n    long_name: LIS Raw Flashes\n    _FillValue: 0.0\n    units: count\nunlimited dimensions: \ncurrent shape = (360, 720)\nfilling on \n\nHRFC_LIS_SF\n<class 'netCDF4.Variable'>\nfloat32 HRFC_LIS_SF(Latitude, Longitude)\n    valid_range: [    0.   12500.17]\n    long_name: LIS Scaled Flashes\n    _FillValue: 0.0\n    units: count\nunlimited dimensions: \ncurrent shape = (360, 720)\nfilling on \n\nHRFC_LIS_VT\n<class 'netCDF4.Variable'>\nfloat32 HRFC_LIS_VT(Latitude, Longitude)\n    valid_range: [0.000000e+00 4.462968e+09]\n    long_name: LIS Viewtime\n    units: km^2 sec\n    _FillValue: 0.0\nunlimited dimensions: \ncurrent shape = (360, 720)\nfilling on \n\nHRFC_OTD_DE\n<class 'netCDF4.Variable'>\nfloat32 HRFC_OTD_DE(DE_By_Threshold, DE_Local_Hour, DE_LowRes_Latitude, DE_LowRes_Longitude)\n    valid_range: [ 0.     53.6012]\n    long_name: Applied OTD Base Detection Efficiency\n    units: Percent\n    _FillValue: 0.0\nunlimited dimensions: \ncurrent shape = (3, 24, 72, 144)\nfilling on \n\nHRFC_OTD_FR\n<class 'netCDF4.Variable'>\nfloat32 HRFC_OTD_FR(Latitude, Longitude)\n    valid_range: [  0.     201.7618]\n    long_name: OTD Flash Rate Climatology\n    _FillValue: 0.0\n    units: count/km^2/year\nunlimited dimensions: \ncurrent shape = (360, 720)\nfilling on \n\nHRFC_OTD_RF\n<class 'netCDF4.Variable'>\nfloat32 HRFC_OTD_RF(Latitude, Longitude)\n    valid_range: [   0. 1298.]\n    long_name: OTD Raw Flashes\n    _FillValue: 0.0\n    units: count\nunlimited dimensions: \ncurrent shape = (360, 720)\nfilling on \n\nHRFC_OTD_SF\n<class 'netCDF4.Variable'>\nfloat32 HRFC_OTD_SF(Latitude, Longitude)\n    valid_range: [   0.    2645.824]\n    long_name: OTD Scaled Flashes\n    _FillValue: 0.0\n    units: count\nunlimited dimensions: \ncurrent shape = (360, 720)\nfilling on \n\nHRFC_OTD_VT\n<class 'netCDF4.Variable'>\nfloat32 HRFC_OTD_VT(Latitude, Longitude)\n    valid_range: [0.000000e+00 1.119883e+09]\n    long_name: OTD Viewtime\n    units: km^2 sec\n    _FillValue: 0.0\nunlimited dimensions: \ncurrent shape = (360, 720)\nfilling on \n\nLatitude\n<class 'netCDF4.Variable'>\nfloat32 Latitude(Latitude)\n    long_name: Latitude\n    standard_name: latitude\n    axis: Y\n    units: degrees_north\nunlimited dimensions: \ncurrent shape = (360,)\nfilling on, default _FillValue of 9.969209968386869e+36 used \n\nLongitude\n<class 'netCDF4.Variable'>\nfloat32 Longitude(Longitude)\n    long_name: Longitude\n    standard_name: longitude\n    axis: X\n    units: degrees_east\nunlimited dimensions: \ncurrent shape = (720,)\nfilling on, default _FillValue of 9.969209968386869e+36 used \n\nlis_rf_data_centered\n<class 'netCDF4.Variable'>\nfloat32 lis_rf_data_centered(latitude, longitude)\n    description: zentrierte Blitzanzahl\n    units: absolute Abweichung vom Mittelwert\nunlimited dimensions: \ncurrent shape = (360, 720)\nfilling on, default _FillValue of 9.969209968386869e+36 used \n\n```\n:::\n:::\n\n\n:::\n\nEbenso können die Dimensionen (`cdf.dimensions`) und die globalen Attribute des Datasets (`cdf.__dict__` oder als Liste mit `cdf.ncattrs()`) ausgegeben werden. Ebenso können die Attribute einer Variablen ausgegeben werden. Variablen werden direkt über den Schlüssel `variable = cdf['key']` oder mit der Funktion `variable = cdf.variables['key']` abgerufen.\n\n::: {.panel-tabset}\n## Dimensionen\n\n::: {.cell execution_count=182}\n``` {.python .cell-code}\ndimensions = cdf.dimensions\n\nfor key, value in dimensions.items():\n    print(key)\n    print(value, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDE_By_Threshold\n\"<class 'netCDF4.Dimension'>\": name = 'DE_By_Threshold', size = 3 \n\nDE_Local_Hour\n\"<class 'netCDF4.Dimension'>\": name = 'DE_Local_Hour', size = 24 \n\nDE_LowRes_Latitude\n\"<class 'netCDF4.Dimension'>\": name = 'DE_LowRes_Latitude', size = 72 \n\nDE_LowRes_Longitude\n\"<class 'netCDF4.Dimension'>\": name = 'DE_LowRes_Longitude', size = 144 \n\nLatitude\n\"<class 'netCDF4.Dimension'>\": name = 'Latitude', size = 360 \n\nLongitude\n\"<class 'netCDF4.Dimension'>\": name = 'Longitude', size = 720 \n\nlatitude\n\"<class 'netCDF4.Dimension'>\": name = 'latitude', size = 360 \n\nlongitude\n\"<class 'netCDF4.Dimension'>\": name = 'longitude', size = 720 \n\n```\n:::\n:::\n\n\n## globale Attribute\n\n::: {.cell execution_count=183}\n``` {.python .cell-code}\nprint(cdf.ncattrs(), \"\\n\")\nglobal_attributes = cdf.__dict__\n\nfor key, value in global_attributes.items():\n    print(key)\n    print(value, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['NCProperties', 'OTD_Date_Window_Start', 'OTD_Date_Window_End', 'OTD_Threshold_Level_Used', 'Conventions', 'history', 'NCO'] \n\nNCProperties\nversion=1|netcdflibversion=4.4.1|hdf5libversion=1.10.0 \n\nOTD_Date_Window_Start\n[1995.333 1995.44  1995.55  1996.81 ] \n\nOTD_Date_Window_End\n[1995.44  1995.55  1996.81  2000.333] \n\nOTD_Threshold_Level_Used\n[0 2 1 0] \n\nConventions\nCF-1.6 \n\nhistory\nFri Dec 09 16:42:21 2016: ncks -4 test.nc3 LISOTD_HRFC_V2.3.2014_rename.nc\nFri Dec 09 16:42:21 2016: ncrename -v DE By Threshold,DE_By_Threshold test.nc3\nFri Dec 09 16:42:21 2016: ncrename -d DE By Threshold,DE_By_Threshold test.nc3\nFri Dec 09 16:42:21 2016: ncrename -v DE Local Hour,DE_Local_Hour test.nc3\nFri Dec 09 16:42:21 2016: ncrename -d DE Local Hour,DE_Local_Hour test.nc3\nFri Dec 09 16:42:21 2016: ncrename -v DE LowRes Longitude,DE_LowRes_Longitude test.nc3\nFri Dec 09 16:42:21 2016: ncrename -d DE LowRes Longitude,DE_LowRes_Longitude test.nc3\nFri Dec 09 16:42:21 2016: ncrename -v DE LowRes Latitude,DE_LowRes_Latitude test.nc3\nFri Dec 09 16:42:21 2016: ncrename -d DE LowRes Latitude,DE_LowRes_Latitude test.nc3\nFri Dec 09 16:42:21 2016: ncrename -a global@OTD Threshold Level Used,OTD_Threshold_Level_Used test.nc3\nFri Dec 09 16:42:21 2016: ncrename -a global@OTD Date Window End,OTD_Date_Window_End test.nc3\nFri Dec 09 16:42:21 2016: ncrename -a global@OTD Date Window Start,OTD_Date_Window_Start test.nc3\nFri Dec 09 16:42:21 2016: ncrename -a global@_NCProperties,NCProperties test.nc3\nFri Dec 09 16:42:20 2016: ncks -3 LISOTD_HRFC_V2.3.2014.nc test.nc3 \n\nNCO\n\"4.6.0\" \n\n```\n:::\n:::\n\n\n## Attribute einer Variablen\n\n::: {.cell execution_count=184}\n``` {.python .cell-code}\nvariable = cdf['HRFC_OTD_FR']\n# Alternativ:\n# variable = cdf.variables['HRFC_OTD_FR']\n\nprint(\"\\nAttribute einer Variablen\")\nprint(f\"Als Liste mit variable.ncattrs():\\n{variable.ncattrs()}\\n\")\n\nprint(f\"Als Dictionary mit variable.__dict__:\\n{variable.__dict__}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nAttribute einer Variablen\nAls Liste mit variable.ncattrs():\n['valid_range', 'long_name', '_FillValue', 'units']\n\nAls Dictionary mit variable.__dict__:\n{'valid_range': array([  0.    , 201.7618], dtype=float32), 'long_name': 'OTD Flash Rate Climatology', '_FillValue': np.float32(0.0), 'units': 'count/km^2/year'}\n\n```\n:::\n:::\n\n\n:::\n\n### Daten in netCDF4-Dateien schreiben\nUm Daten in eine netCDF4-Datei zu schreiben, muss diese in einem der Schreibmodi geöffnet werden `Dataset = nc.Dataset('pfad', mode = 'a')`. Um eine neue Variable anzulegen, muss eine entsprechende Dimension angelegt werden. Wenn die Variable in einen noch nicht bestehenden Pfad gespeichert werden soll, sind auch die entsprechenden Gruppen anzulegen.\n\n  - Gruppen im Wurzelverzeichnis werden mit der Funktion `Dataset.createGroup(\"Gruppenname\")` angelegt. Unterverzeichnisse werden analog als Pfad angelegt: `Dataset.createGroup(\"/Gruppenname/Unterverzeichnis\")` (führenden '/' beachten).\n\n  - Dimensionen beschreiben die Größe einer Variablen und müssen vor der Variablen angelegt werden (Ein Skalar, also ein Einzelwert, hat keine Dimension): `Dataset.createDimension('Name der Dimension', Größe der Dimension als Ganzzahl)`. Durch die Übergabe von None oder 0 als Größe, wird eine unbegrenzt große Dimension angelegt (netCDF4-Variablen verhalten sich wie NumPy-Arrays, besitzen aber keine feste Größe. Das bedeutet, es können weitere Daten angehängt werden.). Um beispielsweise eine zweidimensionale Datenstruktur mit 10 Zeilen und 5 Spalten anzulegen, müssen beide Dimensionen seperat deklariert werden: `Dataset.createDimension('Dimension1', 10)`, `Dataset.createDimension('Dimension2', 5)`.\n\n  - Variablen werden mit der Funktion `Dataset.createVariable('Name oder Pfad', 'Datentypkürzel', ('Name der Dimension', )). Die Argumente 'Name oder Pfad' und 'Datentypkürzel' sind Pflichtangaben. Die Dimensionen der Variable wurden zuvor mit `cdf_neu.createDimension('Name der Dimension', Größe der Dimension als Ganzzahl)` definiert und werden als Tupel übergeben. Ein Einzelwert wird erstellt, indem die Angabe der Dimension weggelassen wird. Eine zweidimensionale Datenstruktur lässt sich so anlegen: `Dataset.createVariable('Name oder Pfad der Variablen', 'Datentypkürzel', ('Dimension1', 'Dimension2', ))` [siehe Dokumentation](https://unidata.github.io/netcdf4-python/#variables-in-a-netcdf-file).\n\n| Datentypkürzel | Datentyp |\n|:---:|:---:|\n| 'f4' | 32-bit floating point |\n| 'f8' | 64-bit floating point |\n| 'i4' | 32-bit signed integer |\n| 'i2' | 16-bit signed integer |\n| 'i8' | 64-bit signed integer |\n| 'i1' | 8-bit signed integer |\n| 'u1' | 8-bit unsigned integer |\n| 'u2' | 16-bit unsigned integer |\n| 'u4' | 32-bit unsigned integer |\n| 'u8' | 64-bit unsigned integer |\n| 'S1' | single-character string |\n\n&nbsp;\n\nIm folgenden Code wird das Objekt abs_time aus dem vorherigen Abschnitt in einer netCDF4-Datei gespeichert. Mit den Attributen `variablenname.description`, `variablename.source` und `variablenname.units` werden die Metadaten zur Einheit (hier: Millisekunden) hinterlegt. Dazu wird die Anweisung zum Erstellen der Variable einem Objekt `variablename` zugewiesen.\n\n::: {.cell execution_count=185}\n``` {.python .cell-code}\n# Neues Dataset im Schreibmodus öffnen\ncdf_neu = nc.Dataset('01-daten/cdf_neu.nc', mode = 'w')\n\n# Gruppe 'Daten' anlegen\ncdf_neu.createGroup(\"Daten\")\n\n# Dimension anlegen\ncdf_neu.createDimension('abs_time', abs_time.shape[0])\n\n# Variable anlegen als 'f4' 32-bit float und einem Objekt zuweisen.\nvariablename = cdf_neu.createVariable('/Daten/abs_time', 'f4', ('abs_time', ))\n\n## Attribute anlegen\nvariablename.description = 'sehr wichtige Zeitinformationen'\nvariablename.source = 'berechnet aus IceBridge ATM L1B Elevation and Return Strength, Version 2'\nvariablename.units = 'Millisekunden'\n\n\n# Dataset schließen\ncdf_neu.close()\n\n# Kontrolle\ncdf_neu = nc.Dataset('01-daten/cdf_neu.nc', mode = 'r')\nprint(cdf_neu.groups, \"\\n\")\n\nvariables = cdf_neu['/Daten'].variables\nfor key, value in variables.items():\n    print(key)\n    print(value, \"\\n\")\n\ncdf_neu.close()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{'Daten': <class 'netCDF4.Group'>\ngroup /Daten:\n    dimensions(sizes): \n    variables(dimensions): float32 abs_time(abs_time)\n    groups: } \n\nabs_time\n<class 'netCDF4.Variable'>\nfloat32 abs_time(abs_time)\n    description: sehr wichtige Zeitinformationen\n    source: berechnet aus IceBridge ATM L1B Elevation and Return Strength, Version 2\n    units: Millisekunden\npath = /Daten\nunlimited dimensions: \ncurrent shape = (327285,)\nfilling on, default _FillValue of 9.969209968386869e+36 used \n\n```\n:::\n:::\n\n\n#### Operationen mit Variablen\nWie bei HDF5-Dateien können Operationen mit Variablen nicht direkt ausgeführt werden.\n\n::: {.cell execution_count=186}\n``` {.python .cell-code}\ntry:\n  print(cdf['HRFC_OTD_FR'] - 1)\nexcept TypeError as error:\n  print(\"Der Direktzugriff führt zu der Fehlermeldung:\\n\", error)\n\n# Mit einer Kopie kann gearbeitet werden\nprint(\"\\nMit einer Kopie geht es:\")\nprint(cdf['HRFC_OTD_FR'][ :, :])\n\n# Mit Indexbereichen ebenso\nprint(\"\\nEbenso mit ausgewählten Indexbereichen:\")\nprint(cdf['HRFC_OTD_FR'][30:40, 30:40])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDer Direktzugriff führt zu der Fehlermeldung:\n unsupported operand type(s) for -: 'netCDF4._netCDF4.Variable' and 'int'\n\nMit einer Kopie geht es:\n[[-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]\n ...\n [-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]]\n\nEbenso mit ausgewählten Indexbereichen:\n[[-- -- -- -- -- -- -- -- -- --]\n [-- -- -- -- -- -- -- -- -- --]\n [-- -- -- -- -- -- -- -- -- --]\n [-- -- -- -- -- -- -- -- -- --]\n [-- -- -- -- -- -- -- -- -- --]\n [-- -- -- -- -- -- -- -- -- --]\n [-- -- -- -- -- -- -- -- -- --]\n [-- -- -- -- -- -- -- -- -- --]\n [-- -- -- -- -- -- -- -- -- --]\n [-- -- -- -- -- -- -- -- -- --]]\n```\n:::\n:::\n\n\nAm Ende wird die netCDF4-Datei geschlossen.\n\n::: {.cell execution_count=187}\n``` {.python .cell-code}\ncdf.close()\n```\n:::\n\n\n### Übung Zugriff auf netCDF-Datasets\n**Greifen Sie in der Datei unter dem Pfad '01-daten/LISOTD_HRFC_V2.3.2015.nc' auf die Variable 'HRFC_LIS_RF' zu.**\n\n  - Geben Sie die Attribute der Variablen aus.\n  \n  - Bestimmen Sie die minimale, maximale und die durchschnittliche Anzahl der Blitze.\n\n  - Zentrieren Sie die Daten (Wert - Mittelwert) und hängen Sie eine neue Variable 'HRFC_LIS_RF_centered' an das Dataset an.\n\n\n::: {#tip-netCDF .callout-tip collapse=\"true\"}\n\nDa die Ausführung des Codes die Datei entsprechend der Aufgabenstellung verändert, ist der Code nicht als Python-Code eingebunden.\n\n``` {.raw}\n# Datei im Modus append einlesen\ndateipfad = \"01-daten/LISOTD_HRFC_V2.3.2015.nc\"\ncdf = nc.Dataset(filename = dateipfad, mode = 'a')\nprint(cdf.data_model, \"\\n\")\n\n# Variable einlesen\nlis_rf =  cdf['HRFC_LIS_RF']\n\n# Attribute der Variablen ausgeben\nprint(\"Attribute der Variablen 'HRFC_LIS_RF':\", lis_rf.ncattrs(), \"\\n\")\n\n# Daten aus der Variablen auslesen\nlis_rf_data =  cdf['HRFC_LIS_RF'][: , :]\nprint(f\"min: {lis_rf_data.min()}\\tmax: {lis_rf_data.max()}\\tmean: {lis_rf_data.mean()}\\n\")\n\n# Daten zentrieren\n# runden wegen interner Darstellung von Gleitkommazahlen - optional\nlis_rf_data_centered = lis_rf_data - lis_rf_data.mean()\nprint(f\"min: {lis_rf_data_centered.min()}\\tmax: {lis_rf_data_centered.max()}\\tmean: {lis_rf_data_centered.mean()}\\tmean gerundet: {round(lis_rf_data_centered.mean())}\\n\")\n\n# Daten an die netCDF4-Datei anhängen\n\n## Dimension anlegen\n## latitute und longitude sind die Dimensionen des 2-dimensionalen Arrays\n## Code kann nur einmal ausgeführt werden, prüfen mit try: legt die Dimension bereits an\n## deshalb Konstruktion mit not in, um zu prüfen, ob Dimension bereites existiert\n\ndimensions = cdf.dimensions\n\n### latitude\nDIMENSION = 'latitude'\nif DIMENSION not in dimensions:\n  print(f\"Die Dimension {DIMENSION} existiert nicht und wird angelegt.\")\n  cdf.createDimension(DIMENSION, lis_rf_data_centered.shape[0])\nelse:\n  print(f\"Die Dimension {DIMENSION} existiert bereits.\")\n\n### longitude\nDIMENSION = 'longitude'\nif DIMENSION not in dimensions:\n  print(f\"Die Dimension {DIMENSION} existiert nicht und wird angelegt.\")\n  cdf.createDimension(DIMENSION, lis_rf_data_centered.shape[1])\nelse:\n  print(f\"Die Dimension {DIMENSION} existiert bereits.\")\n\n## Variable 'lis_rf_data_centered' anlegen als 'f4' 32-bit float und dem Objekt variablename zuweisen.\n## latitute und longitude sind die Dimensionen des 2-dimensionalen Arrays\n## Code kann nur einmal ausgeführt werden, prüfen mit try: legt die Variable bereits an, deshalb Konstruktion mit not in\nvariables = cdf.variables\n\nVARIABLE = 'lis_rf_data_centered'\n\nif VARIABLE not in variables:\n  print(f\"Die Variable {VARIABLE} existiert nicht und wird angelegt.\\n\")\n  variablename = cdf.createVariable('lis_rf_data_centered', 'f4', ('latitude', 'longitude', ))\n\n  ### Daten einsetzen\n  variablename[ :, :] = lis_rf_data_centered\n\n  ### Attribute anlegen\n  variablename.description = 'zentrierte Blitzanzahl'\n  variablename.units = 'absolute Abweichung vom Mittelwert'\nelse:\n  print(f\"Die Variable {VARIABLE} existiert bereits.\\n\")\n\n# Dataset schließen\ncdf.close()\n\n# Kontrolle\ndateipfad = \"01-daten/LISOTD_HRFC_V2.3.2015.nc\"\ncdf = nc.Dataset(filename = dateipfad, mode = 'r')\n\nvariables = cdf.variables\nfor key, value in variables.items():\n  print(key)\n  # print(value, \"\\n\")\n\nkontrolle =  cdf['lis_rf_data_centered'][: , :]\n\nprint(f\"\\nTyp der Datenstruktur lis_rf_data_centered: {type(kontrolle)}\\nAnzahl der Elemente: {kontrolle.size}\\tAnzahl Werte: {kontrolle.mask.sum()}\\tMittelwert der Werte: {kontrolle.mean()}\")\n```\n::: \n\n# Das Wichtigste\nDas Einlesen strukturierter Datensätze mag nach der Bearbeitung dieses Bausteins wie ein undurchdringlicher Dschungel aus erforderlichem Detailwissen scheinen: verschiedene technische Formate, die große Auswahl an Paketen und Funktionen und ein häufig nicht sonderlich intuitiver innerer Aufbau, der aufgeräumt werden muss. Der Erfolg beim Einlesen schwieriger Datensätze liegt aber im strukturierten Vorgehen - dazu mehr in dem folgenden Video:\n\n<div style=\"position: relative; width: 100%; aspect-ratio: 16 / 9;\">\n  <iframe src=\"https://av.tib.eu/player/71639\" allowfullscreen style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;\"></iframe>\n</div>\n\n&nbsp;\n\nZum Schluss noch ein Tipp: Für sehr große Datensätze ist eine visuelle Kontrolle nicht immer praktikabel. Hier ist es hilfreich, sich Testdaten zu schreiben, die die erwartete Struktur des Datensatzes widerspiegeln, um damit die einzelnen Schritte zu programmieren. Vergleichen Sie anschließend das Resultat für die Testdaten mit dem für den eigentlichen Datensatz. \n\n# Lernzielkontrolle\n\n## Kompetenzquiz\n\n1. Unter dem Dateipfad '01-daten/quiz-aufgabe1.csv' liegt folgender Datensatz.\n\n| ID | Name | Geburtsdatum | Gehalt |\n|---|---|---|---|\n| 1 | Anna | 1986-04-12 | 55000 |\n| 2 | Bernd | 1990-05-23 | 62000 |\n| 3 | Carla | 1982-11-30 | 71000 |\n| 4 | David | 1975-03-15 | 58000 |\n\nJedoch gelingt das Einlesen mit der Pandas-Funktion `pd.read_csv(filepath_or_buffer = dateipfad)` nicht wie gewünscht.\n\n::: {.cell execution_count=188}\n``` {.python .cell-code}\nimport pandas as pd\n\ndateipfad = '01-daten/quiz-aufgabe1.csv'\naufgabe1 = pd.read_csv(filepath_or_buffer = dateipfad)\nprint(aufgabe1.info())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4 entries, 0 to 3\nData columns (total 1 columns):\n #   Column                       Non-Null Count  Dtype \n---  ------                       --------------  ----- \n 0   ID;Name;Geburtsdatum;Gehalt  4 non-null      object\ndtypes: object(1)\nmemory usage: 164.0+ bytes\nNone\n```\n:::\n:::\n\n\nWelche Argumente müssen der Pandas-Funktion `pd.read_csv(filepath_or_buffer = dateipfad)` übergeben werden, damit der Datensatz korrekt eingelesen wird?\n\n2. Ein Fehler bei der Datenerhebung soll korrigiert werden. Kennzeichnen Sie alle Beobachtungen für Personen, die vor 1980 geboren wurden als ungültig.\n\n3. Sortieren Sie die Spalte Geburtsdatum aufsteigend (jüngste Person zuerst). Sortieren Sie den gesamten Dataframe aufsteigend.\n\n4. Warum ist das Geburtsdatum der jüngsten Person \"größer\" als das der nächstälteren Person?\n\n5. In einem Ordner liegen folgende Dateien: ['Datei0.jpg', 'Datei1.csv', 'Datei2.txt', 'Datei3.png']. Sie möchten alle Dateien mit der Endung .csv und .txt mit dem Modul glob einlesen. Wie könnte der Dateipfad im Argument 'pathname' beschrieben werden, um alle Dateien einzulesen `glob.glob(pathname = ordnerpfad + '...')`?\n\n6. Maske aufheben: Das maskierte Array m_daten soll demaskiert werden. Jedoch bleibt der Befehl `m_daten.mask = ma.nomask` ohne Erfolg. Was könnte die Ursache sein?\n\n\n::: {.cell execution_count=190}\n``` {.python .cell-code}\nprint(m_daten, \"\\n\")\n\nm_daten.mask = ma.nomask\nprint(m_daten, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1 2 -- 4] \n\n[1 2 -- 4] \n\n```\n:::\n:::\n\n\n7. Welche Aussagen treffen zu?\n\n    A) HDF5 und netCDF4 sind speziell für den Umgang mit großen, komplexen wissenschaftlichen Datensätzen optimiert und unterstützen mehrdimensionale Arrays.\n\n    B) HDF5 ist eine hierarchische Datenstruktur, deren Aufbau der Verzeichnisstruktur eines Computers gleicht. NetCDF4 ist dagegen eine flache Datenstruktur und unterstützt verzeichnisartige Datenstrukturen nicht.\n\n    C) Beide Formate, HDF5 und netCDF4, können ausschließlich ein- und zweidimensionale Datensätze speichern.\n\n    D) Mit HDF5 und netCDF4 können umfangreiche Metadaten hinterlegt werden. Die Formate gelten deshalb als selbstbeschreibend.\n\n\n\n::: {.callout-tip collapse=\"true\"}\n## Lösungen\n\nAufgabe 1: pd.read_csv()\n\n::: {.cell execution_count=191}\n``` {.python .cell-code}\ndateipfad = '01-daten/quiz-aufgabe1.csv'\naufgabe1 = pd.read_csv(filepath_or_buffer = dateipfad, sep = ';', parse_dates = ['Geburtsdatum'], dtype = {'Name': 'string'})\nprint(aufgabe1.info())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4 entries, 0 to 3\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype         \n---  ------        --------------  -----         \n 0   ID            4 non-null      int64         \n 1   Name          4 non-null      string        \n 2   Geburtsdatum  4 non-null      datetime64[ns]\n 3   Gehalt        4 non-null      int64         \ndtypes: datetime64[ns](1), int64(2), string(1)\nmemory usage: 260.0 bytes\nNone\n```\n:::\n:::\n\n\nAufgabe 2: Beobachtungen als ungültig markieren\n\n::: {.cell execution_count=192}\n``` {.python .cell-code}\nbedingung = aufgabe1['Geburtsdatum'].dt.year < 1980\nprint(bedingung, \"\\n\")\n\n# pd.NA für Zeichenketten und Ganzzahlen\n## NumPy-Datentyp int64 führt zur Umwandlung des Datentyps der Spalte Gehalt\naufgabe2 = aufgabe1.copy()\naufgabe2.loc[bedingung, :] = pd.NA\nprint(\"NumPy-Datentyp int64 führt zur Umwandlung des Datentyps der Spalte Gehalt\")\nprint(aufgabe2, \"\\n\")\n# 3  NaN   <NA>          NaT      NaN\n\n## Pandas-Datentyp Int64 unterstützt fehlende Werte für Ganzzahlen\naufgabe2 = aufgabe1.copy()\naufgabe2['Gehalt'] = aufgabe2['Gehalt'].astype('Int64')\naufgabe2['ID'] = aufgabe2['ID'].astype('Int64')\naufgabe2.loc[bedingung, :] = pd.NA\nprint(\"Pandas-Datentyp Int64 unterstützt fehlende Werte für Ganzzahlen\")\nprint(aufgabe2)\n# 3  NaN   <NA>          NaT    <NA>\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0    False\n1    False\n2    False\n3     True\nName: Geburtsdatum, dtype: bool \n\nNumPy-Datentyp int64 führt zur Umwandlung des Datentyps der Spalte Gehalt\n    ID   Name Geburtsdatum   Gehalt\n0  1.0   Anna   1986-04-12  55000.0\n1  2.0  Bernd   1990-05-23  62000.0\n2  3.0  Carla   1982-11-30  71000.0\n3  NaN   <NA>          NaT      NaN \n\nPandas-Datentyp Int64 unterstützt fehlende Werte für Ganzzahlen\n     ID   Name Geburtsdatum  Gehalt\n0     1   Anna   1986-04-12   55000\n1     2  Bernd   1990-05-23   62000\n2     3  Carla   1982-11-30   71000\n3  <NA>   <NA>          NaT    <NA>\n```\n:::\n:::\n\n\nAufgabe 3: datetime aufsteigend sortieren\n\n::: {.cell execution_count=193}\n``` {.python .cell-code}\naufgabe3 = aufgabe2.copy()\nprint(aufgabe3['Geburtsdatum'].sort_values(ascending = False), \"\\n\")\n\n# Sortieren des DataFrames\nprint(aufgabe3.sort_values(by = 'Geburtsdatum', ascending = False, inplace = True), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1   1990-05-23\n0   1986-04-12\n2   1982-11-30\n3          NaT\nName: Geburtsdatum, dtype: datetime64[ns] \n\nNone \n\n```\n:::\n:::\n\n\nAufgabe 4: Welches Datum ist größer und warum?\n\nPython zählt die Zeit ausgehend von der sogenannten Epoche `pd.to_datetime(0)`. Jüngere Menschen wurden in größerem Abstand zur Epoche geboren.\n\n::: {.cell execution_count=194}\n``` {.python .cell-code}\naufgabe4 = pd.DataFrame({'Geburtsdatum': aufgabe3['Geburtsdatum'].sort_values(ascending = False)})\naufgabe4['timedelta zur Epoche'] = aufgabe4['Geburtsdatum'] - pd.to_datetime(0)\n\nprint(aufgabe4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Geburtsdatum timedelta zur Epoche\n1   1990-05-23            7447 days\n0   1986-04-12            5945 days\n2   1982-11-30            4716 days\n3          NaT                  NaT\n```\n:::\n:::\n\n\nAufgabe 5: glob\n\n`glob.glob(pathname = 'ordnerpfad' + '?????[1-2].*')` oder  \n`glob.glob(pathname = 'ordnerpfad' + 'Datei[1-2].*')` oder  \n`glob.glob(pathname = 'ordnerpfad' + '*[1-2].*')`\n\nAufgabe 6: masked Array\n\nDas maskierte Array hat eine hard mask.\n\n::: {.cell execution_count=195}\n``` {.python .cell-code}\nm_daten.soften_mask()\nm_daten.mask = ma.nomask\nprint(m_daten, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1 2 3 4] \n\n```\n:::\n:::\n\n\nAufgabe 7: HDF5 und netCDF4\n\nRichtige Antworten: A) und D)\n\n::: \n\n## Übungsaufgaben\n\n#### Ein sehr unordentlicher Datensatz\nDas Statistische Amt der Europäischen Union Eurostat führt die amtlichen europäischen Statistiken. In der [bei Eurostatt verfügbaren](https://ec.europa.eu/eurostat/statistics-explained/index.php?title=Natural_gas_price_statistics#Natural_gas_prices_for_non-household_consumers) Datei ([Direktlink auf XLSX-Datei](https://ec.europa.eu/eurostat/statistics-explained/images/3/32/SE_figures_for_Gas_2023S2_v3.xlsx)) sind die Preise für Erdgas für die Mitgliedstaaten und verschiedene mit der Europäischen Union assoziierte Staaten erfasst. Lesen Sie das Tabellenblatt 'Table 1' entweder so ein, dass \n  \n  - alle Werte mit einem sinnvollen numerischen Datentyp eingelesen werden.\n  \n  - ENTWEDER in einer zweidimensionalen Struktur jede Zeile eine Beobachtung und jede Beobachtung eine Zeile sowie jede Spalte eine Variable und jede Variable eine Spalte ist.  \n\n  - ODER wählen Sie eine sinnvolle mehrdimensionale Struktur.\n\n::: {style=\"font-size: 90%;\"}\n| Daten | Dateiname |\n|---|---|\n| Preise für Erdgas | \"01-daten/SE_figures_for_Gas_2023S2_v3.xlsx\" |\n:::\n\n#### Ein schwieriges Format\nDas statistische Bundesamt veröffentlicht eine Statistik der erteilten Baugenehmigungen, die zum Zeitpunkt der Erstellung diese Bausteins für die Jahre 2015 bis 2023 vorliegt. Die Daten können hier abgerufen werden: <https://www-genesis.destatis.de/datenbank/online/table/31111-0006/sequenz=tabelleErgebnis&selectionname=31111-0006&zeitscheiben=1>.\n\nLaden Sie mindestens zwei Jahrgänge herunter und lesen Sie die Dateien in einem gemeinsamen Datensatz ein. Ordnen Sie jedem Wert das Jahr der Beobachtung zu, indem Sie eine Variable 'Jahr' einfügen und den Datensatz entsprechend strukturieren.\n\n  - Wie können Sie das Jahr aus dem Datensatz auslesen?\n\n  - Liegen alle Variablen in Spalten vor?\n\nDem Skript liegen die Dateien in folgendem Pfad bei:\n\n::: {style=\"font-size: 90%;\"}\n| Daten | Ordnerpfad |\n|---|---|\n| Baugenehmigungen verschiedener Jahre | \"01-daten/baugenehmigungen\" |\n:::\n\n",
    "supporting": [
      "einlesen-strukturierter-datensaetze_files/figure-pdf"
    ],
    "filters": []
  }
}