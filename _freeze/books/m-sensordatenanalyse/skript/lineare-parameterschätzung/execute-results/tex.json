{
  "hash": "ad8fca8559d97b73483d23a46e9d6334",
  "result": {
    "engine": "jupyter",
    "markdown": "# Lineare Parameterschätzung {#sec-hooke}\nViele physikalische Größen werden indirekt gemessen. Häufig liegt dabei ein linearer Zusammenhang zwischen der gemessenen und der gesuchten Größe vor. In der Praxis sind die Daten einer Messreihe nie exakt und mit mehr oder weniger großen Abweichungen vom wahren Wert der gesuchten Größe behaftet. Die lineare Parameterschätzung ist ein Verfahren, um aus einer Menge von Daten denjenigen Wert zu bestimmen, der die Abweichungen der einzelnen Messwerte minimiert.\n\n## Messreihe Hooke'sches Gesetz\nDas [Hooke'sche Gesetz](https://de.wikipedia.org/wiki/Hookesches_Gesetz), benannt nach dem englischen Wissenschaftler Robert Hooke, beschreibt die Beziehung zwischen der Kraft $F$ und der Längenänderung $\\Delta{x}$ einer Feder durch die Gleichung $F = k \\cdot \\Delta{x}$, wobei $k$ die Federkonstante ist.\n\nDie Federkonstante ist eine grundlegende Eigenschaft elastischer Materialien und gibt an, wie viel Kraft erforderlich ist, um eine Feder um eine bestimmte Länge zu dehnen oder zu komprimieren. Das Hooke'sche Gesetz besagt, dass die Deformation eines elastischen Körpers proportional zur aufgebrachten Kraft ist, solange die Feder nicht über den elastischen Bereich hinaus gedehnt oder gestaucht wird.\n\nIn einem Experiment wurde das Hooke'sche Gesetz überprüft. An einer an einer Halterung hängenden Metallfeder ist ein (variables) Gewicht angebracht. Darunter befindet sich in einigem Abstand ein Ultraschallsensor zur Abstandsmessung. Der Abstand zwischen der Unterseite des an der Feder befestigten Gewichts und dem Ultraschallsensor ist der gemessene Abstand.\n\nDie Gewichte konnten mit einer Genauigkeit von $\\epsilon_{m} = 0,5 g$ mit einer Küchenwaage bestimmt werden.\n\n\n\n![Versuchsaufbau](00-bilder/aufbau.png){fig-alt=\"Darstellung des Versuchsaufbaus.\"}\n\nDie Messreihe liegt in Form einer CSV-Datei unter dem Pfad '01-daten/hooke_data.csv' vor. Die Datei wird mit Pandas eingelesen.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndateipfad = \"01-daten/hooke_data.csv\"\nhooke = pd.read_csv(filepath_or_buffer = dateipfad, sep = ';')\n```\n:::\n\n\n### Deskriptive Statistik\nNach dem Einlesen sollte man sich einen Überblick über die Daten verschaffen. Mit den Methoden `pd.DataFrame.head()` und `pd.DataFrame.tail()` kann ein Ausschnitt vom Beginn und vom Ende der Daten betrachtet werden. \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nprint(hooke.head(), \"\\n\")\nprint(hooke.tail())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   no  mass  distance\n0   0   705    153.29\n1   1   705    152.74\n2   2   705    153.27\n3   3   705    152.81\n4   4   705    152.77 \n\n      no  mass  distance\n109  109     0    173.70\n110  110     0    173.44\n111  111     0    173.75\n112  112     0    173.30\n113  113     0    200.00\n```\n:::\n:::\n\n\nDie Methode `pd.DataFrame.describe()` erstellt die deskriptive Statistik für den Datensatz. Diese ist in diesem Fall jedoch noch nicht sonderlich nützlich. Die Spalte 'no' enthält lediglich eine laufende Versuchsnummer, die Spalte 'mass' enhält verschiedene Gewichte.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nhooke.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>no</th>\n      <th>mass</th>\n      <th>distance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>114.000000</td>\n      <td>114.000000</td>\n      <td>114.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>56.561404</td>\n      <td>394.921053</td>\n      <td>162.301754</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>33.131552</td>\n      <td>226.237605</td>\n      <td>7.483767</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>152.740000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>28.250000</td>\n      <td>201.000000</td>\n      <td>156.622500</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>56.500000</td>\n      <td>452.000000</td>\n      <td>160.720000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>84.750000</td>\n      <td>605.000000</td>\n      <td>167.767500</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>113.000000</td>\n      <td>705.000000</td>\n      <td>200.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n&nbsp;\n\nSinnvoller ist eine nach dem verwendeten Gewicht aufgeteilte beschreibende Statistik der gemessenen Ausdehnung. Dafür kann die Pandas-Methode `pd.DataFrame.groupby()` verwendet werden. So kann für jedes der gemessenen Gewichte der arithmethische Mittelwert und die Standardabweichung abgelesen werden.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nhooke.groupby(by = 'mass')['distance'].describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n    <tr>\n      <th>mass</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>12.0</td>\n      <td>175.828333</td>\n      <td>7.620157</td>\n      <td>173.27</td>\n      <td>173.3150</td>\n      <td>173.570</td>\n      <td>174.1125</td>\n      <td>200.00</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>11.0</td>\n      <td>171.044545</td>\n      <td>0.985833</td>\n      <td>170.15</td>\n      <td>170.3650</td>\n      <td>170.800</td>\n      <td>171.2400</td>\n      <td>173.56</td>\n    </tr>\n    <tr>\n      <th>201</th>\n      <td>11.0</td>\n      <td>167.791818</td>\n      <td>0.296305</td>\n      <td>167.26</td>\n      <td>167.7200</td>\n      <td>167.780</td>\n      <td>167.9750</td>\n      <td>168.19</td>\n    </tr>\n    <tr>\n      <th>301</th>\n      <td>10.0</td>\n      <td>163.710000</td>\n      <td>1.660977</td>\n      <td>161.60</td>\n      <td>162.0575</td>\n      <td>163.825</td>\n      <td>165.3250</td>\n      <td>165.86</td>\n    </tr>\n    <tr>\n      <th>401</th>\n      <td>10.0</td>\n      <td>161.967000</td>\n      <td>0.313229</td>\n      <td>161.42</td>\n      <td>161.8450</td>\n      <td>161.915</td>\n      <td>162.0250</td>\n      <td>162.48</td>\n    </tr>\n    <tr>\n      <th>452</th>\n      <td>10.0</td>\n      <td>160.713000</td>\n      <td>0.627854</td>\n      <td>159.98</td>\n      <td>160.4575</td>\n      <td>160.555</td>\n      <td>160.7400</td>\n      <td>161.83</td>\n    </tr>\n    <tr>\n      <th>503</th>\n      <td>10.0</td>\n      <td>159.314000</td>\n      <td>0.781099</td>\n      <td>158.43</td>\n      <td>158.6400</td>\n      <td>159.220</td>\n      <td>159.9650</td>\n      <td>160.61</td>\n    </tr>\n    <tr>\n      <th>554</th>\n      <td>10.0</td>\n      <td>157.547000</td>\n      <td>0.523791</td>\n      <td>156.92</td>\n      <td>157.2075</td>\n      <td>157.435</td>\n      <td>157.7100</td>\n      <td>158.60</td>\n    </tr>\n    <tr>\n      <th>605</th>\n      <td>10.0</td>\n      <td>156.142000</td>\n      <td>0.354206</td>\n      <td>155.62</td>\n      <td>156.0700</td>\n      <td>156.080</td>\n      <td>156.2075</td>\n      <td>156.84</td>\n    </tr>\n    <tr>\n      <th>655</th>\n      <td>11.0</td>\n      <td>154.022727</td>\n      <td>0.224414</td>\n      <td>153.72</td>\n      <td>153.8800</td>\n      <td>153.920</td>\n      <td>154.2400</td>\n      <td>154.35</td>\n    </tr>\n    <tr>\n      <th>705</th>\n      <td>9.0</td>\n      <td>153.008889</td>\n      <td>0.241425</td>\n      <td>152.74</td>\n      <td>152.8100</td>\n      <td>152.910</td>\n      <td>153.2700</td>\n      <td>153.29</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n&nbsp;\n\nBereits an dieser Stelle könnte die hohe Standardabweichung in der Messreihe mit 0 Gramm auffallen. Leichter ist es jedoch in der grafischen Betrachtung.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nhooke.plot(x = 'mass', y = 'distance', kind = 'scatter', title = \"Messreihe Hooke`sches Gesetz\", ylabel = 'Abstand in cm', xlabel = 'Gewicht in Gramm', label = 'gemessener Abstand')\nplt.legend()\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](lineare-parameterschätzung_files/figure-pdf/cell-7-output-1.png){fig-alt='Darstellung des auf der x-Achse aufgetragenen Gewichts und des auf der y-Achse aufgetragenen gemessenen Abstands.' fig-pos='H'}\n:::\n:::\n\n\n&nbsp;\n\nGrafisch fällt der Messwert von 200 cm für das Gewicht 0 Gramm als stark von den übrigen Messwerten abweichend auf.\n\nDie Messwerte für das Gewicht 0 Gramm sollen näher betrachtet werden. Dafür werden die Messwerte sowohl absolut, als auch [standardisiert in Einheiten der Standardabweichung (z-Werten)](https://de.wikipedia.org/wiki/Standardisierung_(Statistik)) ausgedrückt ausgegeben.\n\nEine Variable wird standardisiert, indem von jedem Wert der Erwartungswert abgezogen und das Ergebnis durch die Standardabweichung geteilt wird.\n\n$$\nZ = \\frac{x - \\mu}{\\sigma}\n$$\n\nDa in der Regel der Erwartungswert und die Standardabweichung unbekannt sind, werden der Stichprobenmittelwert und die Stichprobenstandardabweichung verwendet. Dies nennt man *studentisieren*, nach dem Pseudonym des bereits im vorherigen Kapitel erwähnten William Sealy Gosset.\n\n$$\nz_{i} = \\frac{x_{i} - \\bar{x}}{s}\n$$\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ngewicht = 0\n\n# z-Transformation manuell berechnen\nmittelwert_ausdehnung = hooke[hooke['mass'] == gewicht].loc[: , 'distance'].mean()\nstandardabweichung_ausdehnung = hooke[hooke['mass'] == gewicht].loc[: , 'distance'].std(ddof = 1)\n\nz_values = hooke[hooke['mass'] == gewicht].loc[: , 'distance'].apply(lambda x: ( (x - mittelwert_ausdehnung) / standardabweichung_ausdehnung))\nz_values.name = 'z-values'\n\n# z-Transformation mit scipy\n## scipy gibt ein np.array zurück\n## zur besseren Darstellung wird das np.array in eine pd.Series umgewandelt, die ein name Attribut hat\nscipy_z_values = pd.Series(scipy.stats.zscore(hooke[hooke['mass'] == gewicht].loc[: , 'distance'], ddof = 1))\nscipy_z_values.name = 'scipy z-values'\n\n# gemeinsame Ausgabe der Daten\nprint(pd.concat([hooke[hooke['mass'] == gewicht], z_values, scipy_z_values], axis = 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        no  mass  distance  z-values  scipy z-values\n102  102.0   0.0    173.32 -0.329171             NaN\n103  103.0   0.0    174.11 -0.225498             NaN\n104  104.0   0.0    173.42 -0.316048             NaN\n105  105.0   0.0    174.12 -0.224186             NaN\n106  106.0   0.0    173.30 -0.331795             NaN\n107  107.0   0.0    174.21 -0.212375             NaN\n108  108.0   0.0    173.27 -0.335732             NaN\n109  109.0   0.0    173.70 -0.279303             NaN\n110  110.0   0.0    173.44 -0.313423             NaN\n111  111.0   0.0    173.75 -0.272742             NaN\n112  112.0   0.0    173.30 -0.331795             NaN\n113  113.0   0.0    200.00  3.172069             NaN\n0      NaN   NaN       NaN       NaN       -0.329171\n1      NaN   NaN       NaN       NaN       -0.225498\n2      NaN   NaN       NaN       NaN       -0.316048\n3      NaN   NaN       NaN       NaN       -0.224186\n4      NaN   NaN       NaN       NaN       -0.331795\n5      NaN   NaN       NaN       NaN       -0.212375\n6      NaN   NaN       NaN       NaN       -0.335732\n7      NaN   NaN       NaN       NaN       -0.279303\n8      NaN   NaN       NaN       NaN       -0.313423\n9      NaN   NaN       NaN       NaN       -0.272742\n10     NaN   NaN       NaN       NaN       -0.331795\n11     NaN   NaN       NaN       NaN        3.172069\n```\n:::\n:::\n\n\nDer Wert 200 cm in Zeile 113 scheint fehlerhaft zu sein. Eine Eigendehnung der Feder um zusätzliche 16 Zentimeter ist nicht plausibel. Auch der z-Wert > 3 kennzeichnet den Messwert als [Ausreißer](https://de.wikipedia.org/wiki/Ausrei%C3%9Fer). Die Zeile wird deshalb aus dem Datensatz entfernt.\n\n::: {#imp-ausreißer .callout-important collapse=\"false\"}\n## Ausreißer\n\nIn der Statistik wird ein Messwert als Ausreißer bezeichnet, wenn dieser stark von der übrigen Messreihe abweicht. In einer Messreihe können auch mehrere Ausreißer auftreten. Diese Werte können zur Verbesserung der Schätzung aus der Messreihe entfernt werden, wenn anzunehmen ist, dass diese durch Messfehler und andere Störgrößen verursacht sind. \n\nEine Möglichkeit, Ausreißer zu identifizieren, ist die z-Transformation. Dabei muss ein Schwellenwert gewählt werden, ab dem ein Messwert als Ausreißer klassifiziert werden soll, bspw. 2,5 oder 3 Einheiten der Standardabweichung. In der Statistik wurde eine ganze Reihe von Ausreißertests entwickelt (siehe [Ausreißertests](https://de.wikipedia.org/wiki/Ausrei%C3%9Fer#Ausrei%C3%9Fertests))\n\nDie Einstufung eines Messwerts als Ausreißer kann aber nicht allein auf der Grundlage statistischer Verfahren erfolgen, sondern ist immer eine Ermessensentscheidung auf der Grundlage Ihres Fachwissens. Denn nicht alle abweichenden Werte sind automatisch ungültig, sondern treten mit einer gewissen statistischen Wahrscheinlichkeit auf (siehe Kapitel Normalverteilung). Man spricht dann von gültigen Extremwerten.\n\nAusreißer von verschiedenen [Autor:innen](https://xtools.wmcloud.org/authorship/de.wikipedia.org/Ausrei%C3%9Fer?uselang=de) steht unter der Lizenz [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/deed.de) und ist abrufbar auf [Wikipedia](https://de.wikipedia.org/wiki/Ausrei%C3%9Fer)\n:::\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nhooke.drop(index = 113, inplace = True)\n\nhooke.groupby(by = 'mass')['distance'].describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n    <tr>\n      <th>mass</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11.0</td>\n      <td>173.630909</td>\n      <td>0.367409</td>\n      <td>173.27</td>\n      <td>173.3100</td>\n      <td>173.440</td>\n      <td>173.9300</td>\n      <td>174.21</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>11.0</td>\n      <td>171.044545</td>\n      <td>0.985833</td>\n      <td>170.15</td>\n      <td>170.3650</td>\n      <td>170.800</td>\n      <td>171.2400</td>\n      <td>173.56</td>\n    </tr>\n    <tr>\n      <th>201</th>\n      <td>11.0</td>\n      <td>167.791818</td>\n      <td>0.296305</td>\n      <td>167.26</td>\n      <td>167.7200</td>\n      <td>167.780</td>\n      <td>167.9750</td>\n      <td>168.19</td>\n    </tr>\n    <tr>\n      <th>301</th>\n      <td>10.0</td>\n      <td>163.710000</td>\n      <td>1.660977</td>\n      <td>161.60</td>\n      <td>162.0575</td>\n      <td>163.825</td>\n      <td>165.3250</td>\n      <td>165.86</td>\n    </tr>\n    <tr>\n      <th>401</th>\n      <td>10.0</td>\n      <td>161.967000</td>\n      <td>0.313229</td>\n      <td>161.42</td>\n      <td>161.8450</td>\n      <td>161.915</td>\n      <td>162.0250</td>\n      <td>162.48</td>\n    </tr>\n    <tr>\n      <th>452</th>\n      <td>10.0</td>\n      <td>160.713000</td>\n      <td>0.627854</td>\n      <td>159.98</td>\n      <td>160.4575</td>\n      <td>160.555</td>\n      <td>160.7400</td>\n      <td>161.83</td>\n    </tr>\n    <tr>\n      <th>503</th>\n      <td>10.0</td>\n      <td>159.314000</td>\n      <td>0.781099</td>\n      <td>158.43</td>\n      <td>158.6400</td>\n      <td>159.220</td>\n      <td>159.9650</td>\n      <td>160.61</td>\n    </tr>\n    <tr>\n      <th>554</th>\n      <td>10.0</td>\n      <td>157.547000</td>\n      <td>0.523791</td>\n      <td>156.92</td>\n      <td>157.2075</td>\n      <td>157.435</td>\n      <td>157.7100</td>\n      <td>158.60</td>\n    </tr>\n    <tr>\n      <th>605</th>\n      <td>10.0</td>\n      <td>156.142000</td>\n      <td>0.354206</td>\n      <td>155.62</td>\n      <td>156.0700</td>\n      <td>156.080</td>\n      <td>156.2075</td>\n      <td>156.84</td>\n    </tr>\n    <tr>\n      <th>655</th>\n      <td>11.0</td>\n      <td>154.022727</td>\n      <td>0.224414</td>\n      <td>153.72</td>\n      <td>153.8800</td>\n      <td>153.920</td>\n      <td>154.2400</td>\n      <td>154.35</td>\n    </tr>\n    <tr>\n      <th>705</th>\n      <td>9.0</td>\n      <td>153.008889</td>\n      <td>0.241425</td>\n      <td>152.74</td>\n      <td>152.8100</td>\n      <td>152.910</td>\n      <td>153.2700</td>\n      <td>153.29</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n&nbsp;\n\nHiernach ist die höchste Standardabweichung für die Messreihe mit 301 Gramm zu verzeichnen. Die gemessenen Werte sind jedoch unauffällig.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ngewicht = 301\n\n## scipy gibt ein np.array zurück\n## zur besseren Darstellung wird das np.array in eine pd.Series umgewandelt, die ein name Attribut hat\nz_values = pd.Series(scipy.stats.zscore(hooke[hooke['mass'] == gewicht].loc[: , 'distance'], ddof = 1))\nz_values.name = 'z-values'\n\nprint(pd.concat([hooke[hooke['mass'] == gewicht], z_values], axis = 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      no   mass  distance  z-values\n70  70.0  301.0    162.38       NaN\n71  71.0  301.0    161.93       NaN\n72  72.0  301.0    161.95       NaN\n73  73.0  301.0    161.60       NaN\n74  74.0  301.0    164.59       NaN\n75  75.0  301.0    165.86       NaN\n76  76.0  301.0    163.82       NaN\n77  77.0  301.0    163.83       NaN\n78  78.0  301.0    165.57       NaN\n79  79.0  301.0    165.57       NaN\n0    NaN    NaN       NaN -0.800734\n1    NaN    NaN       NaN -1.071658\n2    NaN    NaN       NaN -1.059617\n3    NaN    NaN       NaN -1.270337\n4    NaN    NaN       NaN  0.529809\n5    NaN    NaN       NaN  1.294419\n6    NaN    NaN       NaN  0.066226\n7    NaN    NaN       NaN  0.072247\n8    NaN    NaN       NaN  1.119823\n9    NaN    NaN       NaN  1.119823\n```\n:::\n:::\n\n\n### Explorative Statistik\nDie Grafik des bereinigten Datensatzes legt einen linearen Zusammenhang nahe. Darüber hinaus sticht der mit zunehmendem Gewicht abfallende Trend der Datenpunkte ins Auge.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nhooke.plot(x = 'mass', y = 'distance', kind = 'scatter', title = 'bereinigter Datensatz', ylabel = 'Abstand in cm', xlabel = 'Gewicht in Gramm', label = 'gemessener Abstand')\nplt.legend()\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](lineare-parameterschätzung_files/figure-pdf/cell-11-output-1.png){fig-alt='Darstellung des auf der x-Achse aufgetragenen Gewichts und des auf der y-Achse aufgetragenen gemessenen Abstands.' fig-pos='H'}\n:::\n:::\n\n\n&nbsp;\n\nEntsprechend des Versuchsaufbaus nimmt mit zunehmender Dehnung der Feder der Abstand zum Abstandssensor ab. Da die Federausdehnung gemessen werden soll, bietet es sich an, die Daten entsprechend zu transformieren. Dazu wird der gemessene Abstand bei 0 Gramm Gewicht als Nullpunkt aufgefasst, von dem aus die Federdehnung gemessen wird. Das bedeutet, dass von allen Datenpunkten das arithmetische Mittel der für 0 Gramm Gewicht gemessen Ausdehnung abgezogen und das Ergebnis mit -1 multipliziert wird.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nnullpunkt = hooke[hooke['mass'] == 0].loc[: , 'distance'].mean()\nprint(f\"Nullpunkt: {nullpunkt:.2f} cm\")\n\nhooke['distance'] = hooke['distance'].sub(nullpunkt).mul(-1)\n\nhooke.plot(x = 'mass', y = 'distance', kind = 'scatter', title = 'bereinigter und invertierter Datensatz', ylabel = 'Federausdehnung in cm', xlabel = 'Gewicht in Gramm', label = 'gemessener Abstand')\nplt.legend()\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNullpunkt: 173.63 cm\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](lineare-parameterschätzung_files/figure-pdf/cell-12-output-2.png){fig-alt='Darstellung des auf der x-Achse aufgetragenen Gewichts und des auf der y-Achse aufgetragenen Federausdehnung.' fig-pos='H'}\n:::\n:::\n\n\n&nbsp;\n\nMit der Funktion `plt.errorbars()` können die Mittelwerte und Standardfehler für jedes Gewicht grafisch dargestellt werden. Da die Standardfehler eher klein sind, werden mit dem Parameter `capsize` horizontale Linien am Ende des Fehlerbalkens eingezeichnet.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# Mittelwerte nach Gewicht\ndistance_means_by_weight = hooke['distance'].groupby(by = hooke['mass']).mean()\ndistance_means_by_weight.name = 'Federausdehnung'\n\n# Standardfehler nach Gewicht\ndistance_stderrors_by_weight = hooke['distance'].groupby(by = hooke['mass']).std(ddof = 1).div(np.sqrt(hooke['distance'].groupby(by = hooke['mass']).size()))\ndistance_stderrors_by_weight.name = 'Standardfehler'\n\ndatenpunkte = hooke.plot(x = 'mass', y = 'distance', kind = 'scatter', title = 'bereinigter und invertierter Datensatz', ylabel = 'Federausdehnung in cm', xlabel = 'Gewicht in Gramm', alpha = 0.6, label = 'gemessene Federausdehnung')\n\n# Legendeneinträge abgreifen\n# siehe: https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.get_legend_handles_labels.html\ndatenpunkte_handle, datenpunkte_label = datenpunkte.get_legend_handles_labels()\n\nerrorbar_container = plt.errorbar(\n  x = distance_means_by_weight.index, y = distance_means_by_weight, yerr = distance_stderrors_by_weight,\n  linestyle = 'none', marker = 'x', color = 'black', markersize = 12, elinewidth = 3, ecolor = 'red', capsize = 12)\n\n# Legende manuell erstellen\n# siehe: https://matplotlib.org/stable/api/container_api.html#matplotlib.container.ErrorbarContainer\nplt.legend([datenpunkte_handle[0], errorbar_container.lines[0], errorbar_container.lines[2][0]],\n           [datenpunkte_label[0], 'Mittelwert', 'Standardfehler'],\n           loc = 'upper left')\nplt.show()\n\nprint(\"\\n\", pd.concat([distance_means_by_weight, distance_stderrors_by_weight], axis = 1))\n```\n\n::: {.cell-output .cell-output-display}\n![](lineare-parameterschätzung_files/figure-pdf/cell-13-output-1.png){fig-alt='Auf der x-Achse ist das Gewicht in Gramm, auf der y-Achse die Federausdehnung in cm abgetragen. Zusätzlich zu den Messpunkten sind für jedes Gewicht der Mittelwert mit einem großen X und von dessen Mittelpunkt ausgehend der Bereich des Mittelwerts ± 1 Standardfehler eingezeichnet.' fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n       Federausdehnung  Standardfehler\nmass                                 \n0       -7.751375e-15        0.110778\n100      2.586364e+00        0.297240\n201      5.839091e+00        0.089339\n301      9.920909e+00        0.525247\n401      1.166391e+01        0.099052\n452      1.291791e+01        0.198545\n503      1.431691e+01        0.247005\n554      1.608391e+01        0.165637\n605      1.748891e+01        0.112010\n655      1.960818e+01        0.067663\n705      2.062202e+01        0.080475\n```\n:::\n:::\n\n\n## Federkonstante bestimmen\nDie Beziehung zwischen der Kraft $F$ und der Längenänderung $\\Delta{x}$ einer Feder mit Federkonstante $k$ wird durch die Gleichung $F = k \\cdot \\Delta{x}$ beschrieben. Dabei entspricht die Kraft $F$ dem mit der Fallbeschleunigung $g$  multiplizierten Gewicht in Kilogramm $m$. Die Fallbeschleunigung beträgt auf der Erde $9,81 \\frac{m}{s^2}$.\n\nDeshalb wird im Datensatz das in der Spalte 'mass' eingetragene Gewicht in Gramm in die wirkende Kraft umgerechnet. Ebenso wird die gemessene Abstandsänderung in der Spalte 'distance' von Zentimeter in Meter umgerechnet.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nhooke['mass'] = hooke['mass'].div(1000).mul(9.81)\nhooke.rename(columns = {'mass': 'force'}, inplace = True)\n\nhooke['distance'] = hooke['distance'].div(100)\n\nprint(hooke.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   no    force  distance\n0   0  6.91605  0.203409\n1   1  6.91605  0.208909\n2   2  6.91605  0.203609\n3   3  6.91605  0.208209\n4   4  6.91605  0.208609\n```\n:::\n:::\n\n\nFür die grafische Darstellung des Zusammenhangs $F = k \\cdot \\Delta{x}$ ist es zweckmäßiger, die Abstandsänderung auf der x-Achse und die wirkende Kraft auf der y-Achse darzustellen.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nhooke.plot(x = 'distance', y = 'force', kind = 'scatter', title = 'umgeformter Datensatz', ylabel = 'wirkende Kraft in $N$', xlabel = 'Abstandsänderung in Meter', label = 'gemessene Abstandsänderung')\nplt.legend()\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](lineare-parameterschätzung_files/figure-pdf/cell-15-output-1.png){fig-alt='Darstellung der auf der x-Achse aufgetragenen Federausdehnung und der auf der y-Achse aufgetragenen wirkenden Kraft.' fig-pos='H'}\n:::\n:::\n\n\n### Lineare Ausgleichsrechnung\nDie Ausgleichsrechnung (oder auch Parameterschätzung) ist eine Methode, um für eine Messreihe die unbekannten Parameter des zugrundeliegenden physikalischen Modells zu schätzen. Das Ziel besteht darin, eine (in diesem Fall lineare) Funktion zu bestimmen, die bestmöglich an die Messdaten angepasst ist. ([Wikipedia](https://de.wikipedia.org/wiki/Ausgleichungsrechnung))\n\nEine lineare Funktion wird durch die Konstante $\\beta_0$, den Schnittpunkt mit der y-Achse, und den Steigungskoeffizienten $\\beta_1$ bestimmt.\n\n$$\ny = \\beta_0 + \\beta_1 \\cdot x\n$$\n\nIn der Regel liegt kein deterministischer Zusammenhang vor, sondern es treten zufällige Abweichungen auf, die mit dem additiven Fehlerterm ausgedrückt und aus dem Englischen error mit $e_i$ notiert werden. Diese Fehler werden Residuen genannt.\n\n$$\ny = \\beta_0 + \\beta_1 \\cdot x + e_i\n$$\n\nZur Bestimmung der Parameter einer linearen Funktion wird die Methode der [kleinsten Quadrate](https://de.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate) verwendet.\n\n::: {#nte-ols .callout-note collapse=\"true\"}\n## Methode der kleinsten Quadrate\n\nMit der Methode der kleinsten Quadrate soll diejenige Gerade $\\hat{y} = \\beta_0 + \\beta_1 \\cdot x$ gefunden werden, die die quadrierten Abstände der  Vorhersagewerte $\\hat{y}$ von den tatsächlich gemessenen Werten $y$ minimiert. Die Werte $y_i - \\hat{y_i}$ sind die Residuen $e_i$. Es gilt also:\n\n$$\n\\sum_{i=1}^{N}(y_i - \\hat{y_i})^2 = \\sum_{i=1}^{N} e_i = \\min\n$$\n\nGrafisch kann man sich die Minimierung der quadrierten Abstände so vorstellen.\n\n:::: {.panel-tabset}\n## Grafik\n\n::: {.cell execution_count=15}\n\n::: {.cell-output .cell-output-display}\n![](lineare-parameterschätzung_files/figure-pdf/cell-16-output-1.png){fig-alt='Auf einer Geraden sind 9 Punkte markiert, die die Vorhersagewerte des linearen Modells repräsentieren. Vertikal darüber oder darunter sind die Messwerte eingezeichnet. Jeder Vorhersagewerte ist durch eine gestrichelte, vertikale Linie mit einem der Messwerte verbunden.'}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nRegressionskoeffizienten: [ 2.93333333 -0.73333333]\n```\n:::\n:::\n\n\n## Code\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nx = np.arange(1, 11)\ny = - x.copy() + 4\ny[0] -= 2\ny[2] -= 2\ny[3] += 3\ny[-3] += 5\n\nlm = poly.polyfit(x, y, 1)\nvorhersagewerte = poly.polyval(x, lm)\n\nplt.scatter(x, vorhersagewerte, label = 'Vorhersagewerte', marker = \"^\", color = \"tab:blue\")\nplt.scatter(x, y, label = 'Messwerte', marker = 'o', color = \"tab:orange\")\nplt.axline(xy1 = (0, lm[0]), slope = lm[1], label = \"Regressionsgerade\", color = \"tab:blue\")\ndotted = plt.vlines(x, ymin = vorhersagewerte, ymax = y, alpha = 0.6, ls = 'dotted', label = 'Residuen')\n\nplt.legend()\nplt.show()\n\nprint(\"Regressionskoeffizienten:\", lm)\n```\n:::\n\n\n::::\n\n&nbsp;\n\nDie eingezeichnete Gerade entspricht der linearen Funktion $\\hat{y} = \\beta_0 + \\beta_1 \\cdot x + e_i$. Die Dreiecksmarker sind die Vorhersagewerte $\\hat{y_i}$ des linearen Modells für die Werte $x_i = np.arange(1, 11)$. Die tatsächlichen Messwerte $y$ sind mit Kreismarkern markiert. Die Länge der gestrichelten Linien entspricht der Größe der Abweichung zwischen den Mess- und Vorhersagewerten $y_i - \\hat{y_i}$, also den Residuen $e_i$.\n\nGesucht wird diejenige Gerade, die die Summe der quadrierten Residuen minimiert. Die gesuchten Werte $\\beta_0$ und $\\beta_1$ sind die Kleinst-Quadrate-Schätzer.\n\n$$\n\\beta_0 = \\bar{y} - \\beta_1 \\cdot \\bar{x}\n$$\n\n$$\n\\beta_1 = { \\sum_{i=1}^n (x_i- \\bar{x}) \\cdot (y_i - \\bar{y}) \\over \\sum_{i=1}^n (x_i - \\bar{x})^2 }\n$$\n\nDer Vollständigkeit halber leiten wir die Kleinst-Quadrate-Schätzer her. Gesucht werden Werte für $\\beta_0$ und $\\beta_1$, damit die Summe der Residuenquadrate $\\sum_{i=1}^{n} e_i^2$ möglichst klein wird. Die Residuenquadratsumme ist die Summe der quadrierten Differenzen aus beobachteten Werten $y_i$ und der durch die lineare Funktion vorhergesagten Werte.\n$$\n\\sum_{i=1}^{n} e_i^2 ~ = ~ \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 \\cdot x_i))^2\n$$\n\nWir untersuchen also eine Funktion, die von zwei Variablen abhängig ist.\n$$\nf(\\beta_0, \\beta_1) = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 \\cdot x_i))^2\n$$\n\nDas Summenzeichen ist die Kurzschreibweise für eine Summe.\n$$\nf(\\beta_0, \\beta_1) = (y_1 - (\\beta_0 + \\beta_1 \\cdot x_1))^2 ~ + ~ (y_2 - (\\beta_0 + \\beta_1 \\cdot x_2))^2 ~ + ~ ... ~ (y_n - (\\beta_0 + \\beta_1 \\cdot x_n))^2\n$$\n\nIm Minimum der Funktion müssen die beiden partiellen Ableitungen gleich Null sein (Warum das so ist, wird [hier](https://www.sofatutor.com/mathematik/funktionen/funktionen-mehrerer-veraenderlicher/lokale-extrempunkte-bei-funktionen-mit-mehreren-veraenderlichen) leicht verständlich erklärt.)\n\n:::: {#nte-partielleableitung .callout-note collapse=\"true\"}\n## Partielle Ableitung\nDie partielle Ableitung ist die Ableitung einer Funktion mit mehreren Variablen nach einer Variablen, wobei die übrigen Variablen als Konstanten behandelt werden.\n\nFür eine Funktion $f(x, y) = 2x + y^2$ wird die partielle Ableitung nach x so ausgedrückt:\n\n$\\frac{\\partial f(x, y)}{\\partial x}$\n\n  - Das Symbol ∂ ist die kursive Darstellung des kyrillischen Kleinbuchstaben д (d) und wird als \"del\" gelesen. Es zeigt an, das eine partielle Ableitung durchgeführt wird.\n  - Im Zähler steht die Funktion, die abgeleitet werden soll. Im Nenner steht die Variable nach der abgeleitet wird. Der Term wird gelesen als \"del f von x und y nach del x\".\n\nDie partielle Ableitung $\\frac{\\partial f(x, y)}{\\partial x} = 2$. $y^2$ wird als Konstante behandelt (z. B. $5^2$ ) und ist abgeleitet Null.\n\nDie partielle Ableitung $\\frac{\\partial f(x, y)}{\\partial y} = 2y$. $2x$ wird als Konstante behandelt (z. B. $2 \\cdot 3$ ) und ist abgeleitet Null.\n\n::::\n\nIn beiden partiellen Ableitungen sind $x_i$ und $y_i$ konstant. In der partiellen Ableitung nach $\\beta_0$ ist außerdem $\\beta_1$ konstant, in der partiellen Ableitung nach $\\beta_1$ ist entsprechend $\\beta_0$ konstant.\n\n:::: {.panel-tabset}\n\n## partielle Ableitung nach dem y-Achsenschnittpunkt\nFür die partielle Ableitung nach $\\beta_0$ gilt also nach der Kettenregel für die äußere Funktion (oben) und die innere Funktion (Mitte):\n$$ \\begin{aligned}\n\\frac{\\partial f(\\beta_0, \\beta_1)}{\\partial \\beta_0} = 2 \\cdot (y_1 - (\\beta_0 + \\beta_1 \\cdot x_1)) ~ + ~ ... ~ (y_n - (\\beta_0 + \\beta_1 \\cdot x_n)) = 2 \\cdot \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 \\cdot x_i)) ~ \\cdot\n\\newline \n(0 - (1 + 0 \\cdot 0)) ~ + ~ ... ~ (0 - (1 + 0 \\cdot 0)) = \\sum_{i=1}^{n} (0 - (1 + 0 \\cdot 0)) =\n\\newline\n2 \\cdot \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 \\cdot x_i)) ~ \\cdot (-1)\n\\end{aligned}\n$$\n\nFür die partielle Ableitung nach $\\beta_0$  gilt also:\n$$\n\\frac{\\partial f(\\beta_0, \\beta_1)}{\\partial \\beta_0} = -2 \\cdot \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 \\cdot x_i)) = 0\n$$\n\nDiese kann vereinfacht werden, indem der Vorfaktor $-2$ entfällt (weil $-2 \\cdot 0 = 0$ gelten muss) und die Vorzeichen aufgelöst werden. Sodass:\n$$\n\\sum_{i=1}^{n} (y_i - \\beta_0 - \\beta_1 \\cdot x_i) = 0\n$$\n\nMan kann auch schreiben:\n$$\n\\sum_{i=1}^{n} y_i - \\sum_{i=1}^{n} \\beta_0 - \\sum_{i=1}^{n} \\beta_1 \\cdot x_i = 0\n$$\n\n$\\beta_0$ und $\\beta_1$ sind Konstanten, sodass gilt $\\sum_{i=1}^{n} \\beta_0  = \\beta_0 \\cdot \\sum_{i=1}^{n} 1 = \\beta_0 \\cdot n$ und $\\sum_{i=1}^{n} \\beta_1 \\cdot x_i = \\beta_1 \\cdot \\sum_{i=1}^{n} 1 \\cdot x_i$. So gilt:\n$$\n\\sum_{i=1}^{n} y_i - n \\cdot \\beta_0 - \\beta_1 \\cdot \\sum_{i=1}^{n} x_i = 0\n$$\n\nJetzt kann man durch $n$ teilen. Dabei entspricht $\\frac{\\sum_{i=1}^{n}y_i}{n}$ dem arithmetischen Mittelwert von $y$ und $\\frac{\\sum_{i=1}^{n}x_i}{n}$ dem arithmetischen Mittelwert von $x$. Somit steht:\n$$\n\\bar{y} - \\beta_0 - \\beta_1 \\cdot \\bar{x}= 0\n$$\n\nUmgestellt:\n$$\n\\beta_0 = \\bar{y} - \\beta_1 \\cdot \\bar{x}\n$$\n\n## partielle Ableitung nach dem Anstieg\nFür die partielle Ableitung nach $\\beta_1$ ist ebenfalls die Kettenregel anzuwenden, sodass die äußere Funktion (oben) identisch abgeleitet wird:\n$$ \\begin{aligned}\n\\frac{\\partial f(\\beta_0, \\beta_1)}{\\partial \\beta_1} = 2 \\cdot (y_1 - (\\beta_0 + \\beta_1 \\cdot x_1)) ~ + ~ ... ~ (y_n - (\\beta_0 + \\beta_1 \\cdot x_n)) = 2 \\cdot \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 \\cdot x_i)) ~ \\cdot\n\\newline \n(0 - (0 + 1 \\cdot x_1)) ~ + ~ ... ~ (0 - (0 + 1 \\cdot x_n)) ~ = ~ \\sum_{i=1}^{n} -x_i = \n\\newline\n2 \\cdot \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 \\cdot x_i)) ~ \\cdot -x_i\n\\end{aligned}\n$$\n\nFür die partielle Ableitung nach $\\beta_1$  gilt also:\n$$\n\\frac{\\partial f(\\beta_0, \\beta_1)}{\\partial \\beta_1} = -2 \\sum_{i=1}^{n} x_i \\cdot (y_i - (\\beta_0 + \\beta_1 \\cdot x_i)) = 0\n$$\n\nAuch diese kann vereinfacht werden, indem der Vorfaktor $-2$ entfällt (weil $-2 \\cdot 0 = 0$ gelten muss) und die Vorzeichen aufgelöst werden. Außerdem kann ausmultipliziert werden:\n$$\n\\sum_{i=1}^{n} x_iy_i - \\sum_{i=1}^{n}  \\beta_0 \\cdot x_i - \\sum_{i=1}^{n}  \\beta_1 \\cdot x_ix_i = 0\n$$\n\nWieder können die Konstanten herausgezogen werden:\n$$\n\\sum_{i=1}^{n} x_iy_i - \\beta_0 \\cdot \\sum_{i=1}^{n} x_i - \\beta_1 \\cdot \\sum_{i=1}^{n} x_ix_i = 0\n$$\n\nJetzt kann man $\\beta_0 = \\bar{y} - \\beta_1 \\cdot \\bar{x}$ und $\\sum_{i=1}^{n} x_i = n \\cdot \\bar{x}$ einsetzen:\n$$\n\\sum_{i=1}^{n} x_iy_i - (\\bar{y} - \\beta_1 \\cdot \\bar{x}) \\cdot  n \\cdot \\bar{x} - \\beta_1 \\cdot \\sum_{i=1}^{n} x_ix_i = 0\n$$\n\nDer mittlere Term wird ausmultipliziert und $x_ix_i$ im letzten Term als $x_i^2$ geschrieben:\n$$\n\\sum_{i=1}^{n} x_iy_i - n\\bar{x}\\bar{y} - \\beta_1 \\cdot n\\bar{x}\\bar{x} - \\beta_1 \\cdot \\sum_{i=1}^{n} x_i^2 = 0\n$$\n\nDie letzten beiden Terme werden unter Anwendung des Distributivgesetzes $a−b = − (b−a)$ zusammengefasst.\n$$\n\\sum_{i=1}^{n} x_iy_i - n\\bar{x}\\bar{y} - \\beta_1 \\cdot ( \\sum_{i=1}^{n} x_i^2 - n \\bar{x}^2) = 0\n$$\n\nJetzt kann nach $\\beta_1$ umgestellt werden. Erst:\n$$\n\\beta_1 \\cdot ( \\sum_{i=1}^{n} x_i^2 - n \\bar{x}^2) = \\sum_{i=1}^{n} x_iy_i - n\\bar{x}\\bar{y}\n$$\n\nDann:\n$$\n\\beta_1 = \\frac{\\sum_{i=1}^{n} x_iy_i - n\\bar{x}\\bar{y}}{\\sum_{i=1}^{n} x_i^2 - n \\bar{x}^2}\n$$\n\nNun kann zuerst mit $\\sum_{i=1}^{n} x_i^2 - n \\bar{x}^2 = \\sum_{i=1}^{n} (x_i - \\bar{x})^2$ umgeformt werden.\n$$\n\\beta_1 = \\frac{\\sum_{i=1}^{n} x_iy_i - n\\bar{x}\\bar{y}}{\\sum_{i=1}^{n}  (x_i - \\bar{x})^2}\n$$\n\nDann - und das wird gleich gezeigt - mit $\\sum_{i=1}^{n} x_iy_i - n\\bar{x}\\bar{y} = \\sum_{i=1}^{n} (x_i - \\bar{x}) (y_i - \\bar{y})$. Sodass steht:\n$$\n\\beta_1 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum_{i=1}^{n}  (x_i - \\bar{x})^2}\n$$\n\nDer letzte Schritt wird ausgehend vom Ergebnis gezeigt und beginnt mit dem Ausmultiplizieren:\n$$\n\\sum_{i=1}^{n} (x_i - \\bar{x}) (y_i - \\bar{y}) = \\sum_{i=1}^{n} (x_iy_i - x_i\\bar{y} - \\bar{x}y_i + \\bar{x}\\bar{y})\n$$\n\nMan kann auch schreiben:\n$$\n\\sum_{i=1}^{n} x_iy_i - \\sum_{i=1}^{n} x_i\\bar{y} - \\sum_{i=1}^{n} \\bar{x}y_i + \\sum_{i=1}^{n} \\bar{x}\\bar{y}\n$$\n\n$\\bar{x}$ und $\\bar{y}$ sind Konstanten, sodass $\\bar{x} \\cdot \\sum_{i=1}^{n} y_i$ und $\\bar{y} \\cdot \\sum_{i=1}^{n} x_i$ geschrieben werden kann. $\\sum_{i=1}^{n} x_i$ ist gleich $n \\cdot \\bar{x}$ (analog für $y$). So ergibt sich:\n\n$$\n\\sum_{i=1}^{n} x_iy_i - \\bar{y} \\cdot n \\cdot \\bar{x} - \\bar{x} \\cdot n \\cdot \\bar{y} + \\sum_{i=1}^{n} \\bar{x}\\bar{y}\n$$\n\nSortieren:\n$$\n\\sum_{i=1}^{n} x_iy_i - n\\bar{x}\\bar{y}  -  n\\bar{x}\\bar{y} + \\sum_{i=1}^{n} \\bar{x}\\bar{y}\n$$\n\nDer letzte Term $\\sum_{i=1}^{n} \\bar{x}\\bar{y}$ kann auch $n \\cdot \\bar{x}\\bar{y}$ geschrieben werden, sodass sich ergibt:\n$$\n\\sum_{i=1}^{n} x_iy_i - n\\bar{x}\\bar{y} - n\\bar{x}\\bar{y} + n\\bar{x}\\bar{y}\n$$\n\nDie letzten beiden Terme entfallen somit und es bleibt:\n$$\n\\sum_{i=1}^{n} x_iy_i - n\\bar{x}\\bar{y} \n$$\n\n[@Baitsch-2019, S. 73-74]\n\n::::\n:::\n\n\nDie Funktionen dafür stellen sowohl das Paket `numpy.polynomial` bzw. für Polynomfunktionen dessen Modul `numpy.polynomial.polynomial` als auch das Modul `scipy.stats.linregress` bereit. Im Folgenden wird die Berechnung mit NumPy gezeigt und anschließend die Funktionen aus dem Modul SciPy vorgestellt. Die Funktionsweise beider Module ist ähnlich.\n\n#### NumPy polyfit und polyeval\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nimport numpy.polynomial.polynomial as poly\n```\n:::\n\n\nZur Schätzung von Funktionsparametern nach der Methode der kleinsten Quadrate wird die Funktion `poly.polyfit(x, y, deg)` verwendet. `x` sind die Werte der unabhängigen Variablen, `y` die Werte der abhängigen Variablen und `deg` spezifiziert den Grad der gesuchten Polynomfunktion. `deg = 1` spezifiziert eine lineare Funktion.\n\n::: {#nte-polyfitpolyeval .callout-note collapse=\"true\"}\n## polyfit und polyeval erklärt\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\n# Beispieldaten erzeugen\nx = np.array(list(range(0, 100)))\ny = x ** 2\n\nprint(poly.polyfit(x, y, 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[-1617.    99.]\n```\n:::\n:::\n\n\nDie Funktion gibt die geschätzten Regressionsparameter als NumPy-Array zurück. Die Terme sind aufsteigend angeordnet, d. h. der Achsabschnitt steht an Indexposition 0, der Steigungskoeffizient an Indexposition 1. Die Ausgabe für ein Polynom zweiten Grades würde beispielsweise so aussehen:\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nprint(poly.polyfit(x, y, 2).round(2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0. 0. 1.]\n```\n:::\n:::\n\n\nMit den Regressionskoeffizienten können die Vorhersagewerte der linearen Funktion berechnet werden. Dafür wird die Funktion `poly.polyeval(x, c)` verwendet. Diese berechnet die Funktionswerte für in `x` übergebene Werte mit den Funktionsparametern `c`. Aus der Differenz der gemessenen Werte und der Vorhersagewerte können die Residuen bestimmt werden.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\n# 'manuelle' Berechnung\nregressions_koeffizienten = poly.polyfit(x, y, 1)\nvorhersagewerte = regressions_koeffizienten[0] + x * regressions_koeffizienten[1]\nresiduen = y - vorhersagewerte\n\n# Berechnung mit polyeval\nlm = poly.polyfit(x, y, 1)\nvorhersagewerte_polyval = poly.polyval(x, lm)\n\nprint(\"Die Ergebnisse stimmen überein:\", np.equal(vorhersagewerte, vorhersagewerte_polyval).all())\nprint(\"\\nAusschnitt der Vorhersagewerte:\", vorhersagewerte[:10])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDie Ergebnisse stimmen überein: True\n\nAusschnitt der Vorhersagewerte: [-1617. -1518. -1419. -1320. -1221. -1122. -1023.  -924.  -825.  -726.]\n```\n:::\n:::\n\n\nDas [Bestimmtheitsmaß](https://de.wikipedia.org/wiki/Bestimmtheitsma%C3%9F) $R^2$ gibt an, wie gut die Schätzfunktion an die Daten angepasst ist. Der Wertebereich reicht von 0 bis 1. Ein Wert von 1 bedeutet eine vollständige Anpassung. Für eine einfache lineare Regression mit nur einer erklärenden Variable kann das Bestimmtheitsmaß als Quadrat des [Bravais-Pearson-Korrelationskoeffizienten](https://de.wikipedia.org/wiki/Korrelationskoeffizient_nach_Bravais-Pearson) $r$ berechnet werden. Dieser wird mit der Funktion `np.corrcoef(x, y)` ermittelt (die eine Matrix der Korrelationskoeffizienten ausgibt).\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nprint(f\"r = {np.corrcoef(x, y)[0, 1]:.2f}\")\nprint(f\"R\\u00b2 = {np.corrcoef(x, y)[0, 1] ** 2:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nr = 0.97\nR² = 0.94\n```\n:::\n:::\n\n\nDie Daten und die geschätzte Gerade können grafisch dargestellt werden.\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nplt.scatter(x, y, label = 'Beispieldaten')\nplt.plot(x, vorhersagewerte, label = 'Vorhersagewerte')\nplt.annotate(\"$R^2$ = {:.2f}\".format(np.corrcoef(x, y)[0, 1] ** 2), (max(x) * 0.9, 1))\n\nplt.title(label = 'Beispieldaten und geschätzte Linearfunktion')\nplt.xlabel('x-Werte')\nplt.ylabel('y-Werte')\nplt.legend()\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](lineare-parameterschätzung_files/figure-pdf/cell-23-output-1.png){fig-pos='H'}\n:::\n:::\n\n\n:::\n\nNumPy umfasst außerdem die inzwischen veralteten Funktionen `np.polyfit(x, y, deg)` und `np.polyval(p, x)`.\n\n::: {#nte-polyfit .callout-note collapse=\"true\"}\n\n## np.polyfit & np.polyval\nDie Funktionen `np.polyfit(x, y, deg)` und `np.polyval(p, x)` funktionieren wie die vorgestellten Funktionen aus dem Modul `numpy.polynomial.polynomial`. Ein wichtiger Unterschied besteht jedoch darin, dass **die Parameter der Funktion `polyfit` in umgekehrter Reihenfolge** ausgegeben werden.\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nprint(poly.polyfit(x, y, deg = 1))\nprint(np.polyfit(x, y, deg = 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[-1617.    99.]\n[   99. -1617.]\n```\n:::\n:::\n\n\n:::: {.border}\n::::: {.callout-note}\n\nThis forms part of the old polynomial API. Since version 1.4, the new polynomial API defined in `numpy.polynomial` is preferred. A summary of the differences can be found in the [transition guide](https://numpy.org/doc/stable/reference/routines.polynomials.html).\n\n:::::\n\n[NumPy-Dokumentation](https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html)\n\n::::\n:::\n\nDie Parameter der an die Messwerte angepassten linearen Funktion und das Bestimmtheitsmaß lauten:\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nprint(poly.polyfit(hooke['distance'], hooke['force'], 1))\n\nprint(f\"r = {np.corrcoef(hooke['distance'], hooke['force'])[0, 1]:.2f}\")\nprint(f\"R\\u00b2 = {np.corrcoef(hooke['distance'], hooke['force'])[0, 1] ** 2:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[ 0.05753159 33.01899551]\nr = 0.99\nR² = 0.99\n```\n:::\n:::\n\n\nMit den Regressionskoeffizienten können die Vorhersagewerte der linearen Funktion berechnet werden. \n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\n# Berechnung mit polyeval\nlm = poly.polyfit(hooke['distance'], hooke['force'], 1)\nvorhersagewerte_hooke = poly.polyval(hooke['distance'], lm)\n```\n:::\n\n\nDie Messreihe und die darauf angepasste lineare Funktion können grafisch dargestellt werden.\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\n# Platzhalter x & y\nx = hooke['distance']\ny = hooke['force']\n\n# Plot erstellen\nplt.scatter(x, y, label = 'Messdaten')\nplt.axline(xy1 = (0, lm[0]), slope = lm[1], label = 'Regressionsgerade\\ny = ' + \"{beta_0:.3f}\".format(beta_0 = lm[0]) + ' + ' + \"{beta_1:.3f} \".format(beta_1 = lm[1]) + 'x' )\nplt.annotate(\"$R^2$ = {:.2f}\".format(np.corrcoef(x, y)[0, 1] ** 2), (max(x) * 0.9, 1))\n\nplt.title(label = 'Messdaten und geschätzte Linearfunktion')\nplt.xlabel('gemessene Abstandsänderung in m')\nplt.ylabel('wirkende Kraft in N')\nplt.legend()\n\nplt.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](lineare-parameterschätzung_files/figure-pdf/cell-27-output-1.png){fig-alt='Messdaten des Federexperiments (Abstandsänderung vs. wirkende Kraft) mit eingezeichneter Regressionsgeraden.' fig-pos='H'}\n:::\n:::\n\n\n### Messabweichung quantifizieren\nFür den geschätzten Regressionskoeffizienten kann für die lineare Regression mit einer erklärenden Variable der Standardfehler des Regressionskoeffizienten $SE = \\hat{\\sigma}_{\\hat{\\beta_1}}$ ermittelt werden (siehe [Wikipedia](https://de.wikipedia.org/wiki/Standardfehler_des_Regressionskoeffizienten#Spezialfall:_Lineare_Einfachregression)). \n\n$$\nSE = \\sqrt{\\frac{\\frac{1}{n-2} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{ \\sum_{i=1}^{n} (x_i - \\bar{x})^2}}\n$$\n\n  - Im Zähler steht die mittlere [Residuenquadratsumme](https://de.wikipedia.org/wiki/Residuenquadratsumme) (Summe der quadrierten Residuen / Anzahl der Freiheitsgrade).\n  - Im Nenner steht die [Summe der Abweichungsquadrate von $x$](https://de.wikipedia.org/wiki/Summe_der_Abweichungsquadrate).\n\nFür ein Signifikanzniveau $\\alpha$ kann ein Konfidenzniveau $1 - \\alpha$ angegeben werden als:\n\n$$\n\\hat{\\beta_1} \\pm SE \\cdot t_{1-\\alpha / 2} ~ (n - 2)\n$$\n\n - $t_{1-\\alpha / 2} ~ (n - 2)$ ist der Wert der t-Verteilung mit n - 2 Freiheitsgraden bzw. der Rückgabewert der Funktion:\n    - `scipy.stats.t.ppf(q = 1 - alpha/2, df = n - 2)` für die obere Intervallgrenze.\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\nprint(f\"Regressionskoeffizient: {lm[1]:.4f}\")\n\n# 'manuell' Standardfehler des Regressionskoeffizienten berechnen\nstandardfehler_beta_1 = np.sqrt( (1 / (len(x) - 2) * sum((y - vorhersagewerte_hooke) ** 2)) / sum( (x - x.mean()) ** 2  ))\n\nprint(f\"Standardfehler des Regressionskoeffizienten: {standardfehler_beta_1:.4f}\")\n\n# Signifikanzniveau (alpha-Niveau) 1 - 95 % wählen\nalpha = 0.05\nn = len(x)\n\nt_wert = scipy.stats.t.ppf(q = 1 - alpha/2, df = n - 2)\nprint(f\"t-Wert 95-%-Intervall (zweiseitig): {t_wert:.4f}\")\nprint(f\"Konfidenzintervall 95 %: {lm[1]:.4f} ± {t_wert:.4f} * {standardfehler_beta_1:.4f}\")\nprint(f\"untere 95-%-Intervallgrenze: {lm[1] - t_wert * standardfehler_beta_1:.4f}\")\nprint(f\"obere 95-%-Intervallgrenze: {lm[1] + t_wert * standardfehler_beta_1:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRegressionskoeffizient: 33.0190\nStandardfehler des Regressionskoeffizienten: 0.3784\nt-Wert 95-%-Intervall (zweiseitig): 1.9816\nKonfidenzintervall 95 %: 33.0190 ± 1.9816 * 0.3784\nuntere 95-%-Intervallgrenze: 32.2692\nobere 95-%-Intervallgrenze: 33.7688\n```\n:::\n:::\n\n\nDas Konfidenzintervall kann auch grafisch dargestellt werden.  \n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\n# Platzhalter x & y\nx = hooke['distance']\ny = hooke['force']\n\n# Plot erstellen\nplt.scatter(x, y, label = 'Messdaten')\nplt.axline(xy1 = (0, lm[0]), slope = lm[1], label = 'Regressionsgerade\\ny = ' + \"{beta_0:.3f}\".format(beta_0 = lm[0]) + ' + ' + \"{beta_1:.3f} \".format(beta_1 = lm[1]) + 'x' )\nplt.annotate(\"$R^2$ = {:.2f}\".format(np.corrcoef(x, y)[0, 1] ** 2), (max(x) * 0.9, 1))\n\n# 95-%-Konfidenzintervall einzeichnen\n## poly.polyval(hooke['distance'], [lm[0]])\nbeta1_lower_boundary = lm[1] - (t_wert * standardfehler_beta_1)\nbeta1_upper_boundary = lm[1] + (t_wert * standardfehler_beta_1)\n\ny_lower_boundary = poly.polyval(hooke['distance'], [lm[0], beta1_lower_boundary])\ny_upper_boundary = poly.polyval(hooke['distance'], [lm[0], beta1_upper_boundary])\n\nplt.fill_between(x = x, y1 = y_lower_boundary , y2 = y_upper_boundary, alpha = 0.3, label = '95-%-Konfidenzintervall $\\\\beta_1$')\n\n\nplt.title(label = 'Messdaten und geschätzte Linearfunktion im 95-%-Intervall')\nplt.xlabel('gemessene Abstandsänderung in m')\nplt.ylabel('wirkende Kraft in N')\nplt.legend()\n\nplt.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](lineare-parameterschätzung_files/figure-pdf/cell-29-output-1.png){fig-alt='Messdaten des Federexperiments (Abstandsänderung vs. wirkende Kraft) mit eingezeichneter Regressionsgeraden und 95-%-Konfidenzintervall des Anstiegs.' fig-pos='H'}\n:::\n:::\n\n\n### Das Modul SciPy\nDie Funktion `scipy.stats.lingress(x, y)` liefert mit einem Funktionsaufruf zahlreiche Rückgabewerte:\n\n  - Steigung der Regressionsgerade,\n  - y-Achsenschnittpunkt der Regressionsgerade,\n  - Bravais-Pearson-Korrelationskoeffizient r,\n  - p-Wert der Nullhypothese, dass die Steigung der Regressionsgerade Null ist,\n  - Standardfehler der Steigung und\n  - Standardfehler des y-Achsenschnittpunkts.\n\nDer Standardfehler des y-Achsenschnittpunkts ist nur verfügbar, wenn die Rückgabewerte in einem Objekt gespeichert werden. Die Rückgabewerte können dann als Attribute abgerufen werden.\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\n# Zuweisung mehrerer Objekte\nslope, intercept, rvalue, pvalue, slope_stderr = scipy.stats.linregress(x, y)\nprint(f\"y = {intercept:.4f} + {slope:.4f} * x\\n\",\n      f\"r = {rvalue:.4f} R2 = {rvalue ** 2:.4f} p = {pvalue:.4f}\\n\",\n      f\"Standardfehler des Anstiegs: {slope_stderr:.4f}\", sep = '')\n\n# Zuweisung eines Objekts\nlm = scipy.stats.linregress(x, y)\n\nprint(\"\\n\", lm, sep = '')\nprint(f\"y-Achsenschnittpunkt: {lm.intercept:.4f}\\nStandardfehler des y-Achsenschnittpunkts:{lm.intercept_stderr:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ny = 0.0575 + 33.0190 * x\nr = 0.9928 R2 = 0.9856 p = 0.0000\nStandardfehler des Anstiegs: 0.3784\n\nLinregressResult(slope=np.float64(33.01899550918018), intercept=np.float64(0.05753158907970102), rvalue=np.float64(0.9927907555799099), pvalue=np.float64(4.115211719827623e-104), stderr=np.float64(0.37837320019327897), intercept_stderr=np.float64(0.0506707972676925))\ny-Achsenschnittpunkt: 0.0575\nStandardfehler des y-Achsenschnittpunkts:0.0507\n```\n:::\n:::\n\n\nSo kann mit dem entsprechenden t-Wert das Konfidenzintervall berechnet werden.\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\nalpha = 0.05\nn = len(x)\n\nprint(f\"{slope - scipy.stats.t.ppf(q = 1 - alpha / 2, df = n - 2) * slope_stderr:.3f}  ≤ {slope:.3f} ≤ {slope + scipy.stats.t.ppf(q = 1 - alpha / 2, df = n - 2) * slope_stderr:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n32.269  ≤ 33.019 ≤ 33.769\n```\n:::\n:::\n\n\n### Ergebnis Federkonstante\nDie Federkonstante des Versuchaufbaus liegt mit 95 prozentiger Sicherheit im Intervall zwischen 32,27 und 33,77. Die Punktschätzung für die Federkonstante beträgt 33,02.\n\n\n**Aufgabe könnte sein, das Konfidenzintervall 99-Prozent zu berechnen.**  \n--> Dann muss man aber nur eine Zahl ändern\n\n",
    "supporting": [
      "lineare-parameterschätzung_files/figure-pdf"
    ],
    "filters": []
  }
}